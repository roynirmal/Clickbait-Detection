{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE of each feature group\n",
    "for classif in [ \"SVC\",\"Log\",  \"Xg\", \"RF\",\"NB\"]:\n",
    "    print(classif)\n",
    "    print(\"content\")\n",
    "    computeMSE(contentM, contentClass,10, \"f_classif\", \"all\", classif)\n",
    "    print(\"sentiment\")\n",
    "    computeMSE(sentimentM, sentimentClass,10, \"f_classif\", \"all\", classif)\n",
    "    print(\"informality\")\n",
    "    computeMSE(informalityM, informalityClass,10, \"f_classif\", \"all\", classif)\n",
    "    print(\"posPattern\")\n",
    "    computeMSE(posPatternM, posPatternClass,10, \"f_classif\", \"all\", classif)\n",
    "    print(\"unigrams\")\n",
    "    computeMSE(unigramsM, unigramsClass,10, \"f_classif\", 250, classif)\n",
    "    print(\"Baseline\")\n",
    "    computeMSE(finalFeaturesMSE, finalFeatures,10, \"f_classif\", \"all\", classif)\n",
    "    print(\"mixed\")\n",
    "    computeMSE(mixedM, mixedClass,10, \"f_classif\", \"all\", classif)\n",
    "    print(\"all\")\n",
    "    computeMSE(allFM, allFClass,10, \"f_classif\", \"all\", classif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate MSE of all Features under their best classifier setting\n",
    "computeMSE(allFM, allFClass,10, \"f_classif\", 500, \"Log\", \"xx\")\n",
    "computeMSE(allFM, allFClass,10, \"f_classif\", 600, \"SVC\", \"xx\")\n",
    "computeMSE(allFM, allFClass,10, \"f_classif\", 300, \"RF\", \"xx\")\n",
    "computeMSE(allFM, allFClass,10, \"f_classif\", 500, \"Xg\", \"xx\")\n",
    "computeMSE(allFM, allFClass,10, \"f_classif\", 10, \"NB\", \"xx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE and Accuracy of SVM regressor\n",
    "nrFeat = [10, 20, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600]\n",
    "resultsallfeatSMSE=pd.DataFrame(index = nrFeat)\n",
    "resultsallfeatSFaccu=pd.DataFrame(index = nrFeat)\n",
    "for param in [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 0.5, 1]:\n",
    "    print(param)\n",
    "    accuracies = []\n",
    "    RMSE = []\n",
    "    for feat in nrfeat:\n",
    "        print(feat)\n",
    "        [rmse,acc]=computeMSE(allFM, allFClass,10, \"f_classif\", feat, \"LinearSVR\", param)\n",
    "        accuracies.append(acc)\n",
    "        RMSE.append(rmse)\n",
    "    resultsallfeatSMSE[param] = RMSE\n",
    "    resultsallfeatSFaccu[param] = accuracies\n",
    "#     print(\"content\")        \n",
    "#     print(\"content\")\n",
    "#     computeMSE(contentM, contentClass,10, \"f_classif\", \"all\", \"LinearSVR\", param)\n",
    "#     print(\"sentiment\")\n",
    "#     computeMSE(sentimentM, sentimentClass,10, \"f_classif\", \"all\", \"LinearSVR\", param)\n",
    "#     print(\"informality\")\n",
    "#     computeMSE(informalityM, informalityClass,10, \"f_classif\", \"all\", \"LinearSVR\", param)\n",
    "#     print(\"posPattern\")\n",
    "#     computeMSE(posPatternM, posPatternClass,10, \"f_classif\", \"all\", \"LinearSVR\", param)\n",
    "#     print(\"unigrams\")\n",
    "#     computeMSE(unigramsM, unigramsClass,10, \"f_classif\", 250, \"LinearSVR\", param)\n",
    "#     print(\"Baseline\")\n",
    "#     computeMSE(finalFeaturesMSE, finalFeatures,10, \"f_classif\", \"all\", \"LinearSVR\", param)\n",
    "#     print(\"mixed\")\n",
    "#     computeMSE(mixedM, mixedClass,10, \"f_classif\", \"all\", \"LinearSVR\", param)\n",
    "#         computeMSE(allFM, allFClass,10, \"f_classif\", feat, \"LinearSVR\", param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MSE and Accuracy of RF regressor\n",
    "nrFeat = [10, 20, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600]\n",
    "resultsallfeatRFMSE=pd.DataFrame(index = nrFeat)\n",
    "resultsallfeatRFaccu=pd.DataFrame(index = nrFeat)\n",
    "for param in [2,4,6,8,9,10,15,20]:\n",
    "    print(param)\n",
    "    accuracies = []\n",
    "    RMSE = []\n",
    "    for feat in nrfeat:\n",
    "        print(feat)\n",
    "        [rmse,acc]=computeMSE(allFM, allFClass,10, \"f_classif\", feat, \"RFreg\", param)\n",
    "        accuracies.append(acc)\n",
    "        RMSE.append(rmse)\n",
    "    resultsallfeatRFMSE[param] = RMSE\n",
    "    resultsallfeatRFaccu[param] = accuracies\n",
    "#     print(\"content\")\n",
    "#     computeMSE(contentM, contentClass,10, \"f_classif\", \"all\", \"RFreg\", param)\n",
    "#     print(\"sentiment\")\n",
    "#     computeMSE(sentimentM, sentimentClass,10, \"f_classif\", \"all\", \"RFreg\", param)\n",
    "#     print(\"informality\")\n",
    "#     computeMSE(informalityM, informalityClass,10, \"f_classif\", \"all\", \"RFreg\", param)\n",
    "#     print(\"posPattern\")\n",
    "#     computeMSE(posPatternM, posPatternClass,10, \"f_classif\", \"all\", \"RFreg\", param)\n",
    "#     print(\"unigrams\")\n",
    "#     computeMSE(unigramsM, unigramsClass,10, \"f_classif\", 250, \"RFreg\", param)\n",
    "#     print(\"Baseline\")\n",
    "#     computeMSE(finalFeaturesMSE, finalFeatures,10, \"f_classif\", \"all\", \"RFreg\", param)\n",
    "#     print(\"mixed\")\n",
    "#     computeMSE(mixedM, mixedClass,10, \"f_classif\", \"all\", \"RFreg\", param)\n",
    "#         computeMSE(allFM, allFClass,10, \"f_classif\", feat, \"RFreg\", param)\n",
    "#     classifiers = [\"Log\",\"SVM\",\"RandomForest\",\"XgBoost\",\"NaiveBayes\"]\n",
    "# nrFeats = [10,50,100,150,250,300,350,400,450, 500,550,600]\n",
    "# resultsposPatterns=pd.DataFrame(index = nrFeats)\n",
    "# for classif in classifiers:\n",
    "#     print(classif)\n",
    "#     accuracies=[]\n",
    "#     for nfeat in nrFeats:\n",
    "#         [acc1,acc2]=computeMSE(allFM, allFClass,10, \"f_classif\", feat, \"RFreg\", param)\n",
    "#         accuracies.append(acc1)\n",
    "#         print(nfeat)\n",
    "#     resultsposPatterns[classif]=accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE and Accuracy of Random Forest regressor\n",
    "\n",
    "for feat in [10, 20, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600]:\n",
    "    print(feat)\n",
    "    computeMSE(allFM, allFClass,10, \"f_classif\", feat, \"XgBoost\", param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE and Accuracy of ensembler with SVC regressor and finding the best setting\n",
    "for c in  [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 0.5, 1]:\n",
    "    print(c)\n",
    "    computeEnsembleClassifier(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE,10,c, \"SVC\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE and Accuracy of ensembler with RF regressor and finding the best setting\n",
    "for tdepth in [2,4,6,8,10,15,20]:\n",
    "    print(tdepth)\n",
    "    computeEnsembleClassifier(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE,10,tdepth, \"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "computeEnsembleClassifier(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE,10,\"xx\", \"XgBoost\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross validation on the big dataset\n",
    "computeEnsembleClassifier(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE,5,0.1, \"SVC\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkingSubsetForEnsemble(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE, cross_val,c, cls):\n",
    "    \n",
    "    ##set up cross validation\n",
    "    cv_idx = KFold(n=len(varpd), n_folds=cross_val, shuffle=True, random_state=1)\n",
    "    \n",
    "    originalclass = []\n",
    "    predictedclass = []\n",
    "    \n",
    "    yclass = finalFeatures.iloc[:,-1]\n",
    "    posPatternMMSE = normalize(posPatternClass)\n",
    "    unigramsMMSE = normalize(unigramsClass)\n",
    "    mixedMMSE = normalize(mixedClass)\n",
    "    finalFeaturesNormMSE = normalize(finalFeatures)\n",
    "    \n",
    "    #only works with nd numpy arrays\n",
    "    \n",
    "    ycont = finalFeaturesMSE.truthMean.values\n",
    "    yp = posPatternClass.truthClass.values\n",
    "    yu = unigramsClass.truthClass.values\n",
    "    ym = mixedClass.truthClass.values\n",
    "    yf = finalFeatures.truthClass.values\n",
    "    \n",
    "    Xp = posPatternMMSE.as_matrix(columns = posPatternMMSE.columns)\n",
    "    Xu = unigramsMMSE.as_matrix(columns = unigramsMMSE.columns)\n",
    "    Xm = mixedMMSE.as_matrix(columns = mixedMMSE.columns)\n",
    "    Xf = finalFeaturesNormMSE.as_matrix(columns = finalFeaturesNormMSE.columns)\n",
    "    \n",
    "\n",
    "    Xp_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xp, yp)\n",
    "    Xu_new = SelectKBest(f_classif, k=250).fit_transform(Xu, yu)\n",
    "    Xm_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xm, ym)\n",
    "    Xf_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xf, yf)\n",
    "    \n",
    "    \n",
    "    y_out = pd.DataFrame()\n",
    "#     scores = []\n",
    "    accM1 = []\n",
    "    accS1 = []\n",
    "    \n",
    "    accM2 =[]\n",
    "    accM3 = []\n",
    "    accM4= []\n",
    "    \n",
    "    accS2 = []\n",
    "    accS3 = []\n",
    "    accS4=[]\n",
    "#     scoresM = []\n",
    "    count = 0\n",
    "    for train_idx, val_idx in cv_idx:\n",
    "           \n",
    "            clrp = SVC(probability = True)\n",
    "            clru = LogisticRegressionCV()\n",
    "            clrm = SVC(probability = True)\n",
    "            clrf = LogisticRegressionCV()\n",
    "                           \n",
    "            clrp.fit(Xp_new[train_idx], yp[train_idx])\n",
    "            clru.fit(Xu_new[train_idx], yu[train_idx])\n",
    "            clrm.fit(Xm_new[train_idx], ym[train_idx])\n",
    "            clrf.fit(Xf_new[train_idx], yf[train_idx])\n",
    "\n",
    "            yp_pred = clrp.predict_proba(Xp_new[val_idx])[:,1]\n",
    "            yp_pred_train = clrp.predict_proba(Xp_new[train_idx])[:,1]\n",
    "\n",
    "            \n",
    "            \n",
    "            yu_pred = clru.predict_proba(Xu_new[val_idx])[:,1]\n",
    "            yu_pred_train = clru.predict_proba(Xu_new[train_idx])[:,1]\n",
    "            \n",
    "            ym_pred = clrm.predict_proba(Xm_new[val_idx])[:,1]\n",
    "            ym_pred_train = clrm.predict_proba(Xm_new[train_idx])[:,1]\n",
    "           \n",
    "            \n",
    "            yf_pred = clrf.predict_proba(Xf_new[val_idx])[:,1]\n",
    "            yf_pred_train = clrf.predict_proba(Xf_new[train_idx])[:,1]\n",
    "            \n",
    "\n",
    "            \n",
    "            d1= {'p':yp_pred.tolist(), 'u':yu_pred.tolist(), 'm':ym_pred.tolist(), 'f':yf_pred.tolist(), 'y': yclass[val_idx].tolist(), 'ycont': ycont[val_idx].tolist()}\n",
    "            d2= {'p':yp_pred_train.tolist(), 'u':yu_pred_train.tolist(), 'm':ym_pred_train.tolist(), 'f':yf_pred_train.tolist(), 'y': yclass[train_idx].tolist(), 'ycont': ycont[train_idx].tolist()}\n",
    "            df1 = pd.DataFrame(d1)\n",
    "            df2 = pd.DataFrame(d2)\n",
    "            \n",
    "            d= pd.concat([df1, df2])\n",
    "            \n",
    "            X_train, y_train, ycont_train = df2[[\"p\", \"u\", \"m\", \"f\"]], df2[\"y\"], df2[\"ycont\"]\n",
    "            #test = d[~msk]\n",
    "            X_test, y_test, ycont_test = df1[[\"p\", \"u\", \"m\", \"f\"]], df1[\"y\"], df1[\"ycont\"]\n",
    "            \n",
    "            if(cls == \"RF\"):\n",
    "                clf = RandomForestRegressor(max_depth=c, random_state=0) #i used random forest\n",
    "                #             prob = cl.predict_proba\n",
    "                clf.fit(X_train, ycont_train) # i changed this to work with regression\n",
    "                #score = clf.score(X_test, ycont_test) ## i removed this so that it worked with regression\n",
    "                prob = clf.predict(X_test)\n",
    "            \n",
    "            \n",
    "            elif(cls == \"SVC\"):\n",
    "                clf1 =  LinearSVR(C=c, loss='squared_epsilon_insensitive', dual=False, random_state=1)\n",
    "                #             prob = cl.predict_proba\n",
    "                clf1.fit(X_train[[\"p\", \"u\", \"m\"]], ycont_train) # i changed this to work with regression\n",
    "                \n",
    "                prob1 = clf1.predict(X_test[[\"p\", \"u\", \"m\"]])\n",
    "                \n",
    "                clf2 =  LinearSVR(C=c, loss='squared_epsilon_insensitive', dual=False, random_state=1)\n",
    "                clf2.fit(X_train[[\"p\", \"u\", \"f\"]], ycont_train) # i changed this to work with regression\n",
    "                \n",
    "                prob2 = clf2.predict(X_test[[\"p\", \"u\", \"f\"]])\n",
    "                \n",
    "                clf3 =  LinearSVR(C=c, loss='squared_epsilon_insensitive', dual=False, random_state=1)\n",
    "                clf3.fit(X_train[[\"p\", \"m\", \"f\"]], ycont_train) # i changed this to work with regression\n",
    "                \n",
    "                prob3 = clf3.predict(X_test[[\"p\", \"m\", \"f\"]])\n",
    "                \n",
    "                clf4 =  LinearSVR(C=c, loss='squared_epsilon_insensitive', dual=False, random_state=1)\n",
    "                clf4.fit(X_train[[\"u\", \"m\", \"f\"]], ycont_train) # i changed this to work with regression\n",
    "                \n",
    "                prob4 = clf4.predict(X_test[[\"u\", \"m\", \"f\"]])\n",
    "            \n",
    "            elif(cls == \"Log\"):\n",
    "                clf = LogisticRegressionCV()\n",
    "                clf.fit(X_train, y_train)\n",
    "                prob = clf.predict_proba(X_test)[:,1]\n",
    "            elif(cls == \"Xg\"):\n",
    "                clf = GradientBoostingClassifier()\n",
    "                clf.fit(X_train, y_train)\n",
    "                prob = clf.predict_proba(X_test)[:,1] \n",
    "            elif(cls == \"XgBoost\"):\n",
    "                clf =  xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "                #             prob = cl.predict_proba\n",
    "                clf.fit(X_train, ycont_train) # i changed this to work with regression\n",
    "                #score = clf.score(X_test, ycont_test) ## i removed this so that it worked with regression\n",
    "                prob = clf.predict(X_test)\n",
    "\n",
    "            \n",
    "            lab_pred1 = prob1.copy() # i added this to work with regression\n",
    "            lab_pred1=np.where(lab_pred1 >= 0.5, 1,-1)\n",
    "            score1 = accuracy_score(lab_pred1,y_test)\n",
    "            rmse1 = mean_squared_error(ycont_test, prob1)\n",
    "            \n",
    "            lab_pred2 = prob2.copy() # i added this to work with regression\n",
    "            lab_pred2 =np.where(lab_pred2 >= 0.5, 1,-1)\n",
    "            score2 = accuracy_score(lab_pred2,y_test)\n",
    "            rmse2 = mean_squared_error(ycont_test, prob2)\n",
    "            \n",
    "            lab_pred3 = prob3.copy() # i added this to work with regression\n",
    "            lab_pred3=np.where(lab_pred3 >= 0.5, 1,-1)\n",
    "            score3 = accuracy_score(lab_pred3,y_test)\n",
    "            rmse3 = mean_squared_error(ycont_test, prob3)\n",
    "            \n",
    "            lab_pred4 = prob4.copy() # i added this to work with regression\n",
    "            lab_pred4 =np.where(lab_pred4 >= 0.5, 1,-1)\n",
    "            score4 = accuracy_score(lab_pred4,y_test)\n",
    "            rmse4 = mean_squared_error(ycont_test, prob4)            \n",
    "#             accuracyM = scores.mean()\n",
    "#             accuracyS = scores.std()\n",
    "            accM1.append(score1)\n",
    "            accS1.append(rmse1)\n",
    "        \n",
    "            accM2.append(score2)\n",
    "            accS2.append(rmse2)\n",
    "            \n",
    "            accM3.append(score3)\n",
    "            accS3.append(rmse3)\n",
    "            \n",
    "            accM4.append(score4)\n",
    "            accS4.append(rmse4)            \n",
    "            \n",
    "\n",
    "\n",
    "    mS1 = np.mean(accS1)\n",
    "    sS1 = np.std(accS1)\n",
    "    print(\"MSE1: %0.3f (+/- %0.2f)\" % (mS1, sS1))\n",
    "\n",
    "#     return(d)\n",
    "    mA1 = np.mean(accM1)\n",
    "    sA1 = np.std(accM1)\n",
    "    print(\"Accuracy1: %0.3f (+/- %0.2f)\" % (mA1, sA1))\n",
    "    \n",
    "    \n",
    "    mS2 = np.mean(accS2)\n",
    "    sS2 = np.std(accS2)\n",
    "    print(\"MSE2: %0.3f (+/- %0.2f)\" % (mS2, sS2))\n",
    "\n",
    "#     return(d)\n",
    "    mA2 = np.mean(accM2)\n",
    "    sA2 = np.std(accM2)\n",
    "    print(\"Accuracy2: %0.3f (+/- %0.2f)\" % (mA2, sA2))\n",
    "    \n",
    "    \n",
    "    mS3 = np.mean(accS3)\n",
    "    sS3 = np.std(accS3)\n",
    "    print(\"MSE3: %0.3f (+/- %0.2f)\" % (mS3, sS3))\n",
    "\n",
    "#     return(d)\n",
    "    mA3 = np.mean(accM1)\n",
    "    sA3 = np.std(accM1)\n",
    "    print(\"Accuracy3: %0.3f (+/- %0.2f)\" % (mA3, sA3))\n",
    "    \n",
    "    mS4 = np.mean(accS4)\n",
    "    sS4 = np.std(accS4)\n",
    "    print(\"MSE4: %0.3f (+/- %0.2f)\" % (mS4, sS4))\n",
    "\n",
    "#     return(d)\n",
    "    mA4 = np.mean(accM4)\n",
    "    sA4 = np.std(accM4)\n",
    "    print(\"Accuracy4: %0.3f (+/- %0.2f)\" % (mA4, sA4))    \n",
    "    \n",
    "#     print(classification_report(originalclass, predictedclass)) \n",
    "    \n",
    "#     mMed = np.mean(scoresM)\n",
    "#     sMed = np.std(scoresM)\n",
    "#     print(\"MSEmedian: %0.3f (+/- %0.2f)\" % (mMed, sMed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkingSubsetForEnsemble(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE, 10,0.1, \"SVC\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
