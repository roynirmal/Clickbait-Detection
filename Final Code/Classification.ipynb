{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyDictionary import PyDictionary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVR, SVR\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "import operator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, accuracy_score, make_scorer\n",
    "\n",
    "import xgboost\n",
    "from nltk.sentiment import SentimentAnalyzer # sentiment analysis\n",
    "from nltk.tokenize import RegexpTokenizer #tokenization\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # sentiment analysis VADER\n",
    "from nltk.tokenize import sent_tokenize #sentence tokenizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(pd):\n",
    "    featNorm = pd.copy()\n",
    "    for k in range(len(pd.columns)):\n",
    "        m = np.mean(pd.iloc[:,k])\n",
    "        s = np.std(pd.iloc[:,k])\n",
    "        featNorm.iloc[:,k] = (pd.iloc[:,k]-m)/s\n",
    "    featNorm = featNorm.iloc[:,1:k]\n",
    "    return(featNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracyFunction(classifier, featuresel, numFeat,df):\n",
    "    accuracy = getAccuracy(classifier, featuresel, numFeat, df, df.iloc[:,range(len(df.columns)-1)])\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\"Log\",\"SVM\",\"RandomForest\",\"XgBoost\",\"NaiveBayes\"]\n",
    "nrFeats = [10,50,100,150,250,300,350,400,450, 500,550,600]\n",
    "resultsposPatterns=pd.DataFrame(index = nrFeats)\n",
    "for classif in classifiers:\n",
    "    print(classif)\n",
    "    accuracies=[]\n",
    "    for nfeat in nrFeats:\n",
    "        [acc1,acc2]=getAccuracyFunction(classif,\"f_classif\",nfeat,allFClass)\n",
    "        accuracies.append(acc1)\n",
    "        print(nfeat)\n",
    "    resultsposPatterns[classif]=accuracies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE and ACCURACY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posPatterns = pd.read_csv(\"posPatterns_ALL_SVM.csv\")\n",
    "unigrams = pd.read_csv(\"unigrams_select250_Log.csv\")\n",
    "mixed = pd.read_csv(\"mixtureDF_AllFeatures_SVM.csv\") \n",
    "allfeat = pd.read_csv(\"features_barbara_all.csv\")\n",
    "\n",
    "posPatternClass = posPatterns.iloc[:,1:len(posPatterns.columns)]\n",
    "unigramsClass = unigrams.iloc[:,1:len(unigrams.columns)]\n",
    "mixedClass = mixed.iloc[:,1:len(mixed.columns)]\n",
    "allfeatClass = allfeat.iloc[:,1:len(allfeat)]\n",
    "\n",
    "posPatternM = posPatterns.iloc[:,1:len(posPatterns.columns)-1]\n",
    "unigramsM = unigrams.iloc[:,1:len(unigrams.columns)-1]\n",
    "mixedM = mixed.iloc[:,1:len(mixed.columns)-1]\n",
    "allfeatM = allfeat.iloc[:,1:len(allfeat.columns)-1]\n",
    "\n",
    "posPatternM = pd.merge(posPatternM, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "unigramsM = pd.merge(unigramsM, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "mixedM = pd.merge(mixedM, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "allfeatM = pd.merge(allfeatM, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixedClass\n",
    "# content = mixedClass[[\"id\"]].copy()\n",
    "sentiment = mixedClass.iloc[:,0:4]\n",
    "informality = mixedClass[[\"id\"]].copy()\n",
    "sentiment\n",
    "informality[[\"inform_CLScore\", \"inform_RIX\",  \"inform_LIX\", \"inform_fmeas\" ]] = mixedClass[[\"inform_CLScore\", \"inform_RIX\",  \"inform_LIX\", \"inform_fmeas\" ]]\n",
    "content = mixedClass.drop([\"sentimentPostText\", \"sentimentTargetText\", \"difsentimentPostTarget\",\"inform_CLScore\", \"inform_RIX\",  \"inform_LIX\", \"inform_fmeas\"], axis = 1 )\n",
    "content = content.drop([\"truthClass\"], axis = 1)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contentM = pd.merge(content, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "informalityM = pd.merge(informality, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "sentimentM = pd.merge(sentiment, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "\n",
    "contentClass = pd.merge(content, Labels[[\"id\",\"truthClass\"]], on = \"id\")\n",
    "informalityClass = pd.merge(informality, Labels[[\"id\",\"truthClass\"]], on = \"id\")\n",
    "sentimentClass = pd.merge(sentiment, Labels[[\"id\",\"truthClass\"]], on = \"id\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracyVal(classifier, featuresel, numFeat, allfeatures, featurelist = finalFeatures):\n",
    "    #Choose Classifier\n",
    "    if(classifier == \"SVM\"):\n",
    "        clf = SVC()\n",
    "    elif(classifier == \"Log\"):\n",
    "        clf = LogisticRegressionCV()\n",
    "    elif(classifier == \"RandomForest\"):\n",
    "        clf = RandomForestClassifier(criterion = 'entropy')\n",
    "    elif(classifier == \"XgBoost\"):\n",
    "        clf = GradientBoostingClassifier()\n",
    "    elif(classifier == \"NaiveBayes\"):\n",
    "        clf = GaussianNB()\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    # select features we want\n",
    "    x = [col for col in allfeatures if col in featurelist]\n",
    "    x = allfeatures[x]\n",
    "\n",
    "    \n",
    "    featNorm = normalize(x)\n",
    "\n",
    "    y = allfeatures.iloc[:,-1]\n",
    "    \n",
    "    if(featuresel== \"chi\"):\n",
    "        X_new = SelectKBest(chi2, k=numFeat).fit_transform(featNorm, y)\n",
    "    elif(featuresel == \"f_classif\"):\n",
    "        X_new = SelectKBest(f_classif, k=numFeat).fit_transform(featNorm, y)\n",
    "    elif(featuresel == \"mutual_info\"):\n",
    "        X_new = SelectKBest(mutual_info_classif, k=numFeat).fit_transform(featNorm, y)\n",
    "    else:\n",
    "        X_new = featNorm\n",
    "    #Classifier result\n",
    "    X_train = X_new[0:2459,:]\n",
    "    y_train = y[0:2459]\n",
    "    X_test = X_new[2459:,:] \n",
    "    y_test = y[2459:]\n",
    "    clf.fit(X_train, y_train)\n",
    "    accuracy = clf.score(X_test, y_test)\n",
    "\n",
    "    return(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def computeMSE(finalFeaturesMSE, finalFeatures, cross_val, featuresel, numFeat, regressor, param):\n",
    "    \n",
    "    ##set up cross validation\n",
    "    cv_idx = KFold(n=len(varpd), n_folds=cross_val, shuffle=True, random_state=1)\n",
    "    \n",
    "    featNormMSE = normalize(finalFeaturesMSE)\n",
    "    \n",
    "    #only works with nd numpy arrays\n",
    "    y = finalFeaturesMSE.truthMean.values\n",
    "    X = featNormMSE.as_matrix(columns = featNormMSE.columns[1:len(featNormMSE.columns)-1])\n",
    "    yhard = finalFeatures.truthClass.values\n",
    "    \n",
    "    #feature selection\n",
    "    if(featuresel== \"chi\"):\n",
    "        X_new = SelectKBest(chi2, k=numFeat).fit_transform(X, y)\n",
    "    elif(featuresel == \"f_classif\"):\n",
    "        X_new = SelectKBest(f_classif, k=numFeat).fit_transform(X, y)\n",
    "    elif(featuresel == \"mutual_info\"):\n",
    "        X_new = SelectKBest(mutual_info_classif, k=numFeat).fit_transform(X, y)\n",
    "    else:\n",
    "        X_new = X\n",
    "\n",
    "    y_out = pd.DataFrame()\n",
    "    scores = []\n",
    "    acc= []\n",
    "    count = 0\n",
    "    \n",
    "    #cross validation and select classifier/regressor\n",
    "    for train_idx, val_idx in cv_idx:\n",
    "            if(regressor == \"LinearSVR\"):\n",
    "                clr = LinearSVR(C=param, loss='squared_epsilon_insensitive', dual=False, random_state=1)\n",
    "                clr.fit(X_new[train_idx], y[train_idx])\n",
    "                y_pred = clr.predict(X_new[val_idx])           \n",
    "                lab_pred = y_pred.copy() # i added this to work with regression\n",
    "                lab_pred=np.where(lab_pred >= 0.5, 1,-1)\n",
    "                score = accuracy_score(lab_pred,yhard[val_idx])\n",
    "                acc.append(score)\n",
    "\n",
    "            elif(regressor == \"Log\"):\n",
    "                clr = LogisticRegression()\n",
    "                clr.fit(X_new[train_idx], yhard[train_idx])\n",
    "                y_pred =clr.predict_proba(X_new[val_idx])[:,1]\n",
    "            elif(regressor == \"XgBoost\"):\n",
    "                clr = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "                clr.fit(X_new[train_idx], y[train_idx])\n",
    "                y_pred = clr.predict(X_new[val_idx])\n",
    "                lab_pred = y_pred.copy() # i added this to work with regression\n",
    "                lab_pred=np.where(lab_pred >= 0.5, 1,-1)\n",
    "                score = accuracy_score(lab_pred,yhard[val_idx])\n",
    "                acc.append(score)\n",
    "\n",
    "            if(regressor == \"RFreg\"):\n",
    "                clr = RandomForestRegressor(max_depth=param, random_state=0) \n",
    "                \n",
    "                clr.fit(X_new[train_idx], y[train_idx]) \n",
    "                \n",
    "                y_pred = clr.predict(X_new[val_idx])\n",
    "                lab_pred = y_pred.copy() # i added this to work with regression\n",
    "                lab_pred=np.where(lab_pred >= 0.5, 1,-1)\n",
    "                score = accuracy_score(lab_pred,yhard[val_idx])\n",
    "                acc.append(score)\n",
    "            elif(regressor == \"SVC\"):\n",
    "                clr = SVC(probability = True)\n",
    "                clr.fit(X_new[train_idx], yhard[train_idx])\n",
    "                y_pred =clr.predict_proba(X_new[val_idx])[:,1]\n",
    "            elif(regressor == \"Xg\"):\n",
    "                clr = GradientBoostingClassifier()\n",
    "                clr.fit(X_new[train_idx], yhard[train_idx])\n",
    "                y_pred =clr.predict_proba(X_new[val_idx])[:,1]\n",
    "            elif(regressor == \"RF\"):\n",
    "                clr = RandomForestClassifier(criterion = 'entropy')\n",
    "                clr.fit(X_new[train_idx], yhard[train_idx])\n",
    "                y_pred =clr.predict_proba(X_new[val_idx])[:,1]\n",
    "            elif(regressor == \"NB\"):\n",
    "                clr = GaussianNB()\n",
    "                clr.fit(X_new[train_idx], yhard[train_idx])\n",
    "                y_pred =clr.predict_proba(X_new[val_idx])[:,1]\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "            rmse = mean_squared_error(y[val_idx], y_pred)\n",
    "            scores.append(rmse)\n",
    "\n",
    "    m = np.mean(scores)\n",
    "    s = np.std(scores)\n",
    "    print(\"MSE: %0.3f (+/- %0.2f)\" % (m, s))\n",
    "    \n",
    "    mA = np.mean(acc)\n",
    "    sA = np.std(acc)\n",
    "    print(\"Accuracy: %0.3f (+/- %0.2f)\" % (mA, sA))\n",
    "    \n",
    "    return(m , mA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeEnsembleClassifier(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE, cross_val,c, cls):\n",
    "    \n",
    "    ##set up cross validation\n",
    "    cv_idx = KFold(n=len(varpd), n_folds=cross_val, shuffle=True, random_state=1)\n",
    "    \n",
    "    originalclass = []\n",
    "    predictedclass = []\n",
    "    \n",
    "    yclass = finalFeatures.iloc[:,-1]\n",
    "    posPatternMMSE = normalize(posPatternClass)\n",
    "    unigramsMMSE = normalize(unigramsClass)\n",
    "    mixedMMSE = normalize(mixedClass)\n",
    "    finalFeaturesNormMSE = normalize(finalFeatures)\n",
    "    \n",
    "    #only works with nd numpy arrays\n",
    "    \n",
    "    ycont = finalFeaturesMSE.truthMean.values\n",
    "    yp = posPatternClass.truthClass.values\n",
    "    yu = unigramsClass.truthClass.values\n",
    "    ym = mixedClass.truthClass.values\n",
    "    yf = finalFeatures.truthClass.values\n",
    "    \n",
    "    Xp = posPatternMMSE.as_matrix(columns = posPatternMMSE.columns)\n",
    "    Xu = unigramsMMSE.as_matrix(columns = unigramsMMSE.columns)\n",
    "    Xm = mixedMMSE.as_matrix(columns = mixedMMSE.columns)\n",
    "    Xf = finalFeaturesNormMSE.as_matrix(columns = finalFeaturesNormMSE.columns)\n",
    "    \n",
    "\n",
    "    Xp_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xp, yp)\n",
    "    Xu_new = SelectKBest(f_classif, k=250).fit_transform(Xu, yu)\n",
    "    Xm_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xm, ym)\n",
    "    Xf_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xf, yf)\n",
    "    \n",
    "    \n",
    "    y_out = pd.DataFrame()\n",
    "\n",
    "    accM = []\n",
    "    accS = []\n",
    "\n",
    "    count = 0\n",
    "    for train_idx, val_idx in cv_idx:\n",
    "           \n",
    "            # select best classifier for each group\n",
    "            clrp = SVC(probability = True)\n",
    "            clru = LogisticRegressionCV()\n",
    "            clrm = SVC(probability = True)\n",
    "            clrf = LogisticRegressionCV()\n",
    "                           \n",
    "            clrp.fit(Xp_new[train_idx], yp[train_idx])\n",
    "            clru.fit(Xu_new[train_idx], yu[train_idx])\n",
    "            clrm.fit(Xm_new[train_idx], ym[train_idx])\n",
    "            clrf.fit(Xf_new[train_idx], yf[train_idx])\n",
    "            \n",
    "            #calculate posterior probabilities\n",
    "            yp_pred = clrp.predict_proba(Xp_new[val_idx])[:,1]\n",
    "            yp_pred_train = clrp.predict_proba(Xp_new[train_idx])[:,1]\n",
    "\n",
    "            \n",
    "            \n",
    "            yu_pred = clru.predict_proba(Xu_new[val_idx])[:,1]\n",
    "            yu_pred_train = clru.predict_proba(Xu_new[train_idx])[:,1]\n",
    "            \n",
    "            ym_pred = clrm.predict_proba(Xm_new[val_idx])[:,1]\n",
    "            ym_pred_train = clrm.predict_proba(Xm_new[train_idx])[:,1]\n",
    "           \n",
    "            \n",
    "            yf_pred = clrf.predict_proba(Xf_new[val_idx])[:,1]\n",
    "            yf_pred_train = clrf.predict_proba(Xf_new[train_idx])[:,1]\n",
    "            \n",
    "\n",
    "            \n",
    "            d1= {'p':yp_pred.tolist(), 'u':yu_pred.tolist(), 'm':ym_pred.tolist(), 'f':yf_pred.tolist(), 'y': yclass[val_idx].tolist(), 'ycont': ycont[val_idx].tolist()}\n",
    "            d2= {'p':yp_pred_train.tolist(), 'u':yu_pred_train.tolist(), 'm':ym_pred_train.tolist(), 'f':yf_pred_train.tolist(), 'y': yclass[train_idx].tolist(), 'ycont': ycont[train_idx].tolist()}\n",
    "            df1 = pd.DataFrame(d1)\n",
    "            df2 = pd.DataFrame(d2)\n",
    "            \n",
    "            d= pd.concat([df1, df2])\n",
    "            \n",
    "            X_train, y_train, ycont_train = df2[[\"p\", \"u\", \"m\", \"f\"]], df2[\"y\"], df2[\"ycont\"]\n",
    "            #test = d[~msk]\n",
    "            X_test, y_test, ycont_test = df1[[\"p\", \"u\", \"m\", \"f\"]], df1[\"y\"], df1[\"ycont\"]\n",
    "            \n",
    "            #feed into gressor\n",
    "            if(cls == \"RF\"):\n",
    "                clf = RandomForestRegressor(max_depth=c, random_state=0) #i used random forest\n",
    "                #             prob = cl.predict_proba\n",
    "                clf.fit(X_train, ycont_train) \n",
    "                #score = clf.score(X_test, ycont_test) \n",
    "                prob = clf.predict(X_test)\n",
    "            \n",
    "            \n",
    "            elif(cls == \"SVC\"):\n",
    "                clf =  LinearSVR(C=c, loss='squared_epsilon_insensitive', dual=False, random_state=1)\n",
    "                #             prob = cl.predict_proba\n",
    "                clf.fit(X_train, ycont_train) \n",
    "                #score = clf.score(X_test, ycont_test) \n",
    "                prob = clf.predict(X_test)\n",
    "            \n",
    "            \n",
    "            elif(cls == \"Log\"):\n",
    "                clf = LogisticRegressionCV()\n",
    "                clf.fit(X_train, y_train)\n",
    "                prob = clf.predict_proba(X_test)[:,1]\n",
    "            elif(cls == \"Xg\"):\n",
    "                clf = GradientBoostingClassifier()\n",
    "                clf.fit(X_train, y_train)\n",
    "                prob = clf.predict_proba(X_test)[:,1] \n",
    "            elif(cls == \"XgBoost\"):\n",
    "                clf =  xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "                #             prob = cl.predict_proba\n",
    "                clf.fit(X_train, ycont_train) \n",
    "                \n",
    "                prob = clf.predict(X_test)\n",
    "\n",
    "            # calculate accuracy and MSE\n",
    "            lab_pred = prob.copy()\n",
    "            lab_pred=np.where(lab_pred >= 0.5, 1,-1)\n",
    "            score = accuracy_score(lab_pred,y_test)\n",
    "            rmse = mean_squared_error(ycont_test, prob)\n",
    "\n",
    "            \n",
    "            accM.append(score)\n",
    "            accS.append(rmse)\n",
    "            \n",
    "            originalclass.extend(y_test)\n",
    "            predictedclass.extend(lab_pred)\n",
    "\n",
    "\n",
    "    mS = np.mean(accS)\n",
    "    sS = np.std(accS)\n",
    "    print(\"MSE: %0.3f (+/- %0.2f)\" % (mS, sS))\n",
    "\n",
    "#     return(d)\n",
    "    mA = np.mean(accM)\n",
    "    sA = np.std(accM)\n",
    "    print(\"Accuracy: %0.3f (+/- %0.2f)\" % (mA, sA))\n",
    "    \n",
    "    print(classification_report(originalclass, predictedclass)) \n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
