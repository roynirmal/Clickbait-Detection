{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyDictionary import PyDictionary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVR, SVR\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "import operator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer # sentiment analysis\n",
    "from nltk.tokenize import RegexpTokenizer #tokenization\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # sentiment analysis VADER\n",
    "from nltk.tokenize import sent_tokenize #sentence tokenizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary=PyDictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'stopclickbait.json'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##read File\n",
    "fileclick = 'stopclickbait.json'\n",
    "# fileclick = 'train1703.jsonl'\n",
    "fileclick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>postText</th>\n",
       "      <th>truthClass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I Get Bings</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Which TV Female Friend Group Do You Belong In</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The New \"Star Wars: The Force Awakens\" Trailer...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>This Vine Of New York On \"Celebrity Big Brothe...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A Couple Did A Stunning Photo Shoot With Their...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>How To Flirt With Queer Girls Without Making A...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>32 Cute Things To Distract From Your Awkward T...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>If Disney Princesses Were From Florida</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>What's A Quote Or Lyric That Best Describes Yo...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Natalie Dormer And Sam Claflin Play A Game To ...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>16 Perfect Responses To The Indian Patriarchy</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>21 Times I Died During The \"Captain America: C...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>17 Times Kourtney Kardashian Shut Down Her Own...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>Does Coffee Make You Poop</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>Who Is Your Celebrity Ex Based On Your Zodiac</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>17 Hairdresser Struggles Every Black Girl Know...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>Are You More Walter White Or Heisenberg</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>The Most Canadian Groom Ever Left His Wedding ...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>Here's One Really Weird Thing About Butterfree</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>15 Resolutions To Make Good On In 2016</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>What New Thing Should You Try In 2016</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>Zoo Animals Around The World Are Opening Their...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>Tell Us About Yourself(ie): Erica Ash</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>9 Times I Cried</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>21 Vegetarian Dump Dinners For The Crock Pot</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>This Goat Has Been Bullying His Tiger Friend</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>8 Fall Shows To Be Excited About, 10 To Give A...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>Another Round, Episode 25: Stop Telling Women ...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>16 Signs You Are Too Stubborn To Live</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>This Country Singer Makes Music On His Game Bo...</td>\n",
       "      <td>clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31970</th>\n",
       "      <td>31971</td>\n",
       "      <td>Utility Rejects Requests as It Buys Land Taint...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31971</th>\n",
       "      <td>31972</td>\n",
       "      <td>Group claims Fred Thompson lobbied for abortio...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31972</th>\n",
       "      <td>31973</td>\n",
       "      <td>Taiwan's cabinet resigns</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31973</th>\n",
       "      <td>31974</td>\n",
       "      <td>Services Make It Easier to Buy via Cellphone</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31974</th>\n",
       "      <td>31975</td>\n",
       "      <td>40 injured after attacks in Thailand</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31975</th>\n",
       "      <td>31976</td>\n",
       "      <td>Budget Crisis in California Could Claim Landmarks</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31976</th>\n",
       "      <td>31977</td>\n",
       "      <td>Rice angry after abuse of aides in Sudan</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31977</th>\n",
       "      <td>31978</td>\n",
       "      <td>Rugby Union: Ballymore Cup North Queensland ca...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31978</th>\n",
       "      <td>31979</td>\n",
       "      <td>McDonald's restaurant to close all locations i...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31979</th>\n",
       "      <td>31980</td>\n",
       "      <td>Microsoft offers to pay blogger to 'correct' W...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31980</th>\n",
       "      <td>31981</td>\n",
       "      <td>St. Michael and Rice Win Class AA Titles</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31981</th>\n",
       "      <td>31982</td>\n",
       "      <td>Sarkozy says burqa is \"not welcome\" in France</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31982</th>\n",
       "      <td>31983</td>\n",
       "      <td>Congressional Hispanic Caucus blocks vote to i...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31983</th>\n",
       "      <td>31984</td>\n",
       "      <td>US Secret Service discovers third uninvited gu...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31984</th>\n",
       "      <td>31985</td>\n",
       "      <td>Three new dinosaurs discovered in Australia</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31985</th>\n",
       "      <td>31986</td>\n",
       "      <td>Corruption blamed for Papuan rainforest destru...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31986</th>\n",
       "      <td>31987</td>\n",
       "      <td>Scotland's Dario Franchitti wins Indianapolis 500</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31987</th>\n",
       "      <td>31988</td>\n",
       "      <td>Scientists recreating the 1918 flu virus say '...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31988</th>\n",
       "      <td>31989</td>\n",
       "      <td>Aide to Stanford May Shift Plea to  Guilty</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31989</th>\n",
       "      <td>31990</td>\n",
       "      <td>British military secrets leaked on social netw...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31990</th>\n",
       "      <td>31991</td>\n",
       "      <td>Bainimarama sworn in as Fiji caretaker PM</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31991</th>\n",
       "      <td>31992</td>\n",
       "      <td>Iran's Supreme Leader wants religious army</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31992</th>\n",
       "      <td>31993</td>\n",
       "      <td>Albanian girl murdered in tangle of crime</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31993</th>\n",
       "      <td>31994</td>\n",
       "      <td>Pentagon unable to explain 'mystery missile' v...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31994</th>\n",
       "      <td>31995</td>\n",
       "      <td>Blair: G8 leaders announce $50 billion aid inc...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31995</th>\n",
       "      <td>31996</td>\n",
       "      <td>To Make Female Hearts Flutter in Iraq, Throw a...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31996</th>\n",
       "      <td>31997</td>\n",
       "      <td>British Liberal Democrat Patsy Calton, 56, die...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31997</th>\n",
       "      <td>31998</td>\n",
       "      <td>Drone smartphone app to help heart attack vict...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31998</th>\n",
       "      <td>31999</td>\n",
       "      <td>Netanyahu Urges Pope Benedict, in Israel, to D...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31999</th>\n",
       "      <td>32000</td>\n",
       "      <td>Computer Makers Prepare to Stake Bigger Claim ...</td>\n",
       "      <td>no-clickbait</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           postText    truthClass\n",
       "0          1                                 Should I Get Bings     clickbait\n",
       "1          2      Which TV Female Friend Group Do You Belong In     clickbait\n",
       "2          3  The New \"Star Wars: The Force Awakens\" Trailer...     clickbait\n",
       "3          4  This Vine Of New York On \"Celebrity Big Brothe...     clickbait\n",
       "4          5  A Couple Did A Stunning Photo Shoot With Their...     clickbait\n",
       "5          6  How To Flirt With Queer Girls Without Making A...     clickbait\n",
       "6          7  32 Cute Things To Distract From Your Awkward T...     clickbait\n",
       "7          8             If Disney Princesses Were From Florida     clickbait\n",
       "8          9  What's A Quote Or Lyric That Best Describes Yo...     clickbait\n",
       "9         10  Natalie Dormer And Sam Claflin Play A Game To ...     clickbait\n",
       "10        11      16 Perfect Responses To The Indian Patriarchy     clickbait\n",
       "11        12  21 Times I Died During The \"Captain America: C...     clickbait\n",
       "12        13  17 Times Kourtney Kardashian Shut Down Her Own...     clickbait\n",
       "13        14                          Does Coffee Make You Poop     clickbait\n",
       "14        15      Who Is Your Celebrity Ex Based On Your Zodiac     clickbait\n",
       "15        16  17 Hairdresser Struggles Every Black Girl Know...     clickbait\n",
       "16        17            Are You More Walter White Or Heisenberg     clickbait\n",
       "17        18  The Most Canadian Groom Ever Left His Wedding ...     clickbait\n",
       "18        19     Here's One Really Weird Thing About Butterfree     clickbait\n",
       "19        20             15 Resolutions To Make Good On In 2016     clickbait\n",
       "20        21              What New Thing Should You Try In 2016     clickbait\n",
       "21        22  Zoo Animals Around The World Are Opening Their...     clickbait\n",
       "22        23              Tell Us About Yourself(ie): Erica Ash     clickbait\n",
       "23        24                                    9 Times I Cried     clickbait\n",
       "24        25       21 Vegetarian Dump Dinners For The Crock Pot     clickbait\n",
       "25        26       This Goat Has Been Bullying His Tiger Friend     clickbait\n",
       "26        27  8 Fall Shows To Be Excited About, 10 To Give A...     clickbait\n",
       "27        28  Another Round, Episode 25: Stop Telling Women ...     clickbait\n",
       "28        29              16 Signs You Are Too Stubborn To Live     clickbait\n",
       "29        30  This Country Singer Makes Music On His Game Bo...     clickbait\n",
       "...      ...                                                ...           ...\n",
       "31970  31971  Utility Rejects Requests as It Buys Land Taint...  no-clickbait\n",
       "31971  31972  Group claims Fred Thompson lobbied for abortio...  no-clickbait\n",
       "31972  31973                           Taiwan's cabinet resigns  no-clickbait\n",
       "31973  31974       Services Make It Easier to Buy via Cellphone  no-clickbait\n",
       "31974  31975               40 injured after attacks in Thailand  no-clickbait\n",
       "31975  31976  Budget Crisis in California Could Claim Landmarks  no-clickbait\n",
       "31976  31977           Rice angry after abuse of aides in Sudan  no-clickbait\n",
       "31977  31978  Rugby Union: Ballymore Cup North Queensland ca...  no-clickbait\n",
       "31978  31979  McDonald's restaurant to close all locations i...  no-clickbait\n",
       "31979  31980  Microsoft offers to pay blogger to 'correct' W...  no-clickbait\n",
       "31980  31981           St. Michael and Rice Win Class AA Titles  no-clickbait\n",
       "31981  31982      Sarkozy says burqa is \"not welcome\" in France  no-clickbait\n",
       "31982  31983  Congressional Hispanic Caucus blocks vote to i...  no-clickbait\n",
       "31983  31984  US Secret Service discovers third uninvited gu...  no-clickbait\n",
       "31984  31985        Three new dinosaurs discovered in Australia  no-clickbait\n",
       "31985  31986  Corruption blamed for Papuan rainforest destru...  no-clickbait\n",
       "31986  31987  Scotland's Dario Franchitti wins Indianapolis 500  no-clickbait\n",
       "31987  31988  Scientists recreating the 1918 flu virus say '...  no-clickbait\n",
       "31988  31989         Aide to Stanford May Shift Plea to  Guilty  no-clickbait\n",
       "31989  31990  British military secrets leaked on social netw...  no-clickbait\n",
       "31990  31991          Bainimarama sworn in as Fiji caretaker PM  no-clickbait\n",
       "31991  31992         Iran's Supreme Leader wants religious army  no-clickbait\n",
       "31992  31993          Albanian girl murdered in tangle of crime  no-clickbait\n",
       "31993  31994  Pentagon unable to explain 'mystery missile' v...  no-clickbait\n",
       "31994  31995  Blair: G8 leaders announce $50 billion aid inc...  no-clickbait\n",
       "31995  31996  To Make Female Hearts Flutter in Iraq, Throw a...  no-clickbait\n",
       "31996  31997  British Liberal Democrat Patsy Calton, 56, die...  no-clickbait\n",
       "31997  31998  Drone smartphone app to help heart attack vict...  no-clickbait\n",
       "31998  31999  Netanyahu Urges Pope Benedict, in Israel, to D...  no-clickbait\n",
       "31999  32000  Computer Makers Prepare to Stake Bigger Claim ...  no-clickbait\n",
       "\n",
       "[32000 rows x 3 columns]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store file in a variable\n",
    "varfile = pd.read_json(fileclick, lines = True)\n",
    "varfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should I Get Bings\n"
     ]
    }
   ],
   "source": [
    "# np.array([varfile['postText'] if varfile['id'] = 608310377143799808])\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "varpd = pd.DataFrame.from_dict(varfile)\n",
    "\n",
    "print(varpd.postText.iloc[0])\n",
    "\n",
    "\n",
    "##define functions to count number of characters and words. return -1 if there are no words/characters present\n",
    "def numChar(content):\n",
    "    if content is None:\n",
    "        return -1\n",
    "    elif not content:\n",
    "        return -1\n",
    "    elif type(content) is str:\n",
    "        return len(content)\n",
    "    else:\n",
    "        \n",
    "        for i in content :\n",
    "            return len(i)\n",
    "        \n",
    "    \n",
    "def numWords(content):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    if content is None:\n",
    "        return -1\n",
    "    elif not content:\n",
    "        return -1\n",
    "    elif type(content) is str:\n",
    "        content_no_punc = tokenizer.tokenize(content)\n",
    "        if(len(content_no_punc)==0):\n",
    "            return -1\n",
    "        else:\n",
    "            return len(content_no_punc)\n",
    "    else:\n",
    "        content_no_punc = tokenizer.tokenize(content[0])\n",
    "        if(len(content_no_punc)==0):\n",
    "            return -1\n",
    "        else:\n",
    "            return len(content_no_punc)\n",
    "        \n",
    "##similar functions like above. but return 0 instead of -1. we need it for counting difference in characters/words\n",
    "        \n",
    "def numChar0(content):\n",
    "    if content is None:\n",
    "        return 0\n",
    "    elif not content:\n",
    "        return 0\n",
    "    elif type(content) is str:\n",
    "        return len(content)\n",
    "    else:\n",
    "        \n",
    "        for i in content :\n",
    "            return len(i)\n",
    "        \n",
    "    \n",
    "def numWords0(content):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    if content is None:\n",
    "        return 0\n",
    "    elif not content:\n",
    "        return 0\n",
    "    elif type(content) is str:\n",
    "        content_no_punc = tokenizer.tokenize(content)\n",
    "        return len(content_no_punc)\n",
    "    else:\n",
    "        content_no_punc = tokenizer.tokenize(content[0])\n",
    "        return len(content_no_punc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: Number of Characters\n",
    "features1 = varpd[['id']].copy()\n",
    "\n",
    "\n",
    "postlen = pd.DataFrame(columns = [\"postlen\"])\n",
    "# titlelen = pd.DataFrame(columns = [\"titlelen\"])\n",
    "# desclen = pd.DataFrame(columns = [\"desclen\"])\n",
    "# keylen = pd.DataFrame(columns = [\"keylen\"])\n",
    "# caplen = pd.DataFrame(columns = [\"caplen\"])\n",
    "# paralen = pd.DataFrame(columns = [\"paralen\"])\n",
    "\n",
    "for ind in range(len(varpd.index)):\n",
    "    \n",
    "    postlen = postlen.append(pd.DataFrame([numChar(varpd.iloc[ind]['postText'])], columns = [\"postlen\"] ), ignore_index = True)\n",
    "#     titlelen = titlelen.append(pd.DataFrame([numChar(varpd.iloc[ind]['targetTitle'])], columns = [\"titlelen\"] ), ignore_index = True)\n",
    "#     desclen = desclen.append(pd.DataFrame([numChar(varpd.iloc[ind]['targetDescription'])], columns = [\"desclen\"] ), ignore_index = True)\n",
    "#     keylen = keylen.append(pd.DataFrame([numChar(varpd.iloc[ind]['targetKeywords'])], columns = [\"keylen\"] ), ignore_index = True)\n",
    "#     caplen = caplen.append(pd.DataFrame([numChar(varpd.iloc[ind]['targetCaptions'])], columns = [\"caplen\"] ), ignore_index = True)\n",
    "#     paralen = paralen.append(pd.DataFrame([numChar(varpd.iloc[ind]['targetParagraphs'])], columns = [\"paralen\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "# features1  =  pd.concat([features1, postlen, titlelen, desclen, keylen, caplen, paralen], axis=1)\n",
    "features1  =  pd.concat([features1, postlen], axis=1)\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dataframe used for counting features which involves measuring difference of words/character(notice the functions used)\n",
    "\n",
    "features0 = varpd[['id']].copy()\n",
    "\n",
    "\n",
    "postlen0 = pd.DataFrame(columns = [\"postlen\"])\n",
    "# titlelen0 = pd.DataFrame(columns = [\"titlelen\"])\n",
    "# desclen0 = pd.DataFrame(columns = [\"desclen\"])\n",
    "# keylen0 = pd.DataFrame(columns = [\"keylen\"])\n",
    "# caplen0 = pd.DataFrame(columns = [\"caplen\"])\n",
    "# paralen0 = pd.DataFrame(columns = [\"paralen\"])\n",
    "\n",
    "for ind in range(len(varpd.index)):\n",
    "    \n",
    "    postlen0 = postlen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['postText'])], columns = [\"postlen\"] ), ignore_index = True)\n",
    "#     titlelen0 = titlelen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['targetTitle'])], columns = [\"titlelen\"] ), ignore_index = True)\n",
    "#     desclen0 = desclen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['targetDescription'])], columns = [\"desclen\"] ), ignore_index = True)\n",
    "#     keylen0 = keylen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['targetKeywords'])], columns = [\"keylen\"] ), ignore_index = True)\n",
    "#     caplen0 = caplen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['targetCaptions'])], columns = [\"caplen\"] ), ignore_index = True)\n",
    "#     paralen0 = paralen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['targetParagraphs'])], columns = [\"paralen\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "# features0  =  pd.concat([features0, postlen0, titlelen0, desclen0, keylen0, caplen0, paralen0], axis=1)\n",
    "features0  =  pd.concat([features0, postlen0], axis=1)\n",
    "\n",
    "\n",
    "features20 = varpd[['id']].copy()\n",
    "postword0 = pd.DataFrame(columns = [\"postword\"])\n",
    "# titleword0 = pd.DataFrame(columns = [\"titleword\"])\n",
    "# descword0 = pd.DataFrame(columns = [\"descword\"])\n",
    "# keyword0 = pd.DataFrame(columns = [\"keyword\"])\n",
    "# capword0 = pd.DataFrame(columns = [\"capword\"])\n",
    "# paraword0 = pd.DataFrame(columns = [\"paraword\"])\n",
    "\n",
    "for ind in range(len(varpd.index)):\n",
    "    \n",
    "    postword0 = postword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['postText'])], columns = [\"postword\"] ), ignore_index = True)\n",
    "#     titleword0 = titleword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['targetTitle'])], columns = [\"titleword\"] ), ignore_index = True)\n",
    "#     descword0 = descword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['targetDescription'])], columns = [\"descword\"] ), ignore_index = True)\n",
    "#     keyword0 = keyword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['targetKeywords'])], columns = [\"keyword\"] ), ignore_index = True)\n",
    "#     capword0 = capword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['targetCaptions'])], columns = [\"capword\"] ), ignore_index = True)\n",
    "#     paraword0 = paraword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['targetParagraphs'])], columns = [\"paraword\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "# features20  =  pd.concat([features20, postword0, titleword0, descword0, keyword0, capword0, paraword0], axis=1)\n",
    "features20  =  pd.concat([features20, postword0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature: Number of Words\n",
    "features2 = varpd[['id']].copy()\n",
    "postword = pd.DataFrame(columns = [\"postword\"])\n",
    "# titleword = pd.DataFrame(columns = [\"titleword\"])\n",
    "# descword = pd.DataFrame(columns = [\"descword\"])\n",
    "# keyword = pd.DataFrame(columns = [\"keyword\"])\n",
    "# capword = pd.DataFrame(columns = [\"capword\"])\n",
    "# paraword = pd.DataFrame(columns = [\"paraword\"])\n",
    "\n",
    "for ind in range(len(varpd.index)):\n",
    "    \n",
    "    postword = postword.append(pd.DataFrame([numWords(varpd.iloc[ind]['postText'])], columns = [\"postword\"] ), ignore_index = True)\n",
    "#     titleword = titleword.append(pd.DataFrame([numWords(varpd.iloc[ind]['targetTitle'])], columns = [\"titleword\"] ), ignore_index = True)\n",
    "#     descword = descword.append(pd.DataFrame([numWords(varpd.iloc[ind]['targetDescription'])], columns = [\"descword\"] ), ignore_index = True)\n",
    "#     keyword = keyword.append(pd.DataFrame([numWords(varpd.iloc[ind]['targetKeywords'])], columns = [\"keyword\"] ), ignore_index = True)\n",
    "#     capword = capword.append(pd.DataFrame([numWords(varpd.iloc[ind]['targetCaptions'])], columns = [\"capword\"] ), ignore_index = True)\n",
    "#     paraword = paraword.append(pd.DataFrame([numWords(varpd.iloc[ind]['targetParagraphs'])], columns = [\"paraword\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "# features2  =  pd.concat([features2, postword, titleword, descword, keyword, capword, paraword], axis=1)\n",
    "features2  =  pd.concat([features2, postword], axis=1)\n",
    "# features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature: Difference in Number of words\n",
    "# Not suitable for just postTitles\n",
    "features3 = varpd[['id']].copy()\n",
    "\n",
    "for i in range(1,6):\n",
    "    for j in range(i+1,7):\n",
    "        temp = pd.DataFrame(abs(features20.iloc[:,i]-features20.iloc[:,j]), columns = [\"wordDiff\"+str(i)+str(j)])\n",
    "        features3 = pd.concat([features3, temp], axis = 1)\n",
    "\n",
    "        \n",
    "# features3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature: Difference in Number of Characters\n",
    "# Not suitable for just postTitles\n",
    "features4 = varpd[['id']].copy()\n",
    "\n",
    "for i in range(1,6):\n",
    "    for j in range(i+1,7):\n",
    "        temp = pd.DataFrame(abs(features0.iloc[:,i]-features0.iloc[:,j]), columns = [\"charDiff\"+str(i)+str(j)])\n",
    "        features4 = pd.concat([features4, temp], axis = 1)\n",
    "\n",
    "# features4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature: Ratio of number of words\n",
    "# Not suitable for just postTitles\n",
    "features5 = varpd[['id']].copy()\n",
    "\n",
    "for i in range(1,6):\n",
    "    for j in range(i+1,7):\n",
    "        temp = pd.DataFrame(abs(features2.iloc[:,i]/features2.iloc[:,j]), columns = [\"wordRatio\"+str(i)+str(j)])\n",
    "        temp.loc[features2.iloc[:,i] == -1, \"wordRatio\"+str(i)+str(j)] = -1\n",
    "        temp.loc[features2.iloc[:,j] == -1, \"wordRatio\"+str(i)+str(j)] = -1\n",
    "        features5 = pd.concat([features5, temp], axis = 1)\n",
    "\n",
    "# features5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Feature: Ratio of number of Characters\n",
    "# Not suitable for just postTitles\n",
    "features6 = varpd[['id']].copy()\n",
    "\n",
    "for i in range(1,6):\n",
    "    for j in range(i+1,7):\n",
    "        temp = pd.DataFrame(abs(features1.iloc[:,i]/features1.iloc[:,j]), columns = [\"charRatio\"+str(i)+str(j)])\n",
    "        temp.loc[features1.iloc[:,i] == -1, \"charRatio\"+str(i)+str(j)] = -1\n",
    "        temp.loc[features1.iloc[:,j] == -1, \"charRatio\"+str(i)+str(j)] = -1\n",
    "        features6 = pd.concat([features6, temp], axis = 1)\n",
    "\n",
    "# features6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function: Common words Between Article Keywords and Others\n",
    "# Not suitable for just postTitles\n",
    "\n",
    "def commonWords(keyword, content):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    keyword_no_punc = tokenizer.tokenize(keyword)\n",
    "    if(len(keyword_no_punc)==0):\n",
    "        return 0\n",
    "    elif (content is None):\n",
    "        return 0\n",
    "    elif (not content):\n",
    "        return 0\n",
    "    elif type(content) is str:\n",
    "        content_no_punc = tokenizer.tokenize(content)\n",
    "        return len(list(set(keyword_no_punc) & set(content_no_punc)))\n",
    "    else:\n",
    "        content_no_punc = tokenizer.tokenize(content[0])\n",
    "        return len(list(set(keyword_no_punc) & set(content_no_punc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature: Common words Between Article Keywords and Others\n",
    "# Not suitable for just postTitles\n",
    "\n",
    "\n",
    "features7 = varpd[['id']].copy()\n",
    "\n",
    "commonpost = pd.DataFrame(columns = [\"commonpost\"])\n",
    "commontitle = pd.DataFrame(columns = [\"commontitle\"])\n",
    "commondesc = pd.DataFrame(columns = [\"commondesc\"])\n",
    "commoncap = pd.DataFrame(columns = [\"commoncap\"])\n",
    "commonpara = pd.DataFrame(columns = [\"commonpara\"])\n",
    "\n",
    "for ind in range(len(varpd.index)):\n",
    "    \n",
    "    commonpost = commonpost.append(pd.DataFrame([commonWords(varpd.iloc[ind]['targetKeywords'], varpd.iloc[ind]['postText'])], columns = [\"commonpost\"] ), ignore_index = True)\n",
    "    commontitle = commontitle.append(pd.DataFrame([commonWords(varpd.iloc[ind]['targetKeywords'], varpd.iloc[ind]['targetTitle'])], columns = [\"commontitle\"] ), ignore_index = True)\n",
    "    commondesc = commondesc.append(pd.DataFrame([commonWords(varpd.iloc[ind]['targetKeywords'], varpd.iloc[ind]['targetDescription'])], columns = [\"commondesc\"] ), ignore_index = True)\n",
    "    commoncap = commoncap.append(pd.DataFrame([commonWords(varpd.iloc[ind]['targetKeywords'], varpd.iloc[ind]['targetCaptions'])], columns = [\"commoncap\"] ), ignore_index = True)\n",
    "    commonpara = commonpara.append(pd.DataFrame([commonWords(varpd.iloc[ind]['targetKeywords'], varpd.iloc[ind]['targetParagraphs'])], columns = [\"commonpara\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "features7  =  pd.concat([features7, commonpost, commontitle, commondesc, commoncap, commonpara], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature: Presence of an image\n",
    "# Not suitable for just postTitles\n",
    "\n",
    "features8 = varpd[['id']].copy()\n",
    "imagePresent = pd.DataFrame(1, index = np.arange(2459), columns = [\"imagePresent\"])\n",
    "\n",
    "for i in range(len(varpd.index)):\n",
    "    if not varfile.iloc[i,1]:\n",
    "        imagePresent.loc[i, \"imagePresent\"] = 0\n",
    "#np.where( varfile['postMedia'] )\n",
    "#np.arange(2459)\n",
    "#varfile.iloc[:,1]\n",
    "features8  =  pd.concat([features8, imagePresent], axis=1)\n",
    "# features8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function: Number of formal words\n",
    "# Not suitable for just postTitles\n",
    "\n",
    "def formalWords(content):\n",
    "    c=0\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    if content is None:\n",
    "        return 0\n",
    "    elif not content:\n",
    "        return 0\n",
    "    elif type(content) is str:\n",
    "        content_no_punc = tokenizer.tokenize(content)\n",
    "        for word in [x.lower() for x in content_no_punc]:\n",
    "            if word in words.words():\n",
    "                c=c+1\n",
    "        return c\n",
    "    else:\n",
    "        content_no_punc = tokenizer.tokenize(content[0])\n",
    "        for word in [x.lower() for x in content_no_punc]:\n",
    "            if word in words.words():\n",
    "                c=c+1\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Feature: Number of Formal Words\n",
    "# features9 = varpd[['id']].copy()\n",
    "# postform = pd.DataFrame(columns = [\"postform\"])\n",
    "# titleform = pd.DataFrame(columns = [\"titleform\"])\n",
    "# descform = pd.DataFrame(columns = [\"descform\"])\n",
    "# keyform = pd.DataFrame(columns = [\"keyform\"])\n",
    "# capform = pd.DataFrame(columns = [\"capform\"])\n",
    "# paraform = pd.DataFrame(columns = [\"paraform\"])\n",
    "\n",
    "# for ind in range(len(varpd.index)):\n",
    "    \n",
    "#     postform = postform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['postText'])], columns = [\"postform\"] ), ignore_index = True)\n",
    "#     titleform = titleform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['targetTitle'])], columns = [\"titleform\"] ), ignore_index = True)\n",
    "#     descform = descform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['targetDescription'])], columns = [\"descform\"] ), ignore_index = True)\n",
    "#     keyform = keyform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['targetKeywords'])], columns = [\"keyform\"] ), ignore_index = True)\n",
    "#     capform = capform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['targetCaptions'])], columns = [\"capform\"] ), ignore_index = True)\n",
    "#     paraform = paraform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['targetParagraphs'])], columns = [\"paraform\"] ), ignore_index = True)\n",
    "#     if(ind%10 == 0): \n",
    "#         print(ind)\n",
    "    \n",
    "    \n",
    "# features9  =  pd.concat([features9, postform, titleform, descform, keyform, capform, paraform], axis=1)\n",
    "# features9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #Feature: Ratio of number of formal words to total number of words\n",
    "# def formalRatio(formal, total):\n",
    "#     if (total==-1):\n",
    "#         return -1\n",
    "#     else:\n",
    "#         return abs(formal/total)\n",
    "\n",
    "\n",
    "# features10 = varpd[['id']].copy()\n",
    "# postratio = pd.DataFrame(columns = [\"postratio\"])\n",
    "# titleratio = pd.DataFrame(columns = [\"titleratio\"])\n",
    "# descratio = pd.DataFrame(columns = [\"descratio\"])\n",
    "# keyratio = pd.DataFrame(columns = [\"keyratio\"])\n",
    "# capratio = pd.DataFrame(columns = [\"capratio\"])\n",
    "# pararatio = pd.DataFrame(columns = [\"pararatio\"])\n",
    "\n",
    "# for ind in range(len(varpd.index)):\n",
    "    \n",
    "#     postratio = postratio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['postform'], features2.iloc[ind]['postword'] )], columns = [\"postratio\"] ), ignore_index = True)\n",
    "#     titleratio = titleratio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['titleform'],features2.iloc[ind]['titleword'])], columns = [\"titleratio\"] ), ignore_index = True)\n",
    "#     descratio = descratio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['descform'],features2.iloc[ind]['descword'])], columns = [\"descratio\"] ), ignore_index = True)\n",
    "#     keyratio = keyratio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['keyform'],features2.iloc[ind]['keyword'])], columns = [\"keyratio\"] ), ignore_index = True)\n",
    "#     capratio = capratio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['capform'],features2.iloc[ind]['capword'])], columns = [\"capratio\"] ), ignore_index = True)\n",
    "#     pararatio = pararatio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['paraform'],features2.iloc[ind]['paraword'])], columns = [\"pararatio\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "# features10  =  pd.concat([features10, postratio, titleratio, descratio, keyratio, capratio, pararatio], axis=1)\n",
    "# features10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# twenty = fetch_20newsgroups()\n",
    "# twenty.data\n",
    "# Not suitable for just postTitles\n",
    "raw_documents1 = varpd[\"postText\"]\n",
    "temp1 = pd.Series.tolist(raw_documents1)\n",
    "temp4 = [item for items in temp1 for item in items]\n",
    "raw_documents2 = varpd[\"targetParagraphs\"]\n",
    "temp2 = pd.Series.tolist(raw_documents2)\n",
    "temp3 = [' '.join(item) for item in temp2]\n",
    "vocab = temp4 + temp3\n",
    "\n",
    "# newlist\n",
    "tfidf = TfidfVectorizer().fit_transform(vocab, y = None)\n",
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# linear_kernel(tfidf[0:1], tfidf[2459:2459+1]).flatten()\n",
    "# i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosine_similarities = []\n",
    "for i in range(int(len(vocab)/2)):\n",
    "    cosine_similarities.append(linear_kernel(tfidf[i:i+1], tfidf[2459+i:2459+i+1]).flatten())\n",
    "cosine_similarities = np.concatenate(cosine_similarities).ravel().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features12 = varpd[['id']].copy()\n",
    "# Not suitable for just postTitles\n",
    "tfidfcosine = pd.DataFrame(cosine_similarities, columns = [\"tfidfcosine\"])\n",
    "features12 = pd.concat([features12, tfidfcosine], axis = 1) \n",
    "# features12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "finalFeaturesT = pd.merge(features1, features2,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features3,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features4,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features5,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features6,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features7,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features8,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features11,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features12,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features,  on='id')\n",
    "\n",
    "# finalFeatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Getting the Labels\n",
    "# testLabelsFile = 'traintruth1703.jsonl'\n",
    "Labels = varpd[['id', 'truthClass']].copy()\n",
    "\n",
    "Labels.loc[Labels.iloc[:,1] == \"no-clickbait\", \"truthClass\"] = -1\n",
    "Labels.loc[Labels.iloc[:,1] == \"clickbait\", \"truthClass\"] = 1\n",
    "finalFeatures = pd.merge(finalFeaturesT, Labels[[\"id\",\"truthClass\"]], on = \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalFeatures.loc[(finalFeatures['id'] <= 16001) & (finalFeatures['id'] >= 15991)]\n",
    "finalFeatures.to_csv(\"features_nirmal_all_stopclickbait.csv\", index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Final Features with Truth Mean\n",
    "finalFeaturesMSE = pd.merge(finalFeaturesT, Labels[[\"id\",\"truthMean\"]], on = \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize(pd):\n",
    "    featNorm = pd.copy()\n",
    "    for k in range(len(pd.columns)):\n",
    "        m = np.mean(pd.iloc[:,k])\n",
    "        s = np.std(pd.iloc[:,k])\n",
    "        featNorm.iloc[:,k] = (pd.iloc[:,k]-m)/s\n",
    "    featNorm = featNorm.iloc[:,1:k]\n",
    "    return(featNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getAccuracy(classifier, featuresel, numFeat, allfeatures, featurelist = finalFeatures):\n",
    "    #Choose Classifier\n",
    "    if(classifier == \"SVM\"):\n",
    "        clf = SVC()\n",
    "    elif(classifier == \"Log\"):\n",
    "        clf = LogisticRegressionCV()\n",
    "    elif(classifier == \"RandomForest\"):\n",
    "        clf = RandomForestClassifier(criterion = 'entropy')\n",
    "    elif(classifier == \"XgBoost\"):\n",
    "        clf = GradientBoostingClassifier()\n",
    "    elif(classifier == \"NaiveBayes\"):\n",
    "        clf = GaussianNB()\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    # select features we want\n",
    "    x = [col for col in allfeatures if col in featurelist]\n",
    "    x = allfeatures[x]\n",
    "\n",
    "    \n",
    "    featNorm = normalize(x)\n",
    "    #print(featNorm)\n",
    "    #print(featNorm)\n",
    "    # Labels\n",
    "    y = allfeatures.iloc[:,-1]\n",
    "    \n",
    "    if(featuresel== \"chi\"):\n",
    "        X_new = SelectKBest(chi2, k=numFeat).fit_transform(featNorm, y)\n",
    "    elif(featuresel == \"f_classif\"):\n",
    "        X_new = SelectKBest(f_classif, k=numFeat).fit_transform(featNorm, y)\n",
    "    elif(featuresel == \"mutual_info\"):\n",
    "        X_new = SelectKBest(mutual_info_classif, k=numFeat).fit_transform(featNorm, y)\n",
    "    else:\n",
    "        X_new = featNorm\n",
    "    #Classifier result\n",
    "    scores = cross_val_score(clf, X_new, y, cv=10)\n",
    "    accuracy = [scores.mean(), scores.std()]\n",
    "    return(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def useNirmalFunctionEfficiently(classifier, featuresel, numFeat,df):\n",
    "    accuracy = getAccuracy(classifier, featuresel, numFeat, df, df.iloc[:,range(len(df.columns)-1)])\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log\n",
      "10\n",
      "50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k should be >=0, <= n_features; got 100.Use k='all' to return all features.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-a52f89433692>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnfeat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnrFeats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0macc1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0museNirmalFunctionEfficiently\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassif\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"f_classif\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnfeat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinalFeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-03dbe9864a4f>\u001b[0m in \u001b[0;36museNirmalFunctionEfficiently\u001b[1;34m(classifier, featuresel, numFeat, df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0museNirmalFunctionEfficiently\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFeat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-61b68ca8138d>\u001b[0m in \u001b[0;36mgetAccuracy\u001b[1;34m(classifier, featuresel, numFeat, allfeatures, featurelist)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumFeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"f_classif\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_classif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumFeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"mutual_info\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmutual_info_classif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumFeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    346\u001b[0m                             % (self.score_func, type(self.score_func)))\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[0mscore_func_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36m_check_params\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    491\u001b[0m             raise ValueError(\"k should be >=0, <= n_features; got %r.\"\n\u001b[0;32m    492\u001b[0m                              \u001b[1;34m\"Use k='all' to return all features.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m                              % self.k)\n\u001b[0m\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_support_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: k should be >=0, <= n_features; got 100.Use k='all' to return all features."
     ]
    }
   ],
   "source": [
    "classifiers = [\"Log\",\"SVM\",\"RandomForest\",\"XgBoost\",\"NaiveBayes\"]\n",
    "nrFeats = [10,50,100,150,250,300,350,400,450, 500,550,600]\n",
    "resultsposPatterns=pd.DataFrame(index = nrFeats)\n",
    "for classif in classifiers:\n",
    "    print(classif)\n",
    "    accuracies=[]\n",
    "    for nfeat in nrFeats:\n",
    "        [acc1,acc2]=useNirmalFunctionEfficiently(classif,\"f_classif\",nfeat,finalFeatures)\n",
    "        accuracies.append(acc1)\n",
    "        print(nfeat)\n",
    "    resultsposPatterns[classif]=accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultsposPatterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"SVM\", \"none\", 0,  finalFeatures, finalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"Log\",  \"none\", 0, finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"RandomForest\", \"none\", finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"XgBoost\", \"none\", finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"Log\",  \"f_classif\", 40, finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"Log\",  \"f_classif\", 60, finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"Log\",  \"f_classif\", 30, finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"SVM\",  \"mutual_info\", 500, finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(featwithLab.iloc[:,1:91], featwithLab.iloc[:,91], test_size=0.4, random_state=0)\n",
    "\n",
    "# clf = LogisticRegressionCV()\n",
    "# scores = cross_val_score(clf, featwithLab.iloc[:,1:81], featwithLab.iloc[:,81], cv=10)\n",
    "# # clf.fit(X_train, y_train)\n",
    "# # clf.score(X_test, y_test)\n",
    "# print(\"Accuracy: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(featwithLab.iloc[:,1:91], featwithLab.iloc[:,91], test_size=0.4, random_state=0)\n",
    "\n",
    "# clf = GradientBoostingClassifier()\n",
    "# scores = cross_val_score(clf, featwithLab.iloc[:,1:81], featwithLab.iloc[:,81], cv=10)\n",
    "# # clf.fit(X_train, y_train)\n",
    "# # clf.score(X_test, y_test)\n",
    "# print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(featwithLab.iloc[:,1:91], featwithLab.iloc[:,91], test_size=0.4, random_state=0)\n",
    "\n",
    "# clf = RandomForestClassifier(criterion = 'entropy')\n",
    "# scores = cross_val_score(clf, featwithLab.iloc[:,1:81], featwithLab.iloc[:,81], cv=20)\n",
    "# # clf.fit(X_train, y_train)\n",
    "# # clf.score(X_test, y_test)\n",
    "# print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "# import operator\n",
    "\n",
    "# info_gain = dict(zip(list(featwithLab.iloc[:,1:81]), mutual_info_classif( featwithLab.iloc[:,1:81], featwithLab.iloc[:,81])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# for w in sorted(info_gain, key = info_gain.get, reverse = True):\n",
    "#     c=c+1\n",
    "#     print (w, info_gain[w])\n",
    "#     if (c==20):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stemming and Word Cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer # count unigrams\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Retrieve words from postTitles\n",
    "postTitles = varpd[\"postText\"].tolist()\n",
    "\n",
    "def stemmingPostTitles(postTitles,stem):\n",
    "    tokenizer =  TweetTokenizer()\n",
    "    if (stem==\"porter\"):\n",
    "        stemmer = nl.PorterStemmer() # Porter Stemming\n",
    "    elif (stem == \"snowball\"):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "    elif (stem == \"lancaster\"):\n",
    "        stemmer = LancasterStemmer()\n",
    "    \n",
    "    number_pattern = re.compile(r'\\d+')\n",
    "    corpus=[]\n",
    "    for title in postTitles:\n",
    "        title = title[0]\n",
    "        \n",
    "        title=title.lower()\n",
    "        tokens = tokenizer.tokenize(title)\n",
    "        stem_tokens = [stemmer.stem(t) for t in tokens ]\n",
    "        stem_title = \" \".join(stem_tokens)\n",
    "        stem_title = number_pattern.sub('[N]', stem_title)\n",
    "        corpus.append(stem_title)\n",
    "    return(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-4400d9951d98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     wordcloud = WordCloud(\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        #stopwords=stopwords,\n",
    "        max_words=50,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import path\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "text = 'all your base are belong to us all of your base base base'\n",
    "wordcloud = WordCloud(\n",
    "                      relative_scaling = 1.0,\n",
    "                      stopwords = {'to', 'of'} # set or space-separated string\n",
    "                      ).generate(text)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "addLabel = pd.merge(varpd, Labels[[\"id\",\"truthClass\"]], on = \"id\")\n",
    "clickbait = addLabel.loc[addLabel[\"truthClass\"]==1]\n",
    "nonclickbait = addLabel.loc[addLabel[\"truthClass\"]==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_wordcloud(clickbait[\"postText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "show_wordcloud(nonclickbait[\"postText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmedclickbait=stemmingPostTitles(clickbait[\"postText\"].tolist(),\"porter\")\n",
    "show_wordcloud(stemmedclickbait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nonstemmedclickbait=stemmingPostTitles(nonclickbait[\"postText\"].tolist(),\"porter\")\n",
    "show_wordcloud(nonstemmedclickbait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Classifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# normalise features\n",
    "featNorm = finalFeatures.copy()\n",
    "for k in range(len(featNorm.columns)):\n",
    "    m = np.mean(finalFeatures.iloc[:,k])\n",
    "    s = np.std(finalFeatures.iloc[:,k])\n",
    "    featNorm.iloc[:,k] = (finalFeatures.iloc[:,k]-m)/s\n",
    "y = finalFeatures.iloc[:,-1]\n",
    "\n",
    "\n",
    "#X_new = SelectKBest(f_classif, k=500).fit_transform(featNorm.iloc[:,1:len(featNorm.columns)-1], y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(featNorm.iloc[:,1:len(featNorm.columns)-1], finalFeatures.iloc[:,-1], test_size=0.2, random_state=0)\n",
    "clf = LogisticRegressionCV()\n",
    "# scores = cross_val_score(clf, featwithLab.iloc[:,1:81], featwithLab.iloc[:,81], cv=10)\n",
    "clf.fit(X_train, y_train)\n",
    "scores = clf.score(X_test, y_test)\n",
    "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "misclassified = np.where(y_test != clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3098820658804392"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalFeatures.loc[finalFeatures[\"truthClass\"]==1])/len(finalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mis = X_test.iloc[misclassified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mis1 = mis.join(varpd, how= \"inner\")\n",
    "# mis1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pd.merge(mis1, Labels[[\"id\",\"truthClass\"]], on = \"id\").to_csv(\"misclassified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finalFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # MSE\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "posPatterns = pd.read_csv(\"posPatterns_ALL_SVM.csv\")\n",
    "unigrams = pd.read_csv(\"unigrams_select250_Log.csv\")\n",
    "mixed = pd.read_csv(\"mixtureDF_AllFeatures_SVM.csv\") \n",
    "\n",
    "posPatternClass = posPatterns.iloc[:,1:len(posPatterns.columns)]\n",
    "unigramsClass = unigrams.iloc[:,1:len(unigrams.columns)]\n",
    "mixedClass = mixed.iloc[:,1:len(mixed.columns)]\n",
    "\n",
    "posPatternM = posPatterns.iloc[:,1:len(posPatterns.columns)-1]\n",
    "unigramsM = unigrams.iloc[:,1:len(unigrams.columns)-1]\n",
    "mixedM = mixed.iloc[:,1:len(mixed.columns)-1]\n",
    "\n",
    "posPatternM = pd.merge(posPatternM, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "unigramsM = pd.merge(unigramsM, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "mixedM = pd.merge(mixedM, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def computeMSE(finalFeaturesMSE, cross_val, featuresel, numFeat, regressor):\n",
    "    \n",
    "    ##set up cross validation\n",
    "    cv_idx = KFold(n=len(varpd), n_folds=cross_val, shuffle=True, random_state=1)\n",
    "    \n",
    "    featNormMSE = normalize(finalFeaturesMSE)\n",
    "    \n",
    "    #only works with nd numpy arrays\n",
    "    y = finalFeaturesMSE.truthMean.values\n",
    "    X = featNormMSE.as_matrix(columns = featNormMSE.columns[1:len(featNormMSE.columns)-1])\n",
    "    \n",
    "    #feature selection\n",
    "    if(featuresel== \"chi\"):\n",
    "        X_new = SelectKBest(chi2, k=numFeat).fit_transform(X, y)\n",
    "    elif(featuresel == \"f_classif\"):\n",
    "        X_new = SelectKBest(f_classif, k=numFeat).fit_transform(X, y)\n",
    "    elif(featuresel == \"mutual_info\"):\n",
    "        X_new = SelectKBest(mutual_info_classif, k=numFeat).fit_transform(X, y)\n",
    "    else:\n",
    "        X_new = X\n",
    "    print(X_new)\n",
    "    #regression\n",
    "    y_out = pd.DataFrame()\n",
    "    scores = []\n",
    "    count = 0\n",
    "    for train_idx, val_idx in cv_idx:\n",
    "            if(regressor == \"LinearSVR\"):\n",
    "                clr = LinearSVR(C=0.001, loss='squared_epsilon_insensitive', dual=False, random_state=1)\n",
    "            elif(regressor == \"Log\"):\n",
    "                clr = LogisticRegression()\n",
    "            elif(regressor == \"XgBoost\"):\n",
    "                clr = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "            clr.fit(X_new[train_idx], y[train_idx])\n",
    "\n",
    "            y_pred = clr.predict(X_new[val_idx])\n",
    "            y_pred[y_pred < 0] = 0.0\n",
    "            y_pred[y_pred > 1] = 1.0\n",
    "#             y_out[\"c\"+str(count)]=y_pred\n",
    "#             count = count + 1\n",
    "            \n",
    "#     return(y_out.mean(axis=1))\n",
    "            rmse = mean_squared_error(y[val_idx], y_pred)\n",
    "            scores.append(rmse)\n",
    "\n",
    "    m = np.mean(scores)\n",
    "    s = np.std(scores)\n",
    "    print(\"MSE: %0.3f (+/- %0.2f)\" % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeMixedAccuracy(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE, cross_val):\n",
    "    \n",
    "    ##set up cross validation\n",
    "    cv_idx = KFold(n=len(varpd), n_folds=cross_val, shuffle=True, random_state=1)\n",
    "    \n",
    "    yclass = finalFeatures.iloc[:,-1]\n",
    "    posPatternMMSE = normalize(posPatternClass)\n",
    "    unigramsMMSE = normalize(unigramsClass)\n",
    "    mixedMMSE = normalize(mixedClass)\n",
    "    finalFeaturesNormMSE = normalize(finalFeatures)\n",
    "    \n",
    "    #only works with nd numpy arrays\n",
    "    \n",
    "    ycont = finalFeaturesMSE.truthMean.values\n",
    "    yp = posPatternClass.truthClass.values\n",
    "    yu = unigramsClass.truthClass.values\n",
    "    ym = mixedClass.truthClass.values\n",
    "    yf = finalFeatures.truthClass.values\n",
    "    \n",
    "    Xp = posPatternMMSE.as_matrix(columns = posPatternMMSE.columns[1:len(posPatternMMSE.columns)-1])\n",
    "    Xu = unigramsMMSE.as_matrix(columns = unigramsMMSE.columns[1:len(unigramsMMSE.columns)-1])\n",
    "    Xm = mixedMMSE.as_matrix(columns = mixedMMSE.columns[1:len(mixedMMSE.columns)-1])\n",
    "    Xf = finalFeaturesNormMSE.as_matrix(columns = finalFeaturesNormMSE.columns[1:len(finalFeaturesNormMSE.columns)-1])\n",
    "    \n",
    "\n",
    "    Xp_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xp, yp)\n",
    "    Xu_new = SelectKBest(f_classif, k=250).fit_transform(Xu, yu)\n",
    "    Xm_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xm, ym)\n",
    "    Xf_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xf, yf)\n",
    "    \n",
    "    \n",
    "    y_out = pd.DataFrame()\n",
    "    scores = []\n",
    "    acc = []\n",
    "    scoresM = []\n",
    "    count = 0\n",
    "    for train_idx, val_idx in cv_idx:\n",
    "           \n",
    "            clrp = SVC(probability = True)\n",
    "            clru = LogisticRegression()\n",
    "            clrm = SVC(probability = True)\n",
    "            clrf = LogisticRegression()\n",
    "                           \n",
    "            clrp.fit(Xp_new[train_idx], yp[train_idx])\n",
    "            clru.fit(Xu_new[train_idx], yu[train_idx])\n",
    "            clrm.fit(Xm_new[train_idx], ym[train_idx])\n",
    "            clrf.fit(Xf_new[train_idx], yf[train_idx])\n",
    "\n",
    "            yp_pred = clrp.predict_proba(Xp_new[val_idx])[:,1]\n",
    "            \n",
    "            \n",
    "            yu_pred = clru.predict_proba(Xu_new[val_idx])[:,1]\n",
    "            \n",
    "            \n",
    "            ym_pred = clrm.predict_proba(Xm_new[val_idx])[:,1]\n",
    "           \n",
    "            \n",
    "            yf_pred = clrf.predict_proba(Xf_new[val_idx])[:,1]\n",
    "            \n",
    "#             y_out[\"c\"+str(count)]=y_pred\n",
    "#             count = count + 1\n",
    "            \n",
    "            d= {'p':yp_pred.tolist(), 'u':yu_pred.tolist(), 'm':ym_pred.tolist(), 'f':yf_pred.tolist()}\n",
    "            y_pred_final = pd.DataFrame(d)\n",
    "            y_pred_final[\"mean\"] = y_pred_final.mean(axis=1)\n",
    "            y_pred_final[\"median\"] = y_pred_final.iloc[:,0:4].median(axis=1)\n",
    "            y_pred_final['class'] = np.where(y_pred_final['mean'] >= 0.5, 1,-1)\n",
    "#             y_pred_final['class'] = np.where(y_pred_final['median'] >= 0.5, 1,-1)\n",
    "            \n",
    "            rmse = mean_squared_error(ycont[val_idx], y_pred_final[\"mean\"])\n",
    "            scores.append(rmse)\n",
    "            \n",
    "            rmsemed = mean_squared_error(ycont[val_idx], y_pred_final[\"median\"])\n",
    "            scoresM.append(rmsemed)\n",
    "            \n",
    "            accuracy = accuracy_score(yclass[val_idx], y_pred_final[\"class\"])\n",
    "            acc.append(accuracy)\n",
    "    \n",
    "#             rmse = mean_squared_error(y[val_idx], y_pred)\n",
    "#             scores.append(rmse)\n",
    "\n",
    "    mM = np.mean(scores)\n",
    "    sM = np.std(scores)\n",
    "    print(\"MSE: %0.3f (+/- %0.2f)\" % (mM, sM))\n",
    "#     return(y_pred_final)\n",
    "    mA = np.mean(acc)\n",
    "    sA = np.std(acc)\n",
    "    print(\"Accuracy: %0.3f (+/- %0.2f)\" % (mA, sA))\n",
    "    \n",
    "    mMed = np.mean(scoresM)\n",
    "    sMed = np.std(scoresM)\n",
    "    print(\"MSEmedian: %0.3f (+/- %0.2f)\" % (mMed, sMed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeMixedAccuracyLog(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE, cross_val,tdepth):\n",
    "    \n",
    "    ##set up cross validation\n",
    "    cv_idx = KFold(n=len(varpd), n_folds=cross_val, shuffle=True, random_state=1)\n",
    "    \n",
    "    yclass = finalFeatures.iloc[:,-1]\n",
    "    posPatternMMSE = normalize(posPatternClass)\n",
    "    unigramsMMSE = normalize(unigramsClass)\n",
    "    mixedMMSE = normalize(mixedClass)\n",
    "    finalFeaturesNormMSE = normalize(finalFeatures)\n",
    "    \n",
    "    #only works with nd numpy arrays\n",
    "    \n",
    "    ycont = finalFeaturesMSE.truthMean.values\n",
    "    yp = posPatternClass.truthClass.values\n",
    "    yu = unigramsClass.truthClass.values\n",
    "    ym = mixedClass.truthClass.values\n",
    "    yf = finalFeatures.truthClass.values\n",
    "    \n",
    "    Xp = posPatternMMSE.as_matrix(columns = posPatternMMSE.columns)\n",
    "    Xu = unigramsMMSE.as_matrix(columns = unigramsMMSE.columns)\n",
    "    Xm = mixedMMSE.as_matrix(columns = mixedMMSE.columns)\n",
    "    Xf = finalFeaturesNormMSE.as_matrix(columns = finalFeaturesNormMSE.columns)\n",
    "    \n",
    "\n",
    "    Xp_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xp, yp)\n",
    "    Xu_new = SelectKBest(f_classif, k=250).fit_transform(Xu, yu)\n",
    "    Xm_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xm, ym)\n",
    "    Xf_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xf, yf)\n",
    "    \n",
    "    \n",
    "    y_out = pd.DataFrame()\n",
    "#     scores = []\n",
    "    accM = []\n",
    "    accS = []\n",
    "#     scoresM = []\n",
    "    count = 0\n",
    "    for train_idx, val_idx in cv_idx:\n",
    "           \n",
    "            clrp = SVC(probability = True)\n",
    "            clru = LogisticRegression()\n",
    "            clrm = SVC(probability = True)\n",
    "            clrf = LogisticRegression()\n",
    "                           \n",
    "            clrp.fit(Xp_new[train_idx], yp[train_idx])\n",
    "            clru.fit(Xu_new[train_idx], yu[train_idx])\n",
    "            clrm.fit(Xm_new[train_idx], ym[train_idx])\n",
    "            clrf.fit(Xf_new[train_idx], yf[train_idx])\n",
    "\n",
    "            yp_pred = clrp.predict_proba(Xp_new[val_idx])[:,1]\n",
    "            yp_pred_train = clrp.predict_proba(Xp_new[train_idx])[:,1]\n",
    "            \n",
    "            \n",
    "            yu_pred = clru.predict_proba(Xu_new[val_idx])[:,1]\n",
    "            yu_pred_train = clru.predict_proba(Xu_new[train_idx])[:,1]\n",
    "            \n",
    "            ym_pred = clrm.predict_proba(Xm_new[val_idx])[:,1]\n",
    "            ym_pred_train = clrm.predict_proba(Xm_new[train_idx])[:,1]\n",
    "           \n",
    "            \n",
    "            yf_pred = clrf.predict_proba(Xf_new[val_idx])[:,1]\n",
    "            yf_pred_train = clrf.predict_proba(Xf_new[train_idx])[:,1]\n",
    "            \n",
    "#             y_out[\"c\"+str(count)]=y_pred\n",
    "#             count = count + 1\n",
    "            \n",
    "            d1= {'p':yp_pred.tolist(), 'u':yu_pred.tolist(), 'm':ym_pred.tolist(), 'f':yf_pred.tolist(), 'y': yclass[val_idx].tolist(), 'ycont': ycont[val_idx].tolist()}\n",
    "            d2= {'p':yp_pred_train.tolist(), 'u':yu_pred_train.tolist(), 'm':ym_pred_train.tolist(), 'f':yf_pred_train.tolist(), 'y': yclass[train_idx].tolist(), 'ycont': ycont[train_idx].tolist()}\n",
    "            df1 = pd.DataFrame(d1)\n",
    "            df2 = pd.DataFrame(d2)\n",
    "            \n",
    "            d= pd.concat([df1, df2])\n",
    "            \n",
    "            clf = RandomForestRegressor(max_depth=tdepth, random_state=0) #i used random forest\n",
    "            \n",
    "#             scores = cross_val_score(clf, d[[\"p\", \"u\", \"m\", \"f\"]], d[\"y\"], cv=10)\n",
    "            #rand = pd.DataFrame(np.random.randn(2459, 2))\n",
    "\n",
    "            #msk = np.random.rand(len(rand)) < 0.8\n",
    "\n",
    "            #train = d[msk]\n",
    "            X_train, y_train, ycont_train = df2[[\"p\", \"u\", \"m\", \"f\"]], df2[\"y\"], df2[\"ycont\"]\n",
    "            #test = d[~msk]\n",
    "            X_test, y_test, ycont_test = df1[[\"p\", \"u\", \"m\", \"f\"]], df1[\"y\"], df1[\"ycont\"]\n",
    "#             X_train, X_test, y_train, y_test = train_test_split( d[[\"p\", \"u\", \"m\", \"f\"]],  d[\"y\"], test_size=0.4, random_state=0)\n",
    "#             prob = cl.predict_proba\n",
    "            clf.fit(X_train, ycont_train) # i changed this to work with regression\n",
    "            #score = clf.score(X_test, ycont_test) ## i removed this so that it worked with regression\n",
    "            prob = clf.predict(X_test)\n",
    "            \n",
    "            lab_pred = prob.copy() # i added this to work with regression\n",
    "            lab_pred=np.where(lab_pred >= 0.5, 1,-1)\n",
    "            score = accuracy_score(lab_pred,y_test)\n",
    "            rmse = mean_squared_error(ycont_test, prob)\n",
    "#             accuracyM = scores.mean()\n",
    "#             accuracyS = scores.std()\n",
    "            accM.append(score)\n",
    "            accS.append(rmse)\n",
    "#             y1 = { 'y': yclass[val_idx].tolist()}\n",
    "#             y2 = { 'y': yclass[train_idx].tolist()}\n",
    "#             yf1 = pd.DataFrame(y1)\n",
    "#             yf2 = pd.DataFrame(y2)\n",
    "#             ylab = pd.concat([y1, y2]) \n",
    "            \n",
    "#             y_pred_final = pd.DataFrame(d)\n",
    "#             y_pred_final = pd.DataFrame(d)\n",
    "#             y_pred_final[\"mean\"] = y_pred_final.mean(axis=1)\n",
    "#             y_pred_final[\"median\"] = y_pred_final.iloc[:,0:4].median(axis=1)\n",
    "#             y_pred_final['class'] = np.where(y_pred_final['mean'] >= 0.5, 1,-1)\n",
    "# #             y_pred_final['class'] = np.where(y_pred_final['median'] >= 0.5, 1,-1)\n",
    "            \n",
    "#             rmse = mean_squared_error(ycont[val_idx], y_pred_final[\"mean\"])\n",
    "#             scores.append(rmse)\n",
    "            \n",
    "#             rmsemed = mean_squared_error(ycont[val_idx], y_pred_final[\"median\"])\n",
    "#             scoresM.append(rmsemed)\n",
    "            \n",
    "#             accuracy = accuracy_score(yclass[val_idx], y_pred_final[\"class\"])\n",
    "#             acc.append(accuracy)\n",
    "    \n",
    "#             rmse = mean_squared_error(y[val_idx], y_pred)\n",
    "#             scores.append(rmse)\n",
    "\n",
    "    mS = np.mean(accS)\n",
    "    sS = np.std(accS)\n",
    "    print(\"MSE: %0.3f (+/- %0.2f)\" % (mS, sS))\n",
    "\n",
    "#     return(d)\n",
    "    mA = np.mean(accM)\n",
    "    sA = np.std(accM)\n",
    "    print(\"Accuracy: %0.3f (+/- %0.2f)\" % (mA, sA))\n",
    "    \n",
    "#     mMed = np.mean(scoresM)\n",
    "#     sMed = np.std(scoresM)\n",
    "#     print(\"MSEmedian: %0.3f (+/- %0.2f)\" % (mMed, sMed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.cross_validation.KFold(n=2459, n_folds=2, shuffle=True, random_state=1)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "MSE: 0.045 (+/- 0.00)\n",
      "Accuracy: 0.749 (+/- 0.03)\n",
      "3\n",
      "MSE: 0.041 (+/- 0.00)\n",
      "Accuracy: 0.769 (+/- 0.03)\n",
      "4\n",
      "MSE: 0.039 (+/- 0.00)\n",
      "Accuracy: 0.776 (+/- 0.03)\n",
      "5\n",
      "MSE: 0.039 (+/- 0.00)\n",
      "Accuracy: 0.779 (+/- 0.03)\n",
      "6\n",
      "MSE: 0.039 (+/- 0.00)\n",
      "Accuracy: 0.782 (+/- 0.03)\n",
      "7\n",
      "MSE: 0.039 (+/- 0.00)\n",
      "Accuracy: 0.771 (+/- 0.02)\n",
      "8\n",
      "MSE: 0.040 (+/- 0.00)\n",
      "Accuracy: 0.770 (+/- 0.02)\n",
      "9\n",
      "MSE: 0.040 (+/- 0.00)\n",
      "Accuracy: 0.770 (+/- 0.02)\n",
      "10\n",
      "MSE: 0.040 (+/- 0.00)\n",
      "Accuracy: 0.771 (+/- 0.02)\n",
      "15\n",
      "MSE: 0.041 (+/- 0.00)\n",
      "Accuracy: 0.767 (+/- 0.02)\n",
      "20\n",
      "MSE: 0.042 (+/- 0.00)\n",
      "Accuracy: 0.762 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "# #only works with nd numpy arrays\n",
    "# posPatternMMSE = normalize(posPatternClass)\n",
    "# y = posPatternClass.truthClass.values\n",
    "# X = posPatternMMSE.as_matrix(columns = posPatternMMSE.columns[1:len(posPatternMMSE.columns)-1])\n",
    "\n",
    "# X_new = SelectKBest(f_classif, k=\"all\").fit_transform(X, y)\n",
    "for tdepth in [2,3,4,5,6,7,8,9,10,15,20]:\n",
    "    print(tdepth)\n",
    "    computeMixedAccuracyLog(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE,10,tdepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82751754 -1.33042654 -0.99268915 ... -0.24765905 -0.20373472\n",
      "  -0.19714909]\n",
      " [ 0.82751754 -1.33042654  0.04339855 ... -0.24765905 -0.20373472\n",
      "  -0.19714909]\n",
      " [-1.59062344 -0.34384063  0.04339855 ... -0.24765905 -0.20373472\n",
      "  -0.19714909]\n",
      " ...\n",
      " [-0.98608819 -0.34384063 -0.99268915 ... -0.24765905 -0.20373472\n",
      "  -0.19714909]\n",
      " [ 0.2229823   1.62933117 -0.99268915 ... -0.24765905 -0.20373472\n",
      "  -0.19714909]\n",
      " [-0.38155295 -1.33042654  0.04339855 ...  3.92353005 -0.20373472\n",
      "  -0.19714909]]\n",
      "MSE: 0.054 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# ypredict = pd.DataFrame()\n",
    "# ypredict[\"y1\"] = computeMSE(posPatternM, 10, \"f_Classif\",\"all\", \"LinearSVR\")\n",
    "# ypredict[\"y2\"] = computeMSE(unigramsM, 10, \"f_Classif\",\"all\" ,\"Log\")\n",
    "# ypredict[\"y3\"] = computeMSE(mixedM, 10, \"f_Classif\",\"all\", \"LinearSVR\")\n",
    "# ypredict[\"y4\"] = computeMSE(finalFeaturesM, 10, \"f_Classif\",\"all\" ,\"Log\")\n",
    "\n",
    "# y_pred = ypredict.mean(axis =1)\n",
    "# rmse = mean_squared_error(finalFeaturesMSE.truthMean.values, y_pred.values)\n",
    "computeMSE(posPatternM, 10, \"f_Classif\",\"all\", \"LinearSVR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allFeatures=pd.merge(posPatternClass[posPatternClass.columns[0:len(posPatternClass.columns)-1]],unigramsClass[unigramsClass.columns[0:len(unigramsClass.columns)-1]], on=[\"id\"])\n",
    "allFeatures=pd.merge(allFeatures,mixedClass[mixedClass.columns[0:len(mixedClass.columns)-1]], on=[\"id\"])\n",
    "allFeatures=pd.merge(allFeatures,finalFeaturesMSE, on=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8275175413279654 -1.3304265357116412 -0.9926891489824088 ...\n",
      "  4.374236300204959 0.2994538073821884 -1.3933368436270321]\n",
      " [0.8275175413279654 -1.3304265357116412 0.04339854938250764 ...\n",
      "  -0.4353280201673156 -0.17838974378770206 0.7177015411412194]\n",
      " [-1.5906234380249364 -0.34384063362631984 0.04339854938250764 ...\n",
      "  -0.4353280201673156 -0.6562332949575925 -1.3933368436270321]\n",
      " ...\n",
      " [-0.986088193186711 -0.34384063362631984 -0.9926891489824088 ...\n",
      "  -0.4353280201673156 -0.6562332949575925 0.7177015411412194]\n",
      " [0.2229822964897399 1.6293311705443227 -0.9926891489824088 ...\n",
      "  0.25175259702872366 0.7772973585520789 -1.3933368436270321]\n",
      " [-0.3815529483484855 -1.3304265357116412 0.04339854938250764 ...\n",
      "  -0.4353280201673156 -0.6562332949575925 0.7177015411412194]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-041ce6fce86d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomputeMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"featureself_classif\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Log\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-105-78b00599575f>\u001b[0m in \u001b[0;36mcomputeMSE\u001b[1;34m(finalFeaturesMSE, cross_val, featuresel, numFeat, regressor)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 clr = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n\u001b[0;32m     35\u001b[0m                            colsample_bytree=1, max_depth=7)\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mclr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m   1216\u001b[0m                          order=\"C\")\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "computeMSE(allFeatures, 10, \"featureself_classif\", 250, \"Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log yields accuracy of:0.6945906779814721\n",
      "SVM yields accuracy of:0.7031355862182929\n",
      "RandomForest yields accuracy of:0.690955329895363\n",
      "XgBoost yields accuracy of:0.7010964290920279\n",
      "NaiveBayes yields accuracy of:0.6738621009774504\n"
     ]
    }
   ],
   "source": [
    "features2withLabel = pd.merge(features2, Labels[[\"id\",\"truthClass\"]], on = \"id\")\n",
    "#Pick classifier for informality\n",
    "classifiers = [\"Log\",\"SVM\",\"RandomForest\",\"XgBoost\",\"NaiveBayes\"]\n",
    "for classif in classifiers:\n",
    "    [acc1,acc2]=useNirmalFunctionEfficiently(classif,\"f_classif\",\"all\",features2withLabel)\n",
    "    print(classif+\" yields accuracy of:\"+str(acc1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log yields accuracy of:0.7031504921510423\n",
      "SVM yields accuracy of:0.7059927022057959\n",
      "RandomForest yields accuracy of:0.6881245192853481\n",
      "XgBoost yields accuracy of:0.7019292938860149\n",
      "NaiveBayes yields accuracy of:0.6738604552120049\n"
     ]
    }
   ],
   "source": [
    "features1withLabel = pd.merge(features1, Labels[[\"id\",\"truthClass\"]], on = \"id\")\n",
    "#Pick classifier for informality\n",
    "classifiers = [\"Log\",\"SVM\",\"RandomForest\",\"XgBoost\",\"NaiveBayes\"]\n",
    "for classif in classifiers:\n",
    "    [acc1,acc2]=useNirmalFunctionEfficiently(classif,\"f_classif\",\"all\",features1withLabel)\n",
    "    print(classif+\" yields accuracy of:\"+str(acc1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
