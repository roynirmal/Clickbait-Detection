{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk as nl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyDictionary import PyDictionary\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC, LinearSVR, SVR\n",
    "from sklearn.feature_selection import chi2, f_classif, mutual_info_classif\n",
    "import operator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from nltk.sentiment import SentimentAnalyzer # sentiment analysis\n",
    "from nltk.tokenize import RegexpTokenizer #tokenization\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer # sentiment analysis VADER\n",
    "from nltk.tokenize import sent_tokenize #sentence tokenizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary=PyDictionary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train1703.jsonl'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##read File\n",
    "fileclick = 'train1703.jsonl'\n",
    "fileclick\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    " from nltk.corpus import words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>postMedia</th>\n",
       "      <th>postText</th>\n",
       "      <th>postTimestamp</th>\n",
       "      <th>targetCaptions</th>\n",
       "      <th>targetDescription</th>\n",
       "      <th>targetKeywords</th>\n",
       "      <th>targetParagraphs</th>\n",
       "      <th>targetTitle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>608310377143799808</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Apple's iOS 9 'App thinning' feature will giv...</td>\n",
       "      <td>Tue Jun 09 16:31:10 +0000 2015</td>\n",
       "      <td>['App thinning' will be supported on Apple's i...</td>\n",
       "      <td>'App thinning' will be supported on Apple's iO...</td>\n",
       "      <td>Apple,gives,gigabytes,iOS,9,app,thinning,featu...</td>\n",
       "      <td>[Paying for a 64GB phone only to discover that...</td>\n",
       "      <td>Apple gives back gigabytes: iOS 9 'app thinnin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>609297109095972864</td>\n",
       "      <td>[media/609297109095972864.jpg]</td>\n",
       "      <td>[RT @kenbrown12: Emerging market investors are...</td>\n",
       "      <td>Fri Jun 12 09:52:05 +0000 2015</td>\n",
       "      <td>[Stocks Fall as Investors Watch Central Banks,...</td>\n",
       "      <td>Global investors have yanked $9.3 billion from...</td>\n",
       "      <td>emerging market,emerging markets,em flows,em i...</td>\n",
       "      <td>[Emerging markets are out of favor., Global in...</td>\n",
       "      <td>Emerging Markets Suffer Largest Outflow in Sev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>609504474621612032</td>\n",
       "      <td>[]</td>\n",
       "      <td>[U.S. Soccer should start answering tough ques...</td>\n",
       "      <td>Fri Jun 12 23:36:05 +0000 2015</td>\n",
       "      <td>[US to vote for Ali in FIFA election and not B...</td>\n",
       "      <td>A U.S. Senator's scathing letter questioned U....</td>\n",
       "      <td></td>\n",
       "      <td>[WINNIPEG, Manitoba â€“ The bubble U.S. Soccer...</td>\n",
       "      <td>U.S. Soccer should start answering tough quest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>609748367049105408</td>\n",
       "      <td>[]</td>\n",
       "      <td>[How theme parks like Disney World left the mi...</td>\n",
       "      <td>Sat Jun 13 15:45:13 +0000 2015</td>\n",
       "      <td>[Some 1,000 persons turned out in Albuquerque,...</td>\n",
       "      <td>America's top family vacation spots, like the ...</td>\n",
       "      <td>disney, disney world, disney ticket prices, di...</td>\n",
       "      <td>[When Walt Disney World opened in an Orlando s...</td>\n",
       "      <td>How theme parks like Disney World left the mid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>608688782821453824</td>\n",
       "      <td>[media/608688782821453825.jpg]</td>\n",
       "      <td>[Could light bulbs hurt your health? One compa...</td>\n",
       "      <td>Wed Jun 10 17:34:49 +0000 2015</td>\n",
       "      <td>[Electric lights have made the world safer and...</td>\n",
       "      <td>One company will put a health notice on all th...</td>\n",
       "      <td>health, Should there be warning labels on your...</td>\n",
       "      <td>[(CNN)The light bulb always makes the world's ...</td>\n",
       "      <td>Warning labels on your light bulbs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>609551038983475200</td>\n",
       "      <td>[media/609551038983475201.png]</td>\n",
       "      <td>[13 classic â€™00s songs that were actually me...</td>\n",
       "      <td>Sat Jun 13 02:41:07 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[One artistâ€™s trash is anotherâ€™s No. 1 sin...</td>\n",
       "      <td>13 Classic â€™00s Songs That Were Actually Mea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>609447408955719680</td>\n",
       "      <td>[media/609447408955719681.jpg]</td>\n",
       "      <td>[Dez Bryant is reportedly considering skipping...</td>\n",
       "      <td>Fri Jun 12 19:49:19 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>Wide receiver  Dez Bryant  has signed a new,...</td>\n",
       "      <td>Football, NFL, NFC East, Dallas Cowboys, Dez B...</td>\n",
       "      <td>[Wide receiver Dez BryantÂ has signed a new, l...</td>\n",
       "      <td>Dez Bryant Contract: Latest News, Rumors, Spec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>609027430624288768</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Pregnant mother of 12 accused of keeping kids...</td>\n",
       "      <td>Thu Jun 11 16:00:29 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>A pregnant mother of 12 is due in court on chi...</td>\n",
       "      <td></td>\n",
       "      <td>[TULSA, Okla. â€“ Â A pregnant mother of 12 is...</td>\n",
       "      <td>Pregnant mother of 12 accused of keeping kids ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>608229011572068352</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RT @fionamatthias: 10 ways the expat life Is ...</td>\n",
       "      <td>Tue Jun 09 11:07:51 +0000 2015</td>\n",
       "      <td>[Scotland to Seek Second Independence Referend...</td>\n",
       "      <td>Thereâ€™s no autopilot when you're an expat li...</td>\n",
       "      <td>adventure,Alienation,Bangkok,Culture,Culture S...</td>\n",
       "      <td>[Thereâ€™s no autopilot when you live abroad. ...</td>\n",
       "      <td>10 Ways the Expat Life Is Like a Continual Esp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>609046214554755072</td>\n",
       "      <td>[media/609046214554755073.jpg]</td>\n",
       "      <td>[House #GOP plans two days of debate, Friday s...</td>\n",
       "      <td>Thu Jun 11 17:15:07 +0000 2015</td>\n",
       "      <td>[Obama, Chairman Paul Ryan and Republicans act...</td>\n",
       "      <td>House Republican leaders have planned for a tw...</td>\n",
       "      <td></td>\n",
       "      <td>[House Republican leaders have planned for a t...</td>\n",
       "      <td>House GOP plans two days of debate, Friday sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>609738211183751168</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Azeri government behind foreign media ban, sa...</td>\n",
       "      <td>Sat Jun 13 15:04:52 +0000 2015</td>\n",
       "      <td>[European Games]</td>\n",
       "      <td>Organisers of the European Games in Azerbaijan...</td>\n",
       "      <td>European Games 2015,Sport,Azerbaijan,World news</td>\n",
       "      <td>[A decision to ban some foreign media from att...</td>\n",
       "      <td>Azeri government behind foreign media ban, say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>609762671554990080</td>\n",
       "      <td>[media/609762671554990080.jpg]</td>\n",
       "      <td>[Only one in three of us complain when we are ...</td>\n",
       "      <td>Sat Jun 13 16:42:04 +0000 2015</td>\n",
       "      <td>[nhs.jpg]</td>\n",
       "      <td>Only one in three people who are unhappy with ...</td>\n",
       "      <td>, Health News, Health &amp; Families, Lifestyle</td>\n",
       "      <td>[Research found that while 90 per cent believe...</td>\n",
       "      <td>Only one in three unhappy NHS patients actuall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>608980783924178944</td>\n",
       "      <td>[]</td>\n",
       "      <td>[An open letter to Jerry Seinfeld from a \"poli...</td>\n",
       "      <td>Thu Jun 11 12:55:07 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>Dear Jerry Seinfeld,\\n\\nRecently, I've</td>\n",
       "      <td>Jerry Seinfeld,Amy Schumer,Louis CK,comedy,col...</td>\n",
       "      <td>[Dear Jerry Seinfeld,, Recently, I've heard ab...</td>\n",
       "      <td>An Open Letter to Jerry Seinfeld from a 'Polit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>609998126737395712</td>\n",
       "      <td>[media/609998126737395712.jpg]</td>\n",
       "      <td>[Britain forced to withdraw spies after Russia...</td>\n",
       "      <td>Sun Jun 14 08:17:41 +0000 2015</td>\n",
       "      <td>[GCHQ-AFPGetty.jpg, GettyImages-74018446.jpg, ...</td>\n",
       "      <td>The news that Russia and China have gained acc...</td>\n",
       "      <td>, Home News, UK, News</td>\n",
       "      <td>[UK security chiefs have warned that the forme...</td>\n",
       "      <td>'Huge catastrophe' as government claims Britai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>608926452596273152</td>\n",
       "      <td>[media/608926452596273152.jpg]</td>\n",
       "      <td>[Tourists detained in Malaysia for getting nak...</td>\n",
       "      <td>Thu Jun 11 09:19:14 +0000 2015</td>\n",
       "      <td>[Four tourists have been detained after stripp...</td>\n",
       "      <td>More tourists are in trouble for getting naked...</td>\n",
       "      <td>travel, destinations, uncategorized, us-world,...</td>\n",
       "      <td>[More tourists are in trouble for getting nake...</td>\n",
       "      <td>Tourists detained in Malaysia for getting nake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>609300383358418944</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Dominique Strauss-Kahn awaits verdict in â€˜a...</td>\n",
       "      <td>Fri Jun 12 10:05:06 +0000 2015</td>\n",
       "      <td>[Scotlandâ€™s Sturgeon to Seek Second Independ...</td>\n",
       "      <td>A French court acquitted Dominique Strauss-Kah...</td>\n",
       "      <td>aggravated pimping,dominique strauss-kahn,dsk,...</td>\n",
       "      <td>[PARISâ€”A French court Friday acquitted Domin...</td>\n",
       "      <td>Dominique Strauss-Kahn Acquitted of Aggravated...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>608875876474851328</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RT @PhilipRucker: Jeb's super PAC unlikely to...</td>\n",
       "      <td>Thu Jun 11 05:58:15 +0000 2015</td>\n",
       "      <td>[Former Florida governor and expected Republic...</td>\n",
       "      <td>A failure to reach the widely expected total w...</td>\n",
       "      <td>Jeb Bush, super PAC</td>\n",
       "      <td>[A super PAC backing former Florida governor J...</td>\n",
       "      <td>Super PAC backing Jeb Bush unlikely to hit $10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>610173844561788928</td>\n",
       "      <td>[]</td>\n",
       "      <td>[John Rohatinsky backs drug allegations made a...</td>\n",
       "      <td>Sun Jun 14 19:55:55 +0000 2015</td>\n",
       "      <td>[Alberto Salazar]</td>\n",
       "      <td>John Rohatinsky, one of Alberto Salazarâ€™s fo...</td>\n",
       "      <td>Alberto Salazar,Mo Farah,Drugs in sport,Athlet...</td>\n",
       "      <td>[The pressure on Mo Farahâ€™s coach Alberto Sa...</td>\n",
       "      <td>John Rohatinsky backs drug allegations made ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>609095030926635008</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RT @b_fung: Just in: A federal court won't ke...</td>\n",
       "      <td>Thu Jun 11 20:29:06 +0000 2015</td>\n",
       "      <td>[FCC Chairman Tom WheelerÂ ( AP Photo/Jose Lui...</td>\n",
       "      <td>The agency has won an early victory against In...</td>\n",
       "      <td>FCC, net neutrality</td>\n",
       "      <td>[A federal court has decided not to block the ...</td>\n",
       "      <td>A federal court just refused to block the FCCâ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>608576531456249856</td>\n",
       "      <td>[media/608576531456249856.jpg]</td>\n",
       "      <td>[Chinese women are growing their armpit hair l...</td>\n",
       "      <td>Wed Jun 10 10:08:46 +0000 2015</td>\n",
       "      <td>[Armpit Hair Don't Care: Chinese women are tak...</td>\n",
       "      <td>Chinese women are taking to social media to sh...</td>\n",
       "      <td>Armpit,hair,don,t,care,Chinese,women,flood,soc...</td>\n",
       "      <td>[Female celebrities have made headlines with t...</td>\n",
       "      <td>Armpit hair, don't care! Chinese women flood s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>609631563597389824</td>\n",
       "      <td>[]</td>\n",
       "      <td>[The brutal dictatorship the world keeps ignor...</td>\n",
       "      <td>Sat Jun 13 08:01:05 +0000 2015</td>\n",
       "      <td>[Eritrean President Isaias Afwerki listens as ...</td>\n",
       "      <td>Why don't U.N. accusations of \"crimes against ...</td>\n",
       "      <td>Eritrea, Africa, North Korea, United Nations, ...</td>\n",
       "      <td>[On Monday, the United Nations released the re...</td>\n",
       "      <td>The brutal dictatorship the world keeps ignoring</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>609372816568360960</td>\n",
       "      <td>[media/609372816568360960.jpg]</td>\n",
       "      <td>[RT @BBCWalesNews: Caerphilly farmer may get p...</td>\n",
       "      <td>Fri Jun 12 14:52:55 +0000 2015</td>\n",
       "      <td>[The High Court at the Royal Courts of Justice]</td>\n",
       "      <td>A Caerphilly farm owner forced to sell land to...</td>\n",
       "      <td></td>\n",
       "      <td>[Share this with, Email, Facebook, Messenger, ...</td>\n",
       "      <td>Caerphilly farmer may get full payout after 24...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>610103181578764288</td>\n",
       "      <td>[media/610103181578764291.jpg]</td>\n",
       "      <td>[Obama actually writes back to people who call...</td>\n",
       "      <td>Sun Jun 14 15:15:08 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>It's been said you should never feed the troll...</td>\n",
       "      <td>obama,obama extra interview,Obama Letters,obam...</td>\n",
       "      <td>[It's been said you should never feed the trol...</td>\n",
       "      <td>Obama Actually Writes Back To People Who Call ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>609425861561962496</td>\n",
       "      <td>[media/609425861561962496.png]</td>\n",
       "      <td>[RT @TheFix: The president's 'bully pulpit' is...</td>\n",
       "      <td>Fri Jun 12 18:23:42 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>Why couldn't the president get 13 percent of H...</td>\n",
       "      <td>trade bill; the fix; washington post politics</td>\n",
       "      <td>[President ObamaÂ wentÂ all-out to convince tw...</td>\n",
       "      <td>The presidentâ€™s â€˜bully pulpitâ€™ is way ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>608658774769958912</td>\n",
       "      <td>[media/608658774769958912.jpg]</td>\n",
       "      <td>[Man dies when car plunges from parking garage]</td>\n",
       "      <td>Wed Jun 10 15:35:34 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>The driver of a pickup was killed when he cras...</td>\n",
       "      <td></td>\n",
       "      <td>[The driver of a pickup truck died when he cra...</td>\n",
       "      <td>Man Dies When Car Plunges from New Orleans Par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>609396415681445888</td>\n",
       "      <td>[media/609396415681445888.png]</td>\n",
       "      <td>[5 inconsistencies in 'Jurassic World' that wi...</td>\n",
       "      <td>Fri Jun 12 16:26:42 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>VIDEO: Science fiction without the science.</td>\n",
       "      <td></td>\n",
       "      <td>[\"Jurassic World,\" the latest installment of t...</td>\n",
       "      <td>\"Jurassic World\" uses bad science - Business I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>609337131698229248</td>\n",
       "      <td>[media/609337131698229249.jpg]</td>\n",
       "      <td>[Why Christopher Lee made villainy sexy: via @...</td>\n",
       "      <td>Fri Jun 12 12:31:07 +0000 2015</td>\n",
       "      <td>[&amp;lt;a href=&amp;quot;http://www.cnn.com/2015/06/1...</td>\n",
       "      <td>The thing about Christopher Lee, who died Sund...</td>\n",
       "      <td>opinions, Christopher Lee, the sexiest bloodsu...</td>\n",
       "      <td>[Lewis Beale writes about culture and film for...</td>\n",
       "      <td>The sexiest bloodsucker on screen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>610152342521028608</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Greece nearing compromise deal on EU debts, s...</td>\n",
       "      <td>Sun Jun 14 18:30:29 +0000 2015</td>\n",
       "      <td>[Tourists under a Greek national flag on the A...</td>\n",
       "      <td>After weekend of intense talks Greek prime min...</td>\n",
       "      <td>Eurozone crisis,Greece,Alexis Tsipras,Financia...</td>\n",
       "      <td>[After weekend of intense talks, Greek prime m...</td>\n",
       "      <td>Alexis Tsipras hints that Greece is nearing co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>608315579913629696</td>\n",
       "      <td>[media/608315579913629696.png]</td>\n",
       "      <td>[A hero dog leapt in front of a school bus to ...</td>\n",
       "      <td>Tue Jun 09 16:51:50 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Both Audrey Stone and her trusty pup Figo sur...</td>\n",
       "      <td>This Heroic Pup Jumped In Front Of A Bus To Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>609417622485139456</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Breaking News: House Rejects Trade Bills, as ...</td>\n",
       "      <td>Fri Jun 12 17:50:58 +0000 2015</td>\n",
       "      <td>[The New York Times, Basic, All Access, Home D...</td>\n",
       "      <td>The vote torpedoed President Obamaâ€™s push to...</td>\n",
       "      <td>International Trade and World Market,Trans-Pac...</td>\n",
       "      <td>[WASHINGTON â€” Hours after President Obama ma...</td>\n",
       "      <td>House Rejects Trade Measure, Rebuffing Obamaâ€...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2429</th>\n",
       "      <td>608881572834619392</td>\n",
       "      <td>[media/608881572834619392.jpg]</td>\n",
       "      <td>[RT @BBCBreakfast: Do you have problems with y...</td>\n",
       "      <td>Thu Jun 11 06:20:54 +0000 2015</td>\n",
       "      <td>[Sharon White]</td>\n",
       "      <td>UK watchdog Ofcom's new chief reveals a plan t...</td>\n",
       "      <td></td>\n",
       "      <td>[Share this with, Email, Facebook, Messenger, ...</td>\n",
       "      <td>Broadband subscribers helped to quit slow prov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2430</th>\n",
       "      <td>610096771746930688</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Former police chief faces retrial in death of...</td>\n",
       "      <td>Sun Jun 14 14:49:40 +0000 2015</td>\n",
       "      <td>[Richard Combs]</td>\n",
       "      <td>Richard Combs is accused of shooting Bernard B...</td>\n",
       "      <td>South Carolina,US news,US policing</td>\n",
       "      <td>[Richard Combs is accused of shooting Bernard ...</td>\n",
       "      <td>Former police chief faces retrial in death of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2431</th>\n",
       "      <td>608289752438177792</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RT @UpshotNYT: So is Jeb Bush running? There ...</td>\n",
       "      <td>Tue Jun 09 15:09:13 +0000 2015</td>\n",
       "      <td>[The New York Times, Basic, All Access, Home D...</td>\n",
       "      <td>Prediction markets would be a way to prevent c...</td>\n",
       "      <td>Campaign Finance,Presidential Election of 2016...</td>\n",
       "      <td>[Follow Us: Get the Upshot in your Inbox The p...</td>\n",
       "      <td>Want to Bet That Jeb Bush Is Running?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2432</th>\n",
       "      <td>608376229893447680</td>\n",
       "      <td>[media/608376229893447680.jpg]</td>\n",
       "      <td>[The Life and Death of A Man Who Spent 3 Years...</td>\n",
       "      <td>Tue Jun 09 20:52:50 +0000 2015</td>\n",
       "      <td>[PHOTO: Kalief Browder during an interview wit...</td>\n",
       "      <td>Many are in mourning and calling for faster pr...</td>\n",
       "      <td>kalief browder, suicide, rikers, rikers island...</td>\n",
       "      <td>[Many are in mourning and calling for faster p...</td>\n",
       "      <td>Sections Shows Live Yahoo!-ABC News Network | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2433</th>\n",
       "      <td>608234446081421312</td>\n",
       "      <td>[media/608234446081421312.png]</td>\n",
       "      <td>[RT @BuzzFeedUK: A parent dressed her cat in t...</td>\n",
       "      <td>Tue Jun 09 11:29:27 +0000 2015</td>\n",
       "      <td>[This crochet bra top is currently on sale on ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[â€œDo not buy this, even for your cat.â€�, My...</td>\n",
       "      <td>A Parent Dressed Her Cat In This Bra Top To Sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2434</th>\n",
       "      <td>609831613887827968</td>\n",
       "      <td>[media/609831613887827968.jpg]</td>\n",
       "      <td>[Appellate court sides with Chicago woman in l...</td>\n",
       "      <td>Sat Jun 13 21:16:01 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>Karla Dunston wants to use her frozen embryos ...</td>\n",
       "      <td></td>\n",
       "      <td>[An Illinois appellate court on Friday sided w...</td>\n",
       "      <td>Appellate Court Sides With Chicago Woman In La...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2435</th>\n",
       "      <td>609695864274223104</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I had two children 15 years apart]</td>\n",
       "      <td>Sat Jun 13 12:16:36 +0000 2015</td>\n",
       "      <td>[Louise Lee and children, Louise Lee with Call...</td>\n",
       "      <td>Louise Lee was 27 when her son was born, 42 wh...</td>\n",
       "      <td>Family,Life and style,Parents and parenting,Ch...</td>\n",
       "      <td>[Itâ€™s 5am. One of my kids is waking up, the ...</td>\n",
       "      <td>I had two children 15 years apart</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2436</th>\n",
       "      <td>609382031659790336</td>\n",
       "      <td>[media/609382031659790336.png]</td>\n",
       "      <td>[RT @BuzzFeedNews: Here Is What Rachel Dolezal...</td>\n",
       "      <td>Fri Jun 12 15:29:32 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[The parents of Dolezal, the president of the ...</td>\n",
       "      <td>Here Are Rachel Dolezalâ€™s Responses So Far C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2437</th>\n",
       "      <td>608607809031278592</td>\n",
       "      <td>[media/608607809031278592.jpg]</td>\n",
       "      <td>[Pledge aside, dead billionaires don't have to...</td>\n",
       "      <td>Wed Jun 10 12:13:03 +0000 2015</td>\n",
       "      <td>[Billionaire investor Warren Buffett, right, a...</td>\n",
       "      <td>Since Bill Gates and Warren Buffett created th...</td>\n",
       "      <td>charity, bill-gates, social-good, warren-buffe...</td>\n",
       "      <td>[In the five years since Bill Gates and Warren...</td>\n",
       "      <td>Pledge aside, dead billionaires don't have to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2438</th>\n",
       "      <td>609178554681675776</td>\n",
       "      <td>[]</td>\n",
       "      <td>[RT @HuffPostSports: Here's the play that bloo...</td>\n",
       "      <td>Fri Jun 12 02:01:00 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>The entertainment network where videos and per...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>Vine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>608254479088144384</td>\n",
       "      <td>[media/608254479088144384.jpg]</td>\n",
       "      <td>[RT @BBCNewsMagazine: 40 years ago Jaws the mo...</td>\n",
       "      <td>Tue Jun 09 12:49:03 +0000 2015</td>\n",
       "      <td>[A scene from the film Jaws]</td>\n",
       "      <td>The film Jaws has shaped the way many of us vi...</td>\n",
       "      <td></td>\n",
       "      <td>[Share this with, Email, Facebook, Messenger, ...</td>\n",
       "      <td>How Jaws misrepresented the great white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2440</th>\n",
       "      <td>609040663225602048</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Shopping on Craigslist can be overwhelming. H...</td>\n",
       "      <td>Thu Jun 11 16:53:04 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>Whether youâ€™re seeking unique decor that won...</td>\n",
       "      <td></td>\n",
       "      <td>[Whether youâ€™re seeking unique decor that wo...</td>\n",
       "      <td>If You Want to Find the Best Steals, You Need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2441</th>\n",
       "      <td>607932867050504192</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Follow @pogue, @alyssabereznak, and @tynanwri...</td>\n",
       "      <td>Mon Jun 08 15:31:05 +0000 2015</td>\n",
       "      <td>[Liveblog: Appleâ€™s Worldwide Developerâ€™s C...</td>\n",
       "      <td>Join us here on Monday at 9 am Pacific Time fo...</td>\n",
       "      <td></td>\n",
       "      <td>[Join us on Monday at 9 am Pacific Time for li...</td>\n",
       "      <td>Liveblog: Appleâ€™s Worldwide Developerâ€™s Co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2442</th>\n",
       "      <td>608063741779951616</td>\n",
       "      <td>[media/608063741779951616.jpg]</td>\n",
       "      <td>[Community leaders will bypass prosecutors to ...</td>\n",
       "      <td>Tue Jun 09 00:11:07 +0000 2015</td>\n",
       "      <td>[The New York Times, Basic, All Access, Home D...</td>\n",
       "      <td>Community leaders will invoke a seldom-used Oh...</td>\n",
       "      <td>Rice  Tamir E (2002-14),Police Brutality  Misc...</td>\n",
       "      <td>[WASHINGTON â€” Community leaders in Cleveland...</td>\n",
       "      <td>Cleveland Leaders Bypass Prosecutors to Seek C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>609059063113084928</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Google's next project: fixing congested cities.]</td>\n",
       "      <td>Thu Jun 11 18:06:11 +0000 2015</td>\n",
       "      <td>[Dyn Says Cyberattack Has Ended, Investigation...</td>\n",
       "      <td>Not content with trying to prolong human life ...</td>\n",
       "      <td>Calico,Cities,congestion,Google X,Sidewalk,Sid...</td>\n",
       "      <td>[Not content with trying to prolong human life...</td>\n",
       "      <td>Googleâ€™s Next Project: Fixing Congested Citi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>608959948077301760</td>\n",
       "      <td>[]</td>\n",
       "      <td>[UK leads Europe for wealthy female breadwinne...</td>\n",
       "      <td>Thu Jun 11 11:32:20 +0000 2015</td>\n",
       "      <td>[young couple househunting]</td>\n",
       "      <td>Ipsos Affluent Survey Europe finds that 44% of...</td>\n",
       "      <td>Gender,Work &amp; careers,UK news,Equality,Money,S...</td>\n",
       "      <td>[Ipsos Affluent Survey Europe finds that 44% o...</td>\n",
       "      <td>UK leads Europe for wealthy female earners, su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2445</th>\n",
       "      <td>609701774979563520</td>\n",
       "      <td>[]</td>\n",
       "      <td>[The FCCâ€™s new net neutrality rules are in e...</td>\n",
       "      <td>Sat Jun 13 12:40:05 +0000 2015</td>\n",
       "      <td>[The Fight for Net Neutrality]</td>\n",
       "      <td>UPDATE: Net neutrality rules have officially g...</td>\n",
       "      <td></td>\n",
       "      <td>[UPDATE: On Friday, June 12, net neutrality ru...</td>\n",
       "      <td>The Fight for Net Neutrality</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2446</th>\n",
       "      <td>609034778738724864</td>\n",
       "      <td>[media/609034778738724864.jpg]</td>\n",
       "      <td>[RT @capitalweather: The Derecho, from D to O:...</td>\n",
       "      <td>Thu Jun 11 16:29:41 +0000 2015</td>\n",
       "      <td>[Kevin Ambrose filmed the Derecho, a rapidly m...</td>\n",
       "      <td>Our region is coming into prime derecho season.</td>\n",
       "      <td>derecho, dc derecho, june 29 2012 derecho,</td>\n",
       "      <td>[This post was originally published on June 11...</td>\n",
       "      <td>Itâ€™s prime derecho season in Washington: Eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2447</th>\n",
       "      <td>607959208194154496</td>\n",
       "      <td>[media/607959208194154496.jpg]</td>\n",
       "      <td>[RT @ESPNNFL: The 49ers' drop-off could lead t...</td>\n",
       "      <td>Mon Jun 08 17:15:45 +0000 2015</td>\n",
       "      <td>[The 49ers could face an interesting decision ...</td>\n",
       "      <td>As the 49ers begin their post-Jim Harbaugh reb...</td>\n",
       "      <td>san francisco 49ers, jim harbaugh, colin kaepe...</td>\n",
       "      <td>[The San Francisco 49ers are going through Pos...</td>\n",
       "      <td>Why the San Francisco 49ers could trade quarte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2448</th>\n",
       "      <td>608887646086008832</td>\n",
       "      <td>[media/608887646086008832.jpg]</td>\n",
       "      <td>[Housing market grinds to a halt as sales hit ...</td>\n",
       "      <td>Thu Jun 11 06:45:02 +0000 2015</td>\n",
       "      <td>[Couple looking in estate agent's window, Dan ...</td>\n",
       "      <td>Warnings of an \"acute shortage of supply\" as B...</td>\n",
       "      <td>House prices, house sales, selling, stamp duty...</td>\n",
       "      <td>[House prices could rise by a quarter in the n...</td>\n",
       "      <td>Housing market grinds to a halt as sales hit l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2449</th>\n",
       "      <td>608502553949650944</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Restaurant won't apologize for tasteless Cait...</td>\n",
       "      <td>Wed Jun 10 05:14:48 +0000 2015</td>\n",
       "      <td>[Default, Nacho Mama's yelp review I, Nacho Ma...</td>\n",
       "      <td>Nacho Mama's Mexican restaurant in Baltimore, ...</td>\n",
       "      <td>restaurant, jokes, uncategorized, watercooler,...</td>\n",
       "      <td>[Whoever comes up with the menus for Nacho Mam...</td>\n",
       "      <td>Restaurant won't apologize for tasteless Caitl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2450</th>\n",
       "      <td>609329602910097408</td>\n",
       "      <td>[media/609329602910097408.jpg]</td>\n",
       "      <td>[As a BASE jumper leaped, his girlfriend snapp...</td>\n",
       "      <td>Fri Jun 12 12:01:12 +0000 2015</td>\n",
       "      <td>[The New York Times, Basic, All Access, Home D...</td>\n",
       "      <td>Dean Potter jumped. Graham Hunt followed. Pott...</td>\n",
       "      <td>Rock Climbing,Parachutes and Parachute Jumping...</td>\n",
       "      <td>[Dean Potter jumped. Graham Hunt followed. Pot...</td>\n",
       "      <td>Lost Brother in Yosemite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2451</th>\n",
       "      <td>609082704458604544</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Scott Walker hasn't been to a Bucks game in \"...</td>\n",
       "      <td>Thu Jun 11 19:40:07 +0000 2015</td>\n",
       "      <td>[scott walker]</td>\n",
       "      <td>Wisconsin Gov. Scott Walker (and much speculat...</td>\n",
       "      <td>Scott Walker,2016,republicans,milwaukee,milwua...</td>\n",
       "      <td>[Wisconsin Gov. Scott Walker (and much specula...</td>\n",
       "      <td>Scott Walker Hasn't Gone To A Bucks Game In 'Y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2452</th>\n",
       "      <td>608040173805445120</td>\n",
       "      <td>[media/608040173805445120.jpg]</td>\n",
       "      <td>[RT @WNTonight: Grand jury indicts South Carol...</td>\n",
       "      <td>Mon Jun 08 22:37:28 +0000 2015</td>\n",
       "      <td>[PHOTO: A sequence of images made from a bysta...</td>\n",
       "      <td>Grand Jury Indicts Ex-Police Officer Who Shot ...</td>\n",
       "      <td>Walter Scott, police brutality, South Carolina...</td>\n",
       "      <td>[The police officer who fatally shot Walter Sc...</td>\n",
       "      <td>Sections Shows Live Yahoo!-ABC News Network | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2453</th>\n",
       "      <td>609389112848547840</td>\n",
       "      <td>[media/609389112848547840.png]</td>\n",
       "      <td>[Women scientists are tweeting \"sexyâ€� photos...</td>\n",
       "      <td>Fri Jun 12 15:57:41 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>[Marine biologists, archeologists, and many ot...</td>\n",
       "      <td>Women Scientists Are Tweeting â€œSexyâ€� Photo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2454</th>\n",
       "      <td>609056814819323904</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Man who received world's first penis transpla...</td>\n",
       "      <td>Thu Jun 11 17:57:15 +0000 2015</td>\n",
       "      <td>[Six months after undergoing the world's first...</td>\n",
       "      <td>Surgeons at Stellenbosch University, who carri...</td>\n",
       "      <td>World,s,penis,transplant,patient,set,FATHER,an...</td>\n",
       "      <td>[The man who underwent the world's first succe...</td>\n",
       "      <td>World's first penis transplant patient is set ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2455</th>\n",
       "      <td>610125815116865536</td>\n",
       "      <td>[media/610125815116865536.jpg]</td>\n",
       "      <td>[RT @NYTSports: Abby didn't start, team couldn...</td>\n",
       "      <td>Sun Jun 14 16:45:04 +0000 2015</td>\n",
       "      <td>[The New York Times, Basic, All Access, Home D...</td>\n",
       "      <td>With Abby Wambach not starting for the first t...</td>\n",
       "      <td>Soccer,Wambach  Abby,Press  Christen,United St...</td>\n",
       "      <td>[WINNIPEG, Manitoba, The moment Abby Wambach s...</td>\n",
       "      <td>At Womenâ€™s World Cup, Tie Leaves U.S. on Sol...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2456</th>\n",
       "      <td>608338587495628800</td>\n",
       "      <td>[media/608338587495628801.jpg]</td>\n",
       "      <td>[Obama defends Affordable Care Act ahead of Su...</td>\n",
       "      <td>Tue Jun 09 18:23:16 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>President Obama talks at the G7 summit in Germ...</td>\n",
       "      <td></td>\n",
       "      <td>[With the Supreme Court set to issue a ruling ...</td>\n",
       "      <td>Obama Defends Health Law Ahead of Supreme Cour...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2457</th>\n",
       "      <td>609684420082180096</td>\n",
       "      <td>[]</td>\n",
       "      <td>[New study of the Deflategate report concludes...</td>\n",
       "      <td>Sat Jun 13 11:31:07 +0000 2015</td>\n",
       "      <td>[The New York Times, Basic, All Access, Home D...</td>\n",
       "      <td>A new study weakens the case against the Patri...</td>\n",
       "      <td>Football,Cheating,American Enterprise Institut...</td>\n",
       "      <td>[BEFORE â€œDeflategate,â€� the National Footba...</td>\n",
       "      <td>Deflating â€˜Deflategateâ€™</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2458</th>\n",
       "      <td>608392385425338368</td>\n",
       "      <td>[media/608392385425338368.jpg]</td>\n",
       "      <td>[Netflix secures rights to Brad Pitt Afghanist...</td>\n",
       "      <td>Tue Jun 09 21:57:02 +0000 2015</td>\n",
       "      <td>[]</td>\n",
       "      <td>In a coup for Netflix, the star will star as f...</td>\n",
       "      <td></td>\n",
       "      <td>[EXCLUSIVE: has acquired distribution rights t...</td>\n",
       "      <td>Brad Pitt to Star in Netflix Film About Afghan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2459 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       postMedia  \\\n",
       "0     608310377143799808                              []   \n",
       "1     609297109095972864  [media/609297109095972864.jpg]   \n",
       "2     609504474621612032                              []   \n",
       "3     609748367049105408                              []   \n",
       "4     608688782821453824  [media/608688782821453825.jpg]   \n",
       "5     609551038983475200  [media/609551038983475201.png]   \n",
       "6     609447408955719680  [media/609447408955719681.jpg]   \n",
       "7     609027430624288768                              []   \n",
       "8     608229011572068352                              []   \n",
       "9     609046214554755072  [media/609046214554755073.jpg]   \n",
       "10    609738211183751168                              []   \n",
       "11    609762671554990080  [media/609762671554990080.jpg]   \n",
       "12    608980783924178944                              []   \n",
       "13    609998126737395712  [media/609998126737395712.jpg]   \n",
       "14    608926452596273152  [media/608926452596273152.jpg]   \n",
       "15    609300383358418944                              []   \n",
       "16    608875876474851328                              []   \n",
       "17    610173844561788928                              []   \n",
       "18    609095030926635008                              []   \n",
       "19    608576531456249856  [media/608576531456249856.jpg]   \n",
       "20    609631563597389824                              []   \n",
       "21    609372816568360960  [media/609372816568360960.jpg]   \n",
       "22    610103181578764288  [media/610103181578764291.jpg]   \n",
       "23    609425861561962496  [media/609425861561962496.png]   \n",
       "24    608658774769958912  [media/608658774769958912.jpg]   \n",
       "25    609396415681445888  [media/609396415681445888.png]   \n",
       "26    609337131698229248  [media/609337131698229249.jpg]   \n",
       "27    610152342521028608                              []   \n",
       "28    608315579913629696  [media/608315579913629696.png]   \n",
       "29    609417622485139456                              []   \n",
       "...                  ...                             ...   \n",
       "2429  608881572834619392  [media/608881572834619392.jpg]   \n",
       "2430  610096771746930688                              []   \n",
       "2431  608289752438177792                              []   \n",
       "2432  608376229893447680  [media/608376229893447680.jpg]   \n",
       "2433  608234446081421312  [media/608234446081421312.png]   \n",
       "2434  609831613887827968  [media/609831613887827968.jpg]   \n",
       "2435  609695864274223104                              []   \n",
       "2436  609382031659790336  [media/609382031659790336.png]   \n",
       "2437  608607809031278592  [media/608607809031278592.jpg]   \n",
       "2438  609178554681675776                              []   \n",
       "2439  608254479088144384  [media/608254479088144384.jpg]   \n",
       "2440  609040663225602048                              []   \n",
       "2441  607932867050504192                              []   \n",
       "2442  608063741779951616  [media/608063741779951616.jpg]   \n",
       "2443  609059063113084928                              []   \n",
       "2444  608959948077301760                              []   \n",
       "2445  609701774979563520                              []   \n",
       "2446  609034778738724864  [media/609034778738724864.jpg]   \n",
       "2447  607959208194154496  [media/607959208194154496.jpg]   \n",
       "2448  608887646086008832  [media/608887646086008832.jpg]   \n",
       "2449  608502553949650944                              []   \n",
       "2450  609329602910097408  [media/609329602910097408.jpg]   \n",
       "2451  609082704458604544                              []   \n",
       "2452  608040173805445120  [media/608040173805445120.jpg]   \n",
       "2453  609389112848547840  [media/609389112848547840.png]   \n",
       "2454  609056814819323904                              []   \n",
       "2455  610125815116865536  [media/610125815116865536.jpg]   \n",
       "2456  608338587495628800  [media/608338587495628801.jpg]   \n",
       "2457  609684420082180096                              []   \n",
       "2458  608392385425338368  [media/608392385425338368.jpg]   \n",
       "\n",
       "                                               postText  \\\n",
       "0     [Apple's iOS 9 'App thinning' feature will giv...   \n",
       "1     [RT @kenbrown12: Emerging market investors are...   \n",
       "2     [U.S. Soccer should start answering tough ques...   \n",
       "3     [How theme parks like Disney World left the mi...   \n",
       "4     [Could light bulbs hurt your health? One compa...   \n",
       "5     [13 classic â€™00s songs that were actually me...   \n",
       "6     [Dez Bryant is reportedly considering skipping...   \n",
       "7     [Pregnant mother of 12 accused of keeping kids...   \n",
       "8     [RT @fionamatthias: 10 ways the expat life Is ...   \n",
       "9     [House #GOP plans two days of debate, Friday s...   \n",
       "10    [Azeri government behind foreign media ban, sa...   \n",
       "11    [Only one in three of us complain when we are ...   \n",
       "12    [An open letter to Jerry Seinfeld from a \"poli...   \n",
       "13    [Britain forced to withdraw spies after Russia...   \n",
       "14    [Tourists detained in Malaysia for getting nak...   \n",
       "15    [Dominique Strauss-Kahn awaits verdict in â€˜a...   \n",
       "16    [RT @PhilipRucker: Jeb's super PAC unlikely to...   \n",
       "17    [John Rohatinsky backs drug allegations made a...   \n",
       "18    [RT @b_fung: Just in: A federal court won't ke...   \n",
       "19    [Chinese women are growing their armpit hair l...   \n",
       "20    [The brutal dictatorship the world keeps ignor...   \n",
       "21    [RT @BBCWalesNews: Caerphilly farmer may get p...   \n",
       "22    [Obama actually writes back to people who call...   \n",
       "23    [RT @TheFix: The president's 'bully pulpit' is...   \n",
       "24      [Man dies when car plunges from parking garage]   \n",
       "25    [5 inconsistencies in 'Jurassic World' that wi...   \n",
       "26    [Why Christopher Lee made villainy sexy: via @...   \n",
       "27    [Greece nearing compromise deal on EU debts, s...   \n",
       "28    [A hero dog leapt in front of a school bus to ...   \n",
       "29    [Breaking News: House Rejects Trade Bills, as ...   \n",
       "...                                                 ...   \n",
       "2429  [RT @BBCBreakfast: Do you have problems with y...   \n",
       "2430  [Former police chief faces retrial in death of...   \n",
       "2431  [RT @UpshotNYT: So is Jeb Bush running? There ...   \n",
       "2432  [The Life and Death of A Man Who Spent 3 Years...   \n",
       "2433  [RT @BuzzFeedUK: A parent dressed her cat in t...   \n",
       "2434  [Appellate court sides with Chicago woman in l...   \n",
       "2435                [I had two children 15 years apart]   \n",
       "2436  [RT @BuzzFeedNews: Here Is What Rachel Dolezal...   \n",
       "2437  [Pledge aside, dead billionaires don't have to...   \n",
       "2438  [RT @HuffPostSports: Here's the play that bloo...   \n",
       "2439  [RT @BBCNewsMagazine: 40 years ago Jaws the mo...   \n",
       "2440  [Shopping on Craigslist can be overwhelming. H...   \n",
       "2441  [Follow @pogue, @alyssabereznak, and @tynanwri...   \n",
       "2442  [Community leaders will bypass prosecutors to ...   \n",
       "2443  [Google's next project: fixing congested cities.]   \n",
       "2444  [UK leads Europe for wealthy female breadwinne...   \n",
       "2445  [The FCCâ€™s new net neutrality rules are in e...   \n",
       "2446  [RT @capitalweather: The Derecho, from D to O:...   \n",
       "2447  [RT @ESPNNFL: The 49ers' drop-off could lead t...   \n",
       "2448  [Housing market grinds to a halt as sales hit ...   \n",
       "2449  [Restaurant won't apologize for tasteless Cait...   \n",
       "2450  [As a BASE jumper leaped, his girlfriend snapp...   \n",
       "2451  [Scott Walker hasn't been to a Bucks game in \"...   \n",
       "2452  [RT @WNTonight: Grand jury indicts South Carol...   \n",
       "2453  [Women scientists are tweeting \"sexyâ€� photos...   \n",
       "2454  [Man who received world's first penis transpla...   \n",
       "2455  [RT @NYTSports: Abby didn't start, team couldn...   \n",
       "2456  [Obama defends Affordable Care Act ahead of Su...   \n",
       "2457  [New study of the Deflategate report concludes...   \n",
       "2458  [Netflix secures rights to Brad Pitt Afghanist...   \n",
       "\n",
       "                       postTimestamp  \\\n",
       "0     Tue Jun 09 16:31:10 +0000 2015   \n",
       "1     Fri Jun 12 09:52:05 +0000 2015   \n",
       "2     Fri Jun 12 23:36:05 +0000 2015   \n",
       "3     Sat Jun 13 15:45:13 +0000 2015   \n",
       "4     Wed Jun 10 17:34:49 +0000 2015   \n",
       "5     Sat Jun 13 02:41:07 +0000 2015   \n",
       "6     Fri Jun 12 19:49:19 +0000 2015   \n",
       "7     Thu Jun 11 16:00:29 +0000 2015   \n",
       "8     Tue Jun 09 11:07:51 +0000 2015   \n",
       "9     Thu Jun 11 17:15:07 +0000 2015   \n",
       "10    Sat Jun 13 15:04:52 +0000 2015   \n",
       "11    Sat Jun 13 16:42:04 +0000 2015   \n",
       "12    Thu Jun 11 12:55:07 +0000 2015   \n",
       "13    Sun Jun 14 08:17:41 +0000 2015   \n",
       "14    Thu Jun 11 09:19:14 +0000 2015   \n",
       "15    Fri Jun 12 10:05:06 +0000 2015   \n",
       "16    Thu Jun 11 05:58:15 +0000 2015   \n",
       "17    Sun Jun 14 19:55:55 +0000 2015   \n",
       "18    Thu Jun 11 20:29:06 +0000 2015   \n",
       "19    Wed Jun 10 10:08:46 +0000 2015   \n",
       "20    Sat Jun 13 08:01:05 +0000 2015   \n",
       "21    Fri Jun 12 14:52:55 +0000 2015   \n",
       "22    Sun Jun 14 15:15:08 +0000 2015   \n",
       "23    Fri Jun 12 18:23:42 +0000 2015   \n",
       "24    Wed Jun 10 15:35:34 +0000 2015   \n",
       "25    Fri Jun 12 16:26:42 +0000 2015   \n",
       "26    Fri Jun 12 12:31:07 +0000 2015   \n",
       "27    Sun Jun 14 18:30:29 +0000 2015   \n",
       "28    Tue Jun 09 16:51:50 +0000 2015   \n",
       "29    Fri Jun 12 17:50:58 +0000 2015   \n",
       "...                              ...   \n",
       "2429  Thu Jun 11 06:20:54 +0000 2015   \n",
       "2430  Sun Jun 14 14:49:40 +0000 2015   \n",
       "2431  Tue Jun 09 15:09:13 +0000 2015   \n",
       "2432  Tue Jun 09 20:52:50 +0000 2015   \n",
       "2433  Tue Jun 09 11:29:27 +0000 2015   \n",
       "2434  Sat Jun 13 21:16:01 +0000 2015   \n",
       "2435  Sat Jun 13 12:16:36 +0000 2015   \n",
       "2436  Fri Jun 12 15:29:32 +0000 2015   \n",
       "2437  Wed Jun 10 12:13:03 +0000 2015   \n",
       "2438  Fri Jun 12 02:01:00 +0000 2015   \n",
       "2439  Tue Jun 09 12:49:03 +0000 2015   \n",
       "2440  Thu Jun 11 16:53:04 +0000 2015   \n",
       "2441  Mon Jun 08 15:31:05 +0000 2015   \n",
       "2442  Tue Jun 09 00:11:07 +0000 2015   \n",
       "2443  Thu Jun 11 18:06:11 +0000 2015   \n",
       "2444  Thu Jun 11 11:32:20 +0000 2015   \n",
       "2445  Sat Jun 13 12:40:05 +0000 2015   \n",
       "2446  Thu Jun 11 16:29:41 +0000 2015   \n",
       "2447  Mon Jun 08 17:15:45 +0000 2015   \n",
       "2448  Thu Jun 11 06:45:02 +0000 2015   \n",
       "2449  Wed Jun 10 05:14:48 +0000 2015   \n",
       "2450  Fri Jun 12 12:01:12 +0000 2015   \n",
       "2451  Thu Jun 11 19:40:07 +0000 2015   \n",
       "2452  Mon Jun 08 22:37:28 +0000 2015   \n",
       "2453  Fri Jun 12 15:57:41 +0000 2015   \n",
       "2454  Thu Jun 11 17:57:15 +0000 2015   \n",
       "2455  Sun Jun 14 16:45:04 +0000 2015   \n",
       "2456  Tue Jun 09 18:23:16 +0000 2015   \n",
       "2457  Sat Jun 13 11:31:07 +0000 2015   \n",
       "2458  Tue Jun 09 21:57:02 +0000 2015   \n",
       "\n",
       "                                         targetCaptions  \\\n",
       "0     ['App thinning' will be supported on Apple's i...   \n",
       "1     [Stocks Fall as Investors Watch Central Banks,...   \n",
       "2     [US to vote for Ali in FIFA election and not B...   \n",
       "3     [Some 1,000 persons turned out in Albuquerque,...   \n",
       "4     [Electric lights have made the world safer and...   \n",
       "5                                                    []   \n",
       "6                                                    []   \n",
       "7                                                    []   \n",
       "8     [Scotland to Seek Second Independence Referend...   \n",
       "9     [Obama, Chairman Paul Ryan and Republicans act...   \n",
       "10                                     [European Games]   \n",
       "11                                            [nhs.jpg]   \n",
       "12                                                   []   \n",
       "13    [GCHQ-AFPGetty.jpg, GettyImages-74018446.jpg, ...   \n",
       "14    [Four tourists have been detained after stripp...   \n",
       "15    [Scotlandâ€™s Sturgeon to Seek Second Independ...   \n",
       "16    [Former Florida governor and expected Republic...   \n",
       "17                                    [Alberto Salazar]   \n",
       "18    [FCC Chairman Tom WheelerÂ ( AP Photo/Jose Lui...   \n",
       "19    [Armpit Hair Don't Care: Chinese women are tak...   \n",
       "20    [Eritrean President Isaias Afwerki listens as ...   \n",
       "21      [The High Court at the Royal Courts of Justice]   \n",
       "22                                                   []   \n",
       "23                                                   []   \n",
       "24                                                   []   \n",
       "25                                                   []   \n",
       "26    [&lt;a href=&quot;http://www.cnn.com/2015/06/1...   \n",
       "27    [Tourists under a Greek national flag on the A...   \n",
       "28                                                   []   \n",
       "29    [The New York Times, Basic, All Access, Home D...   \n",
       "...                                                 ...   \n",
       "2429                                     [Sharon White]   \n",
       "2430                                    [Richard Combs]   \n",
       "2431  [The New York Times, Basic, All Access, Home D...   \n",
       "2432  [PHOTO: Kalief Browder during an interview wit...   \n",
       "2433  [This crochet bra top is currently on sale on ...   \n",
       "2434                                                 []   \n",
       "2435  [Louise Lee and children, Louise Lee with Call...   \n",
       "2436                                                 []   \n",
       "2437  [Billionaire investor Warren Buffett, right, a...   \n",
       "2438                                                 []   \n",
       "2439                       [A scene from the film Jaws]   \n",
       "2440                                                 []   \n",
       "2441  [Liveblog: Appleâ€™s Worldwide Developerâ€™s C...   \n",
       "2442  [The New York Times, Basic, All Access, Home D...   \n",
       "2443  [Dyn Says Cyberattack Has Ended, Investigation...   \n",
       "2444                        [young couple househunting]   \n",
       "2445                     [The Fight for Net Neutrality]   \n",
       "2446  [Kevin Ambrose filmed the Derecho, a rapidly m...   \n",
       "2447  [The 49ers could face an interesting decision ...   \n",
       "2448  [Couple looking in estate agent's window, Dan ...   \n",
       "2449  [Default, Nacho Mama's yelp review I, Nacho Ma...   \n",
       "2450  [The New York Times, Basic, All Access, Home D...   \n",
       "2451                                     [scott walker]   \n",
       "2452  [PHOTO: A sequence of images made from a bysta...   \n",
       "2453                                                 []   \n",
       "2454  [Six months after undergoing the world's first...   \n",
       "2455  [The New York Times, Basic, All Access, Home D...   \n",
       "2456                                                 []   \n",
       "2457  [The New York Times, Basic, All Access, Home D...   \n",
       "2458                                                 []   \n",
       "\n",
       "                                      targetDescription  \\\n",
       "0     'App thinning' will be supported on Apple's iO...   \n",
       "1     Global investors have yanked $9.3 billion from...   \n",
       "2     A U.S. Senator's scathing letter questioned U....   \n",
       "3     America's top family vacation spots, like the ...   \n",
       "4     One company will put a health notice on all th...   \n",
       "5                                                         \n",
       "6       Wide receiver  Dez Bryant  has signed a new,...   \n",
       "7     A pregnant mother of 12 is due in court on chi...   \n",
       "8     Thereâ€™s no autopilot when you're an expat li...   \n",
       "9     House Republican leaders have planned for a tw...   \n",
       "10    Organisers of the European Games in Azerbaijan...   \n",
       "11    Only one in three people who are unhappy with ...   \n",
       "12               Dear Jerry Seinfeld,\\n\\nRecently, I've   \n",
       "13    The news that Russia and China have gained acc...   \n",
       "14    More tourists are in trouble for getting naked...   \n",
       "15    A French court acquitted Dominique Strauss-Kah...   \n",
       "16    A failure to reach the widely expected total w...   \n",
       "17    John Rohatinsky, one of Alberto Salazarâ€™s fo...   \n",
       "18    The agency has won an early victory against In...   \n",
       "19    Chinese women are taking to social media to sh...   \n",
       "20    Why don't U.N. accusations of \"crimes against ...   \n",
       "21    A Caerphilly farm owner forced to sell land to...   \n",
       "22    It's been said you should never feed the troll...   \n",
       "23    Why couldn't the president get 13 percent of H...   \n",
       "24    The driver of a pickup was killed when he cras...   \n",
       "25          VIDEO: Science fiction without the science.   \n",
       "26    The thing about Christopher Lee, who died Sund...   \n",
       "27    After weekend of intense talks Greek prime min...   \n",
       "28                                                        \n",
       "29    The vote torpedoed President Obamaâ€™s push to...   \n",
       "...                                                 ...   \n",
       "2429  UK watchdog Ofcom's new chief reveals a plan t...   \n",
       "2430  Richard Combs is accused of shooting Bernard B...   \n",
       "2431  Prediction markets would be a way to prevent c...   \n",
       "2432  Many are in mourning and calling for faster pr...   \n",
       "2433                                                      \n",
       "2434  Karla Dunston wants to use her frozen embryos ...   \n",
       "2435  Louise Lee was 27 when her son was born, 42 wh...   \n",
       "2436                                                      \n",
       "2437  Since Bill Gates and Warren Buffett created th...   \n",
       "2438  The entertainment network where videos and per...   \n",
       "2439  The film Jaws has shaped the way many of us vi...   \n",
       "2440  Whether youâ€™re seeking unique decor that won...   \n",
       "2441  Join us here on Monday at 9 am Pacific Time fo...   \n",
       "2442  Community leaders will invoke a seldom-used Oh...   \n",
       "2443  Not content with trying to prolong human life ...   \n",
       "2444  Ipsos Affluent Survey Europe finds that 44% of...   \n",
       "2445  UPDATE: Net neutrality rules have officially g...   \n",
       "2446    Our region is coming into prime derecho season.   \n",
       "2447  As the 49ers begin their post-Jim Harbaugh reb...   \n",
       "2448  Warnings of an \"acute shortage of supply\" as B...   \n",
       "2449  Nacho Mama's Mexican restaurant in Baltimore, ...   \n",
       "2450  Dean Potter jumped. Graham Hunt followed. Pott...   \n",
       "2451  Wisconsin Gov. Scott Walker (and much speculat...   \n",
       "2452  Grand Jury Indicts Ex-Police Officer Who Shot ...   \n",
       "2453                                                      \n",
       "2454  Surgeons at Stellenbosch University, who carri...   \n",
       "2455  With Abby Wambach not starting for the first t...   \n",
       "2456  President Obama talks at the G7 summit in Germ...   \n",
       "2457  A new study weakens the case against the Patri...   \n",
       "2458  In a coup for Netflix, the star will star as f...   \n",
       "\n",
       "                                         targetKeywords  \\\n",
       "0     Apple,gives,gigabytes,iOS,9,app,thinning,featu...   \n",
       "1     emerging market,emerging markets,em flows,em i...   \n",
       "2                                                         \n",
       "3     disney, disney world, disney ticket prices, di...   \n",
       "4     health, Should there be warning labels on your...   \n",
       "5                                                         \n",
       "6     Football, NFL, NFC East, Dallas Cowboys, Dez B...   \n",
       "7                                                         \n",
       "8     adventure,Alienation,Bangkok,Culture,Culture S...   \n",
       "9                                                         \n",
       "10      European Games 2015,Sport,Azerbaijan,World news   \n",
       "11          , Health News, Health & Families, Lifestyle   \n",
       "12    Jerry Seinfeld,Amy Schumer,Louis CK,comedy,col...   \n",
       "13                                , Home News, UK, News   \n",
       "14    travel, destinations, uncategorized, us-world,...   \n",
       "15    aggravated pimping,dominique strauss-kahn,dsk,...   \n",
       "16                                  Jeb Bush, super PAC   \n",
       "17    Alberto Salazar,Mo Farah,Drugs in sport,Athlet...   \n",
       "18                                  FCC, net neutrality   \n",
       "19    Armpit,hair,don,t,care,Chinese,women,flood,soc...   \n",
       "20    Eritrea, Africa, North Korea, United Nations, ...   \n",
       "21                                                        \n",
       "22    obama,obama extra interview,Obama Letters,obam...   \n",
       "23        trade bill; the fix; washington post politics   \n",
       "24                                                        \n",
       "25                                                        \n",
       "26    opinions, Christopher Lee, the sexiest bloodsu...   \n",
       "27    Eurozone crisis,Greece,Alexis Tsipras,Financia...   \n",
       "28                                                        \n",
       "29    International Trade and World Market,Trans-Pac...   \n",
       "...                                                 ...   \n",
       "2429                                                      \n",
       "2430                 South Carolina,US news,US policing   \n",
       "2431  Campaign Finance,Presidential Election of 2016...   \n",
       "2432  kalief browder, suicide, rikers, rikers island...   \n",
       "2433                                                      \n",
       "2434                                                      \n",
       "2435  Family,Life and style,Parents and parenting,Ch...   \n",
       "2436                                                      \n",
       "2437  charity, bill-gates, social-good, warren-buffe...   \n",
       "2438                                                      \n",
       "2439                                                      \n",
       "2440                                                      \n",
       "2441                                                      \n",
       "2442  Rice  Tamir E (2002-14),Police Brutality  Misc...   \n",
       "2443  Calico,Cities,congestion,Google X,Sidewalk,Sid...   \n",
       "2444  Gender,Work & careers,UK news,Equality,Money,S...   \n",
       "2445                                                      \n",
       "2446        derecho, dc derecho, june 29 2012 derecho,    \n",
       "2447  san francisco 49ers, jim harbaugh, colin kaepe...   \n",
       "2448  House prices, house sales, selling, stamp duty...   \n",
       "2449  restaurant, jokes, uncategorized, watercooler,...   \n",
       "2450  Rock Climbing,Parachutes and Parachute Jumping...   \n",
       "2451  Scott Walker,2016,republicans,milwaukee,milwua...   \n",
       "2452  Walter Scott, police brutality, South Carolina...   \n",
       "2453                                                      \n",
       "2454  World,s,penis,transplant,patient,set,FATHER,an...   \n",
       "2455  Soccer,Wambach  Abby,Press  Christen,United St...   \n",
       "2456                                                      \n",
       "2457  Football,Cheating,American Enterprise Institut...   \n",
       "2458                                                      \n",
       "\n",
       "                                       targetParagraphs  \\\n",
       "0     [Paying for a 64GB phone only to discover that...   \n",
       "1     [Emerging markets are out of favor., Global in...   \n",
       "2     [WINNIPEG, Manitoba â€“ The bubble U.S. Soccer...   \n",
       "3     [When Walt Disney World opened in an Orlando s...   \n",
       "4     [(CNN)The light bulb always makes the world's ...   \n",
       "5     [One artistâ€™s trash is anotherâ€™s No. 1 sin...   \n",
       "6     [Wide receiver Dez BryantÂ has signed a new, l...   \n",
       "7     [TULSA, Okla. â€“ Â A pregnant mother of 12 is...   \n",
       "8     [Thereâ€™s no autopilot when you live abroad. ...   \n",
       "9     [House Republican leaders have planned for a t...   \n",
       "10    [A decision to ban some foreign media from att...   \n",
       "11    [Research found that while 90 per cent believe...   \n",
       "12    [Dear Jerry Seinfeld,, Recently, I've heard ab...   \n",
       "13    [UK security chiefs have warned that the forme...   \n",
       "14    [More tourists are in trouble for getting nake...   \n",
       "15    [PARISâ€”A French court Friday acquitted Domin...   \n",
       "16    [A super PAC backing former Florida governor J...   \n",
       "17    [The pressure on Mo Farahâ€™s coach Alberto Sa...   \n",
       "18    [A federal court has decided not to block the ...   \n",
       "19    [Female celebrities have made headlines with t...   \n",
       "20    [On Monday, the United Nations released the re...   \n",
       "21    [Share this with, Email, Facebook, Messenger, ...   \n",
       "22    [It's been said you should never feed the trol...   \n",
       "23    [President ObamaÂ wentÂ all-out to convince tw...   \n",
       "24    [The driver of a pickup truck died when he cra...   \n",
       "25    [\"Jurassic World,\" the latest installment of t...   \n",
       "26    [Lewis Beale writes about culture and film for...   \n",
       "27    [After weekend of intense talks, Greek prime m...   \n",
       "28    [Both Audrey Stone and her trusty pup Figo sur...   \n",
       "29    [WASHINGTON â€” Hours after President Obama ma...   \n",
       "...                                                 ...   \n",
       "2429  [Share this with, Email, Facebook, Messenger, ...   \n",
       "2430  [Richard Combs is accused of shooting Bernard ...   \n",
       "2431  [Follow Us: Get the Upshot in your Inbox The p...   \n",
       "2432  [Many are in mourning and calling for faster p...   \n",
       "2433  [â€œDo not buy this, even for your cat.â€�, My...   \n",
       "2434  [An Illinois appellate court on Friday sided w...   \n",
       "2435  [Itâ€™s 5am. One of my kids is waking up, the ...   \n",
       "2436  [The parents of Dolezal, the president of the ...   \n",
       "2437  [In the five years since Bill Gates and Warren...   \n",
       "2438                                                 []   \n",
       "2439  [Share this with, Email, Facebook, Messenger, ...   \n",
       "2440  [Whether youâ€™re seeking unique decor that wo...   \n",
       "2441  [Join us on Monday at 9 am Pacific Time for li...   \n",
       "2442  [WASHINGTON â€” Community leaders in Cleveland...   \n",
       "2443  [Not content with trying to prolong human life...   \n",
       "2444  [Ipsos Affluent Survey Europe finds that 44% o...   \n",
       "2445  [UPDATE: On Friday, June 12, net neutrality ru...   \n",
       "2446  [This post was originally published on June 11...   \n",
       "2447  [The San Francisco 49ers are going through Pos...   \n",
       "2448  [House prices could rise by a quarter in the n...   \n",
       "2449  [Whoever comes up with the menus for Nacho Mam...   \n",
       "2450  [Dean Potter jumped. Graham Hunt followed. Pot...   \n",
       "2451  [Wisconsin Gov. Scott Walker (and much specula...   \n",
       "2452  [The police officer who fatally shot Walter Sc...   \n",
       "2453  [Marine biologists, archeologists, and many ot...   \n",
       "2454  [The man who underwent the world's first succe...   \n",
       "2455  [WINNIPEG, Manitoba, The moment Abby Wambach s...   \n",
       "2456  [With the Supreme Court set to issue a ruling ...   \n",
       "2457  [BEFORE â€œDeflategate,â€� the National Footba...   \n",
       "2458  [EXCLUSIVE: has acquired distribution rights t...   \n",
       "\n",
       "                                            targetTitle  \n",
       "0     Apple gives back gigabytes: iOS 9 'app thinnin...  \n",
       "1     Emerging Markets Suffer Largest Outflow in Sev...  \n",
       "2     U.S. Soccer should start answering tough quest...  \n",
       "3     How theme parks like Disney World left the mid...  \n",
       "4                    Warning labels on your light bulbs  \n",
       "5     13 Classic â€™00s Songs That Were Actually Mea...  \n",
       "6     Dez Bryant Contract: Latest News, Rumors, Spec...  \n",
       "7     Pregnant mother of 12 accused of keeping kids ...  \n",
       "8     10 Ways the Expat Life Is Like a Continual Esp...  \n",
       "9     House GOP plans two days of debate, Friday sho...  \n",
       "10    Azeri government behind foreign media ban, say...  \n",
       "11    Only one in three unhappy NHS patients actuall...  \n",
       "12    An Open Letter to Jerry Seinfeld from a 'Polit...  \n",
       "13    'Huge catastrophe' as government claims Britai...  \n",
       "14    Tourists detained in Malaysia for getting nake...  \n",
       "15    Dominique Strauss-Kahn Acquitted of Aggravated...  \n",
       "16    Super PAC backing Jeb Bush unlikely to hit $10...  \n",
       "17    John Rohatinsky backs drug allegations made ag...  \n",
       "18    A federal court just refused to block the FCCâ...  \n",
       "19    Armpit hair, don't care! Chinese women flood s...  \n",
       "20     The brutal dictatorship the world keeps ignoring  \n",
       "21    Caerphilly farmer may get full payout after 24...  \n",
       "22    Obama Actually Writes Back To People Who Call ...  \n",
       "23    The presidentâ€™s â€˜bully pulpitâ€™ is way ov...  \n",
       "24    Man Dies When Car Plunges from New Orleans Par...  \n",
       "25    \"Jurassic World\" uses bad science - Business I...  \n",
       "26                    The sexiest bloodsucker on screen  \n",
       "27    Alexis Tsipras hints that Greece is nearing co...  \n",
       "28    This Heroic Pup Jumped In Front Of A Bus To Sa...  \n",
       "29    House Rejects Trade Measure, Rebuffing Obamaâ€...  \n",
       "...                                                 ...  \n",
       "2429  Broadband subscribers helped to quit slow prov...  \n",
       "2430  Former police chief faces retrial in death of ...  \n",
       "2431             Want to Bet That Jeb Bush Is Running?   \n",
       "2432  Sections Shows Live Yahoo!-ABC News Network | ...  \n",
       "2433  A Parent Dressed Her Cat In This Bra Top To Sh...  \n",
       "2434  Appellate Court Sides With Chicago Woman In La...  \n",
       "2435                  I had two children 15 years apart  \n",
       "2436  Here Are Rachel Dolezalâ€™s Responses So Far C...  \n",
       "2437  Pledge aside, dead billionaires don't have to ...  \n",
       "2438                                               Vine  \n",
       "2439            How Jaws misrepresented the great white  \n",
       "2440  If You Want to Find the Best Steals, You Need ...  \n",
       "2441  Liveblog: Appleâ€™s Worldwide Developerâ€™s Co...  \n",
       "2442  Cleveland Leaders Bypass Prosecutors to Seek C...  \n",
       "2443  Googleâ€™s Next Project: Fixing Congested Citi...  \n",
       "2444  UK leads Europe for wealthy female earners, su...  \n",
       "2445                       The Fight for Net Neutrality  \n",
       "2446  Itâ€™s prime derecho season in Washington: Eve...  \n",
       "2447  Why the San Francisco 49ers could trade quarte...  \n",
       "2448  Housing market grinds to a halt as sales hit l...  \n",
       "2449  Restaurant won't apologize for tasteless Caitl...  \n",
       "2450                          Lost Brother in Yosemite   \n",
       "2451  Scott Walker Hasn't Gone To A Bucks Game In 'Y...  \n",
       "2452  Sections Shows Live Yahoo!-ABC News Network | ...  \n",
       "2453  Women Scientists Are Tweeting â€œSexyâ€� Photo...  \n",
       "2454  World's first penis transplant patient is set ...  \n",
       "2455  At Womenâ€™s World Cup, Tie Leaves U.S. on Sol...  \n",
       "2456  Obama Defends Health Law Ahead of Supreme Cour...  \n",
       "2457                       Deflating â€˜Deflategateâ€™   \n",
       "2458  Brad Pitt to Star in Netflix Film About Afghan...  \n",
       "\n",
       "[2459 rows x 9 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Store file in a variable\n",
    "varfile = pd.read_json(fileclick, lines = True)\n",
    "varfile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Apple's iOS 9 'App thinning' feature will give your phone's storage a boost\"]\n"
     ]
    }
   ],
   "source": [
    "# np.array([varfile['postText'] if varfile['id'] = 608310377143799808])\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "varpd = pd.DataFrame.from_dict(varfile)\n",
    "\n",
    "print(varpd.postText.iloc[0])\n",
    "\n",
    "\n",
    "##define functions to count number of characters and words. return -1 if there are no words/characters present\n",
    "def numChar(content):\n",
    "    if content is None:\n",
    "        return -1\n",
    "    elif not content:\n",
    "        return -1\n",
    "    elif type(content) is str:\n",
    "        return len(content)\n",
    "    else:\n",
    "        \n",
    "        for i in content :\n",
    "            return len(i)\n",
    "        \n",
    "    \n",
    "def numWords(content):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    if content is None:\n",
    "        return -1\n",
    "    elif not content:\n",
    "        return -1\n",
    "    elif type(content) is str:\n",
    "        content_no_punc = tokenizer.tokenize(content)\n",
    "        if(len(content_no_punc)==0):\n",
    "            return -1\n",
    "        else:\n",
    "            return len(content_no_punc)\n",
    "    else:\n",
    "        content_no_punc = tokenizer.tokenize(content[0])\n",
    "        if(len(content_no_punc)==0):\n",
    "            return -1\n",
    "        else:\n",
    "            return len(content_no_punc)\n",
    "        \n",
    "##similar functions like above. but return 0 instead of -1. we need it for counting difference in characters/words\n",
    "        \n",
    "def numChar0(content):\n",
    "    if content is None:\n",
    "        return 0\n",
    "    elif not content:\n",
    "        return 0\n",
    "    elif type(content) is str:\n",
    "        return len(content)\n",
    "    else:\n",
    "        \n",
    "        for i in content :\n",
    "            return len(i)\n",
    "        \n",
    "    \n",
    "def numWords0(content):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    if content is None:\n",
    "        return 0\n",
    "    elif not content:\n",
    "        return 0\n",
    "    elif type(content) is str:\n",
    "        content_no_punc = tokenizer.tokenize(content)\n",
    "        return len(content_no_punc)\n",
    "    else:\n",
    "        content_no_punc = tokenizer.tokenize(content[0])\n",
    "        return len(content_no_punc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: Number of Characters\n",
    "features1 = varpd[['id']].copy()\n",
    "\n",
    "\n",
    "postlen = pd.DataFrame(columns = [\"postlen\"])\n",
    "titlelen = pd.DataFrame(columns = [\"titlelen\"])\n",
    "desclen = pd.DataFrame(columns = [\"desclen\"])\n",
    "keylen = pd.DataFrame(columns = [\"keylen\"])\n",
    "caplen = pd.DataFrame(columns = [\"caplen\"])\n",
    "paralen = pd.DataFrame(columns = [\"paralen\"])\n",
    "\n",
    "for ind in range(len(varpd.index)):\n",
    "    \n",
    "    postlen = postlen.append(pd.DataFrame([numChar(varpd.iloc[ind]['postText'])], columns = [\"postlen\"] ), ignore_index = True)\n",
    "    titlelen = titlelen.append(pd.DataFrame([numChar(varpd.iloc[ind]['targetTitle'])], columns = [\"titlelen\"] ), ignore_index = True)\n",
    "    desclen = desclen.append(pd.DataFrame([numChar(varpd.iloc[ind]['targetDescription'])], columns = [\"desclen\"] ), ignore_index = True)\n",
    "    keylen = keylen.append(pd.DataFrame([numChar(varpd.iloc[ind]['targetKeywords'])], columns = [\"keylen\"] ), ignore_index = True)\n",
    "    caplen = caplen.append(pd.DataFrame([numChar(varpd.iloc[ind]['targetCaptions'])], columns = [\"caplen\"] ), ignore_index = True)\n",
    "    paralen = paralen.append(pd.DataFrame([numChar(varpd.iloc[ind]['targetParagraphs'])], columns = [\"paralen\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "features1  =  pd.concat([features1, postlen, titlelen, desclen, keylen, caplen, paralen], axis=1)\n",
    "#features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataframe used for counting features which involves measuring difference of words/character(notice the functions used)\n",
    "\n",
    "features0 = varpd[['id']].copy()\n",
    "\n",
    "\n",
    "postlen0 = pd.DataFrame(columns = [\"postlen\"])\n",
    "titlelen0 = pd.DataFrame(columns = [\"titlelen\"])\n",
    "desclen0 = pd.DataFrame(columns = [\"desclen\"])\n",
    "keylen0 = pd.DataFrame(columns = [\"keylen\"])\n",
    "caplen0 = pd.DataFrame(columns = [\"caplen\"])\n",
    "paralen0 = pd.DataFrame(columns = [\"paralen\"])\n",
    "\n",
    "for ind in range(len(varpd.index)):\n",
    "    \n",
    "    postlen0 = postlen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['postText'])], columns = [\"postlen\"] ), ignore_index = True)\n",
    "    titlelen0 = titlelen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['targetTitle'])], columns = [\"titlelen\"] ), ignore_index = True)\n",
    "    desclen0 = desclen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['targetDescription'])], columns = [\"desclen\"] ), ignore_index = True)\n",
    "    keylen0 = keylen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['targetKeywords'])], columns = [\"keylen\"] ), ignore_index = True)\n",
    "    caplen0 = caplen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['targetCaptions'])], columns = [\"caplen\"] ), ignore_index = True)\n",
    "    paralen0 = paralen0.append(pd.DataFrame([numChar0(varpd.iloc[ind]['targetParagraphs'])], columns = [\"paralen\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "features0  =  pd.concat([features0, postlen0, titlelen0, desclen0, keylen0, caplen0, paralen0], axis=1)\n",
    "\n",
    "\n",
    "features20 = varpd[['id']].copy()\n",
    "postword0 = pd.DataFrame(columns = [\"postword\"])\n",
    "titleword0 = pd.DataFrame(columns = [\"titleword\"])\n",
    "descword0 = pd.DataFrame(columns = [\"descword\"])\n",
    "keyword0 = pd.DataFrame(columns = [\"keyword\"])\n",
    "capword0 = pd.DataFrame(columns = [\"capword\"])\n",
    "paraword0 = pd.DataFrame(columns = [\"paraword\"])\n",
    "\n",
    "for ind in range(len(varpd.index)):\n",
    "    \n",
    "    postword0 = postword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['postText'])], columns = [\"postword\"] ), ignore_index = True)\n",
    "    titleword0 = titleword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['targetTitle'])], columns = [\"titleword\"] ), ignore_index = True)\n",
    "    descword0 = descword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['targetDescription'])], columns = [\"descword\"] ), ignore_index = True)\n",
    "    keyword0 = keyword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['targetKeywords'])], columns = [\"keyword\"] ), ignore_index = True)\n",
    "    capword0 = capword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['targetCaptions'])], columns = [\"capword\"] ), ignore_index = True)\n",
    "    paraword0 = paraword0.append(pd.DataFrame([numWords0(varpd.iloc[ind]['targetParagraphs'])], columns = [\"paraword\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "features20  =  pd.concat([features20, postword0, titleword0, descword0, keyword0, capword0, paraword0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: Number of Words\n",
    "features2 = varpd[['id']].copy()\n",
    "postword = pd.DataFrame(columns = [\"postword\"])\n",
    "titleword = pd.DataFrame(columns = [\"titleword\"])\n",
    "descword = pd.DataFrame(columns = [\"descword\"])\n",
    "keyword = pd.DataFrame(columns = [\"keyword\"])\n",
    "capword = pd.DataFrame(columns = [\"capword\"])\n",
    "paraword = pd.DataFrame(columns = [\"paraword\"])\n",
    "\n",
    "for ind in range(len(varpd.index)):\n",
    "    \n",
    "    postword = postword.append(pd.DataFrame([numWords(varpd.iloc[ind]['postText'])], columns = [\"postword\"] ), ignore_index = True)\n",
    "    titleword = titleword.append(pd.DataFrame([numWords(varpd.iloc[ind]['targetTitle'])], columns = [\"titleword\"] ), ignore_index = True)\n",
    "    descword = descword.append(pd.DataFrame([numWords(varpd.iloc[ind]['targetDescription'])], columns = [\"descword\"] ), ignore_index = True)\n",
    "    keyword = keyword.append(pd.DataFrame([numWords(varpd.iloc[ind]['targetKeywords'])], columns = [\"keyword\"] ), ignore_index = True)\n",
    "    capword = capword.append(pd.DataFrame([numWords(varpd.iloc[ind]['targetCaptions'])], columns = [\"capword\"] ), ignore_index = True)\n",
    "    paraword = paraword.append(pd.DataFrame([numWords(varpd.iloc[ind]['targetParagraphs'])], columns = [\"paraword\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "features2  =  pd.concat([features2, postword, titleword, descword, keyword, capword, paraword], axis=1)\n",
    "# features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: Difference in Number of words\n",
    "features3 = varpd[['id']].copy()\n",
    "\n",
    "for i in range(1,6):\n",
    "    for j in range(i+1,7):\n",
    "        temp = pd.DataFrame(abs(features20.iloc[:,i]-features20.iloc[:,j]), columns = [\"wordDiff\"+str(i)+str(j)])\n",
    "        features3 = pd.concat([features3, temp], axis = 1)\n",
    "\n",
    "        \n",
    "# features3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: Difference in Number of Characters\n",
    "features4 = varpd[['id']].copy()\n",
    "\n",
    "for i in range(1,6):\n",
    "    for j in range(i+1,7):\n",
    "        temp = pd.DataFrame(abs(features0.iloc[:,i]-features0.iloc[:,j]), columns = [\"charDiff\"+str(i)+str(j)])\n",
    "        features4 = pd.concat([features4, temp], axis = 1)\n",
    "\n",
    "# features4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature: Ratio of number of words\n",
    "features5 = varpd[['id']].copy()\n",
    "\n",
    "for i in range(1,6):\n",
    "    for j in range(i+1,7):\n",
    "        temp = pd.DataFrame(abs(features2.iloc[:,i]/features2.iloc[:,j]), columns = [\"wordRatio\"+str(i)+str(j)])\n",
    "        temp.loc[features2.iloc[:,i] == -1, \"wordRatio\"+str(i)+str(j)] = -1\n",
    "        temp.loc[features2.iloc[:,j] == -1, \"wordRatio\"+str(i)+str(j)] = -1\n",
    "        features5 = pd.concat([features5, temp], axis = 1)\n",
    "\n",
    "# features5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature: Ratio of number of Characters\n",
    "features6 = varpd[['id']].copy()\n",
    "\n",
    "for i in range(1,6):\n",
    "    for j in range(i+1,7):\n",
    "        temp = pd.DataFrame(abs(features1.iloc[:,i]/features1.iloc[:,j]), columns = [\"charRatio\"+str(i)+str(j)])\n",
    "        temp.loc[features1.iloc[:,i] == -1, \"charRatio\"+str(i)+str(j)] = -1\n",
    "        temp.loc[features1.iloc[:,j] == -1, \"charRatio\"+str(i)+str(j)] = -1\n",
    "        features6 = pd.concat([features6, temp], axis = 1)\n",
    "\n",
    "# features6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function: Common words Between Article Keywords and Others\n",
    "\n",
    "def commonWords(keyword, content):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    keyword_no_punc = tokenizer.tokenize(keyword)\n",
    "    if(len(keyword_no_punc)==0):\n",
    "        return 0\n",
    "    elif (content is None):\n",
    "        return 0\n",
    "    elif (not content):\n",
    "        return 0\n",
    "    elif type(content) is str:\n",
    "        content_no_punc = tokenizer.tokenize(content)\n",
    "        return len(list(set(keyword_no_punc) & set(content_no_punc)))\n",
    "    else:\n",
    "        content_no_punc = tokenizer.tokenize(content[0])\n",
    "        return len(list(set(keyword_no_punc) & set(content_no_punc)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: Common words Between Article Keywords and Others\n",
    "\n",
    "\n",
    "features7 = varpd[['id']].copy()\n",
    "\n",
    "commonpost = pd.DataFrame(columns = [\"commonpost\"])\n",
    "commontitle = pd.DataFrame(columns = [\"commontitle\"])\n",
    "commondesc = pd.DataFrame(columns = [\"commondesc\"])\n",
    "commoncap = pd.DataFrame(columns = [\"commoncap\"])\n",
    "commonpara = pd.DataFrame(columns = [\"commonpara\"])\n",
    "\n",
    "for ind in range(len(varpd.index)):\n",
    "    \n",
    "    commonpost = commonpost.append(pd.DataFrame([commonWords(varpd.iloc[ind]['targetKeywords'], varpd.iloc[ind]['postText'])], columns = [\"commonpost\"] ), ignore_index = True)\n",
    "    commontitle = commontitle.append(pd.DataFrame([commonWords(varpd.iloc[ind]['targetKeywords'], varpd.iloc[ind]['targetTitle'])], columns = [\"commontitle\"] ), ignore_index = True)\n",
    "    commondesc = commondesc.append(pd.DataFrame([commonWords(varpd.iloc[ind]['targetKeywords'], varpd.iloc[ind]['targetDescription'])], columns = [\"commondesc\"] ), ignore_index = True)\n",
    "    commoncap = commoncap.append(pd.DataFrame([commonWords(varpd.iloc[ind]['targetKeywords'], varpd.iloc[ind]['targetCaptions'])], columns = [\"commoncap\"] ), ignore_index = True)\n",
    "    commonpara = commonpara.append(pd.DataFrame([commonWords(varpd.iloc[ind]['targetKeywords'], varpd.iloc[ind]['targetParagraphs'])], columns = [\"commonpara\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "features7  =  pd.concat([features7, commonpost, commontitle, commondesc, commoncap, commonpara], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: Presence of an image\n",
    "\n",
    "features8 = varpd[['id']].copy()\n",
    "imagePresent = pd.DataFrame(1, index = np.arange(2459), columns = [\"imagePresent\"])\n",
    "\n",
    "for i in range(len(varpd.index)):\n",
    "    if not varfile.iloc[i,1]:\n",
    "        imagePresent.loc[i, \"imagePresent\"] = 0\n",
    "#np.where( varfile['postMedia'] )\n",
    "#np.arange(2459)\n",
    "#varfile.iloc[:,1]\n",
    "features8  =  pd.concat([features8, imagePresent], axis=1)\n",
    "# features8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function: Number of formal words\n",
    "\n",
    "def formalWords(content):\n",
    "    c=0\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    if content is None:\n",
    "        return 0\n",
    "    elif not content:\n",
    "        return 0\n",
    "    elif type(content) is str:\n",
    "        content_no_punc = tokenizer.tokenize(content)\n",
    "        for word in [x.lower() for x in content_no_punc]:\n",
    "            if word in words.words():\n",
    "                c=c+1\n",
    "        return c\n",
    "    else:\n",
    "        content_no_punc = tokenizer.tokenize(content[0])\n",
    "        for word in [x.lower() for x in content_no_punc]:\n",
    "            if word in words.words():\n",
    "                c=c+1\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature: Number of Formal Words\n",
    "# features9 = varpd[['id']].copy()\n",
    "# postform = pd.DataFrame(columns = [\"postform\"])\n",
    "# titleform = pd.DataFrame(columns = [\"titleform\"])\n",
    "# descform = pd.DataFrame(columns = [\"descform\"])\n",
    "# keyform = pd.DataFrame(columns = [\"keyform\"])\n",
    "# capform = pd.DataFrame(columns = [\"capform\"])\n",
    "# paraform = pd.DataFrame(columns = [\"paraform\"])\n",
    "\n",
    "# for ind in range(len(varpd.index)):\n",
    "    \n",
    "#     postform = postform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['postText'])], columns = [\"postform\"] ), ignore_index = True)\n",
    "#     titleform = titleform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['targetTitle'])], columns = [\"titleform\"] ), ignore_index = True)\n",
    "#     descform = descform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['targetDescription'])], columns = [\"descform\"] ), ignore_index = True)\n",
    "#     keyform = keyform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['targetKeywords'])], columns = [\"keyform\"] ), ignore_index = True)\n",
    "#     capform = capform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['targetCaptions'])], columns = [\"capform\"] ), ignore_index = True)\n",
    "#     paraform = paraform.append(pd.DataFrame([formalWords(varpd.iloc[ind]['targetParagraphs'])], columns = [\"paraform\"] ), ignore_index = True)\n",
    "#     if(ind%10 == 0): \n",
    "#         print(ind)\n",
    "    \n",
    "    \n",
    "# features9  =  pd.concat([features9, postform, titleform, descform, keyform, capform, paraform], axis=1)\n",
    "# features9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Feature: Ratio of number of formal words to total number of words\n",
    "# def formalRatio(formal, total):\n",
    "#     if (total==-1):\n",
    "#         return -1\n",
    "#     else:\n",
    "#         return abs(formal/total)\n",
    "\n",
    "\n",
    "# features10 = varpd[['id']].copy()\n",
    "# postratio = pd.DataFrame(columns = [\"postratio\"])\n",
    "# titleratio = pd.DataFrame(columns = [\"titleratio\"])\n",
    "# descratio = pd.DataFrame(columns = [\"descratio\"])\n",
    "# keyratio = pd.DataFrame(columns = [\"keyratio\"])\n",
    "# capratio = pd.DataFrame(columns = [\"capratio\"])\n",
    "# pararatio = pd.DataFrame(columns = [\"pararatio\"])\n",
    "\n",
    "# for ind in range(len(varpd.index)):\n",
    "    \n",
    "#     postratio = postratio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['postform'], features2.iloc[ind]['postword'] )], columns = [\"postratio\"] ), ignore_index = True)\n",
    "#     titleratio = titleratio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['titleform'],features2.iloc[ind]['titleword'])], columns = [\"titleratio\"] ), ignore_index = True)\n",
    "#     descratio = descratio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['descform'],features2.iloc[ind]['descword'])], columns = [\"descratio\"] ), ignore_index = True)\n",
    "#     keyratio = keyratio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['keyform'],features2.iloc[ind]['keyword'])], columns = [\"keyratio\"] ), ignore_index = True)\n",
    "#     capratio = capratio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['capform'],features2.iloc[ind]['capword'])], columns = [\"capratio\"] ), ignore_index = True)\n",
    "#     pararatio = pararatio.append(pd.DataFrame([formalRatio(features9.iloc[ind]['paraform'],features2.iloc[ind]['paraword'])], columns = [\"pararatio\"] ), ignore_index = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "# features10  =  pd.concat([features10, postratio, titleratio, descratio, keyratio, capratio, pararatio], axis=1)\n",
    "# features10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# twenty = fetch_20newsgroups()\n",
    "# twenty.data\n",
    "raw_documents1 = varpd[\"postText\"]\n",
    "temp1 = pd.Series.tolist(raw_documents1)\n",
    "temp4 = [item for items in temp1 for item in items]\n",
    "raw_documents2 = varpd[\"targetParagraphs\"]\n",
    "temp2 = pd.Series.tolist(raw_documents2)\n",
    "temp3 = [' '.join(item) for item in temp2]\n",
    "vocab = temp4 + temp3\n",
    "\n",
    "# newlist\n",
    "tfidf = TfidfVectorizer().fit_transform(vocab, y = None)\n",
    "# tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear_kernel(tfidf[0:1], tfidf[2459:2459+1]).flatten()\n",
    "# i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarities = []\n",
    "for i in range(int(len(vocab)/2)):\n",
    "    cosine_similarities.append(linear_kernel(tfidf[i:i+1], tfidf[2459+i:2459+i+1]).flatten())\n",
    "cosine_similarities = np.concatenate(cosine_similarities).ravel().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "features12 = varpd[['id']].copy()\n",
    "tfidfcosine = pd.DataFrame(cosine_similarities, columns = [\"tfidfcosine\"])\n",
    "features12 = pd.concat([features12, tfidfcosine], axis = 1) \n",
    "# features12 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalFeaturesT = pd.merge(features1, features2,  on='id')\n",
    "finalFeaturesT = pd.merge(finalFeaturesT, features3,  on='id')\n",
    "finalFeaturesT = pd.merge(finalFeaturesT, features4,  on='id')\n",
    "finalFeaturesT = pd.merge(finalFeaturesT, features5,  on='id')\n",
    "finalFeaturesT = pd.merge(finalFeaturesT, features6,  on='id')\n",
    "finalFeaturesT = pd.merge(finalFeaturesT, features7,  on='id')\n",
    "finalFeaturesT = pd.merge(finalFeaturesT, features8,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features11,  on='id')\n",
    "finalFeaturesT = pd.merge(finalFeaturesT, features12,  on='id')\n",
    "# finalFeaturesT = pd.merge(finalFeaturesT, features,  on='id')\n",
    "\n",
    "# finalFeatures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Getting the Labels\n",
    "testLabelsFile = 'traintruth1703.jsonl'\n",
    "Labels = pd.read_json(testLabelsFile, lines=True)\n",
    "\n",
    "Labels.loc[Labels.iloc[:,1] == \"no-clickbait\", \"truthClass\"] = -1\n",
    "Labels.loc[Labels.iloc[:,1] == \"clickbait\", \"truthClass\"] = 1\n",
    "finalFeatures = pd.merge(finalFeaturesT, Labels[[\"id\",\"truthClass\"]], on = \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Final Features with Truth Mean\n",
    "finalFeaturesMSE = pd.merge(finalFeaturesT, Labels[[\"id\",\"truthMean\"]], on = \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(pd):\n",
    "    featNorm = pd.copy()\n",
    "    for k in range(len(pd.columns)):\n",
    "        m = np.mean(pd.iloc[:,k])\n",
    "        s = np.std(pd.iloc[:,k])\n",
    "        featNorm.iloc[:,k] = (pd.iloc[:,k]-m)/s\n",
    "    featNorm = featNorm.iloc[:,1:k]\n",
    "    return(featNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(classifier, featuresel, numFeat, allfeatures, featurelist = finalFeatures):\n",
    "    #Choose Classifier\n",
    "    if(classifier == \"SVM\"):\n",
    "        clf = SVC()\n",
    "    elif(classifier == \"Log\"):\n",
    "        clf = LogisticRegressionCV()\n",
    "    elif(classifier == \"RandomForest\"):\n",
    "        clf = RandomForestClassifier(criterion = 'entropy')\n",
    "    elif(classifier == \"XgBoost\"):\n",
    "        clf = GradientBoostingClassifier()\n",
    "    elif(classifier == \"NaiveBayes\"):\n",
    "        clf = GaussianNB()\n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "    # select features we want\n",
    "    x = [col for col in allfeatures if col in featurelist]\n",
    "    x = allfeatures[x]\n",
    "\n",
    "    \n",
    "    featNorm = normalize(x)\n",
    "    #print(featNorm)\n",
    "    #print(featNorm)\n",
    "    # Labels\n",
    "    y = allfeatures.iloc[:,-1]\n",
    "    \n",
    "    if(featuresel== \"chi\"):\n",
    "        X_new = SelectKBest(chi2, k=numFeat).fit_transform(featNorm, y)\n",
    "    elif(featuresel == \"f_classif\"):\n",
    "        X_new = SelectKBest(f_classif, k=numFeat).fit_transform(featNorm, y)\n",
    "    elif(featuresel == \"mutual_info\"):\n",
    "        X_new = SelectKBest(mutual_info_classif, k=numFeat).fit_transform(featNorm, y)\n",
    "    else:\n",
    "        X_new = featNorm\n",
    "    #Classifier result\n",
    "    scores = cross_val_score(clf, X_new, y, cv=10)\n",
    "    accuracy = [scores.mean(), scores.std()]\n",
    "    return(accuracy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def useNirmalFunctionEfficiently(classifier, featuresel, numFeat,df):\n",
    "    accuracy = getAccuracy(classifier, featuresel, numFeat, df, df.iloc[:,range(len(df.columns)-1)])\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log\n",
      "10\n",
      "50\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "k should be >=0, <= n_features; got 100.Use k='all' to return all features.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-a52f89433692>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0maccuracies\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mnfeat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnrFeats\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[1;33m[\u001b[0m\u001b[0macc1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0macc2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0museNirmalFunctionEfficiently\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassif\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"f_classif\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnfeat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfinalFeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0maccuracies\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0macc1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnfeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-03dbe9864a4f>\u001b[0m in \u001b[0;36museNirmalFunctionEfficiently\u001b[1;34m(classifier, featuresel, numFeat, df)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0museNirmalFunctionEfficiently\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFeat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0maccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAccuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeaturesel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFeat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-79-61b68ca8138d>\u001b[0m in \u001b[0;36mgetAccuracy\u001b[1;34m(classifier, featuresel, numFeat, allfeatures, featurelist)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchi2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumFeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"f_classif\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_classif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumFeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[1;32melif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeaturesel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"mutual_info\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mX_new\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSelectKBest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmutual_info_classif\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnumFeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatNorm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    346\u001b[0m                             % (self.score_func, type(self.score_func)))\n\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[0mscore_func_ret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore_func_ret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\univariate_selection.py\u001b[0m in \u001b[0;36m_check_params\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    491\u001b[0m             raise ValueError(\"k should be >=0, <= n_features; got %r.\"\n\u001b[0;32m    492\u001b[0m                              \u001b[1;34m\"Use k='all' to return all features.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m                              % self.k)\n\u001b[0m\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_support_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: k should be >=0, <= n_features; got 100.Use k='all' to return all features."
     ]
    }
   ],
   "source": [
    "classifiers = [\"Log\",\"SVM\",\"RandomForest\",\"XgBoost\",\"NaiveBayes\"]\n",
    "nrFeats = [10,50,100,150,250,300,350,400,450, 500,550,600]\n",
    "resultsposPatterns=pd.DataFrame(index = nrFeats)\n",
    "for classif in classifiers:\n",
    "    print(classif)\n",
    "    accuracies=[]\n",
    "    for nfeat in nrFeats:\n",
    "        [acc1,acc2]=useNirmalFunctionEfficiently(classif,\"f_classif\",nfeat,finalFeatures)\n",
    "        accuracies.append(acc1)\n",
    "        print(nfeat)\n",
    "    resultsposPatterns[classif]=accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsposPatterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"SVM\", \"none\", 0,  finalFeatures, finalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"Log\",  \"none\", 0, finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"RandomForest\", \"none\", finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"XgBoost\", \"none\", finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"Log\",  \"f_classif\", 40, finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"Log\",  \"f_classif\", 60, finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"Log\",  \"f_classif\", 30, finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info_gain = getAccuracy(\"SVM\",  \"mutual_info\", 500, finalFeatures, finalFeatures.iloc[:,range(len(finalFeatures.columns)-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(featwithLab.iloc[:,1:91], featwithLab.iloc[:,91], test_size=0.4, random_state=0)\n",
    "\n",
    "# clf = LogisticRegressionCV()\n",
    "# scores = cross_val_score(clf, featwithLab.iloc[:,1:81], featwithLab.iloc[:,81], cv=10)\n",
    "# # clf.fit(X_train, y_train)\n",
    "# # clf.score(X_test, y_test)\n",
    "# print(\"Accuracy: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(featwithLab.iloc[:,1:91], featwithLab.iloc[:,91], test_size=0.4, random_state=0)\n",
    "\n",
    "# clf = GradientBoostingClassifier()\n",
    "# scores = cross_val_score(clf, featwithLab.iloc[:,1:81], featwithLab.iloc[:,81], cv=10)\n",
    "# # clf.fit(X_train, y_train)\n",
    "# # clf.score(X_test, y_test)\n",
    "# print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# import matplotlib.pyplot as plt\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# # X_train, X_test, y_train, y_test = train_test_split(featwithLab.iloc[:,1:91], featwithLab.iloc[:,91], test_size=0.4, random_state=0)\n",
    "\n",
    "# clf = RandomForestClassifier(criterion = 'entropy')\n",
    "# scores = cross_val_score(clf, featwithLab.iloc[:,1:81], featwithLab.iloc[:,81], cv=20)\n",
    "# # clf.fit(X_train, y_train)\n",
    "# # clf.score(X_test, y_test)\n",
    "# print(\"Accuracy: %0.3f (+/- %0.3f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "# import operator\n",
    "\n",
    "# info_gain = dict(zip(list(featwithLab.iloc[:,1:81]), mutual_info_classif( featwithLab.iloc[:,1:81], featwithLab.iloc[:,81])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c = 0\n",
    "# for w in sorted(info_gain, key = info_gain.get, reverse = True):\n",
    "#     c=c+1\n",
    "#     print (w, info_gain[w])\n",
    "#     if (c==20):\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stemming and Word Cloud\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.stem import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer # count unigrams\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve words from postTitles\n",
    "postTitles = varpd[\"postText\"].tolist()\n",
    "\n",
    "def stemmingPostTitles(postTitles,stem):\n",
    "    tokenizer =  TweetTokenizer()\n",
    "    if (stem==\"porter\"):\n",
    "        stemmer = nl.PorterStemmer() # Porter Stemming\n",
    "    elif (stem == \"snowball\"):\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "    elif (stem == \"lancaster\"):\n",
    "        stemmer = LancasterStemmer()\n",
    "    \n",
    "    number_pattern = re.compile(r'\\d+')\n",
    "    corpus=[]\n",
    "    for title in postTitles:\n",
    "        title = title[0]\n",
    "        \n",
    "        title=title.lower()\n",
    "        tokens = tokenizer.tokenize(title)\n",
    "        stem_tokens = [stemmer.stem(t) for t in tokens ]\n",
    "        stem_title = \" \".join(stem_tokens)\n",
    "        stem_title = number_pattern.sub('[N]', stem_title)\n",
    "        corpus.append(stem_title)\n",
    "    return(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-97-4400d9951d98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSTOPWORDS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow_wordcloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     wordcloud = WordCloud(\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        #stopwords=stopwords,\n",
    "        max_words=50,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from scipy.misc import imread\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "text = 'all your base are belong to us all of your base base base'\n",
    "wordcloud = WordCloud(\n",
    "                      relative_scaling = 1.0,\n",
    "                      stopwords = {'to', 'of'} # set or space-separated string\n",
    "                      ).generate(text)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "addLabel = pd.merge(varpd, Labels[[\"id\",\"truthClass\"]], on = \"id\")\n",
    "clickbait = addLabel.loc[addLabel[\"truthClass\"]==1]\n",
    "nonclickbait = addLabel.loc[addLabel[\"truthClass\"]==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(clickbait[\"postText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(nonclickbait[\"postText\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmedclickbait=stemmingPostTitles(clickbait[\"postText\"].tolist(),\"porter\")\n",
    "show_wordcloud(stemmedclickbait)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nonstemmedclickbait=stemmingPostTitles(nonclickbait[\"postText\"].tolist(),\"porter\")\n",
    "show_wordcloud(nonstemmedclickbait)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of Classifier results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# normalise features\n",
    "featNorm = finalFeatures.copy()\n",
    "for k in range(len(featNorm.columns)):\n",
    "    m = np.mean(finalFeatures.iloc[:,k])\n",
    "    s = np.std(finalFeatures.iloc[:,k])\n",
    "    featNorm.iloc[:,k] = (finalFeatures.iloc[:,k]-m)/s\n",
    "y = finalFeatures.iloc[:,-1]\n",
    "\n",
    "\n",
    "#X_new = SelectKBest(f_classif, k=500).fit_transform(featNorm.iloc[:,1:len(featNorm.columns)-1], y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(featNorm.iloc[:,1:len(featNorm.columns)-1], finalFeatures.iloc[:,-1], test_size=0.2, random_state=0)\n",
    "clf = LogisticRegressionCV()\n",
    "# scores = cross_val_score(clf, featwithLab.iloc[:,1:81], featwithLab.iloc[:,81], cv=10)\n",
    "clf.fit(X_train, y_train)\n",
    "scores = clf.score(X_test, y_test)\n",
    "print(\"Accuracy: %0.3f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "misclassified = np.where(y_test != clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3098820658804392"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(finalFeatures.loc[finalFeatures[\"truthClass\"]==1])/len(finalFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# misclassified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis = X_test.iloc[misclassified]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "mis1 = mis.join(varpd, how= \"inner\")\n",
    "# mis1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.merge(mis1, Labels[[\"id\",\"truthClass\"]], on = \"id\").to_csv(\"misclassified.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finalFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   # MSE\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "posPatterns = pd.read_csv(\"posPatterns_ALL_SVM.csv\")\n",
    "unigrams = pd.read_csv(\"unigrams_select250_Log.csv\")\n",
    "mixed = pd.read_csv(\"mixtureDF_AllFeatures_SVM.csv\") \n",
    "\n",
    "posPatternClass = posPatterns.iloc[:,1:len(posPatterns.columns)]\n",
    "unigramsClass = unigrams.iloc[:,1:len(unigrams.columns)]\n",
    "mixedClass = mixed.iloc[:,1:len(mixed.columns)]\n",
    "\n",
    "posPatternM = posPatterns.iloc[:,1:len(posPatterns.columns)-1]\n",
    "unigramsM = unigrams.iloc[:,1:len(unigrams.columns)-1]\n",
    "mixedM = mixed.iloc[:,1:len(mixed.columns)-1]\n",
    "\n",
    "posPatternM = pd.merge(posPatternM, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "unigramsM = pd.merge(unigramsM, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "mixedM = pd.merge(mixedM, Labels[[\"id\",\"truthMean\"]], on = \"id\")\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def computeMSE(finalFeaturesMSE, cross_val, featuresel, numFeat, regressor):\n",
    "    \n",
    "    ##set up cross validation\n",
    "    cv_idx = KFold(n=len(varpd), n_folds=cross_val, shuffle=True, random_state=1)\n",
    "    \n",
    "    featNormMSE = normalize(finalFeaturesMSE)\n",
    "    \n",
    "    #only works with nd numpy arrays\n",
    "    y = finalFeaturesMSE.truthMean.values\n",
    "    X = featNormMSE.as_matrix(columns = featNormMSE.columns[1:len(featNormMSE.columns)-1])\n",
    "    \n",
    "    #feature selection\n",
    "    if(featuresel== \"chi\"):\n",
    "        X_new = SelectKBest(chi2, k=numFeat).fit_transform(X, y)\n",
    "    elif(featuresel == \"f_classif\"):\n",
    "        X_new = SelectKBest(f_classif, k=numFeat).fit_transform(X, y)\n",
    "    elif(featuresel == \"mutual_info\"):\n",
    "        X_new = SelectKBest(mutual_info_classif, k=numFeat).fit_transform(X, y)\n",
    "    else:\n",
    "        X_new = X\n",
    "    print(X_new)\n",
    "    #regression\n",
    "    y_out = pd.DataFrame()\n",
    "    scores = []\n",
    "    count = 0\n",
    "    for train_idx, val_idx in cv_idx:\n",
    "            if(regressor == \"LinearSVR\"):\n",
    "                clr = LinearSVR(C=0.001, loss='squared_epsilon_insensitive', dual=False, random_state=1)\n",
    "            elif(regressor == \"Log\"):\n",
    "                clr = LogisticRegression()\n",
    "            elif(regressor == \"XgBoost\"):\n",
    "                clr = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n",
    "                           colsample_bytree=1, max_depth=7)\n",
    "            clr.fit(X_new[train_idx], y[train_idx])\n",
    "\n",
    "            y_pred = clr.predict(X_new[val_idx])\n",
    "            y_pred[y_pred < 0] = 0.0\n",
    "            y_pred[y_pred > 1] = 1.0\n",
    "#             y_out[\"c\"+str(count)]=y_pred\n",
    "#             count = count + 1\n",
    "            \n",
    "#     return(y_out.mean(axis=1))\n",
    "            rmse = mean_squared_error(y[val_idx], y_pred)\n",
    "            scores.append(rmse)\n",
    "\n",
    "    m = np.mean(scores)\n",
    "    s = np.std(scores)\n",
    "    print(\"MSE: %0.3f (+/- %0.2f)\" % (m, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMixedAccuracy(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE, cross_val):\n",
    "    \n",
    "    ##set up cross validation\n",
    "    cv_idx = KFold(n=len(varpd), n_folds=cross_val, shuffle=True, random_state=1)\n",
    "    \n",
    "    yclass = finalFeatures.iloc[:,-1]\n",
    "    posPatternMMSE = normalize(posPatternClass)\n",
    "    unigramsMMSE = normalize(unigramsClass)\n",
    "    mixedMMSE = normalize(mixedClass)\n",
    "    finalFeaturesNormMSE = normalize(finalFeatures)\n",
    "    \n",
    "    #only works with nd numpy arrays\n",
    "    \n",
    "    ycont = finalFeaturesMSE.truthMean.values\n",
    "    yp = posPatternClass.truthClass.values\n",
    "    yu = unigramsClass.truthClass.values\n",
    "    ym = mixedClass.truthClass.values\n",
    "    yf = finalFeatures.truthClass.values\n",
    "    \n",
    "    Xp = posPatternMMSE.as_matrix(columns = posPatternMMSE.columns[1:len(posPatternMMSE.columns)-1])\n",
    "    Xu = unigramsMMSE.as_matrix(columns = unigramsMMSE.columns[1:len(unigramsMMSE.columns)-1])\n",
    "    Xm = mixedMMSE.as_matrix(columns = mixedMMSE.columns[1:len(mixedMMSE.columns)-1])\n",
    "    Xf = finalFeaturesNormMSE.as_matrix(columns = finalFeaturesNormMSE.columns[1:len(finalFeaturesNormMSE.columns)-1])\n",
    "    \n",
    "\n",
    "    Xp_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xp, yp)\n",
    "    Xu_new = SelectKBest(f_classif, k=250).fit_transform(Xu, yu)\n",
    "    Xm_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xm, ym)\n",
    "    Xf_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xf, yf)\n",
    "    \n",
    "    \n",
    "    y_out = pd.DataFrame()\n",
    "    scores = []\n",
    "    acc = []\n",
    "    scoresM = []\n",
    "    count = 0\n",
    "    for train_idx, val_idx in cv_idx:\n",
    "           \n",
    "            clrp = SVC(probability = True)\n",
    "            clru = LogisticRegression()\n",
    "            clrm = SVC(probability = True)\n",
    "            clrf = LogisticRegression()\n",
    "                           \n",
    "            clrp.fit(Xp_new[train_idx], yp[train_idx])\n",
    "            clru.fit(Xu_new[train_idx], yu[train_idx])\n",
    "            clrm.fit(Xm_new[train_idx], ym[train_idx])\n",
    "            clrf.fit(Xf_new[train_idx], yf[train_idx])\n",
    "\n",
    "            yp_pred = clrp.predict_proba(Xp_new[val_idx])[:,1]\n",
    "            \n",
    "            \n",
    "            yu_pred = clru.predict_proba(Xu_new[val_idx])[:,1]\n",
    "            \n",
    "            \n",
    "            ym_pred = clrm.predict_proba(Xm_new[val_idx])[:,1]\n",
    "           \n",
    "            \n",
    "            yf_pred = clrf.predict_proba(Xf_new[val_idx])[:,1]\n",
    "            \n",
    "#             y_out[\"c\"+str(count)]=y_pred\n",
    "#             count = count + 1\n",
    "            \n",
    "            d= {'p':yp_pred.tolist(), 'u':yu_pred.tolist(), 'm':ym_pred.tolist(), 'f':yf_pred.tolist()}\n",
    "            y_pred_final = pd.DataFrame(d)\n",
    "            y_pred_final[\"mean\"] = y_pred_final.mean(axis=1)\n",
    "            y_pred_final[\"median\"] = y_pred_final.iloc[:,0:4].median(axis=1)\n",
    "            y_pred_final['class'] = np.where(y_pred_final['mean'] >= 0.5, 1,-1)\n",
    "#             y_pred_final['class'] = np.where(y_pred_final['median'] >= 0.5, 1,-1)\n",
    "            \n",
    "            rmse = mean_squared_error(ycont[val_idx], y_pred_final[\"mean\"])\n",
    "            scores.append(rmse)\n",
    "            \n",
    "            rmsemed = mean_squared_error(ycont[val_idx], y_pred_final[\"median\"])\n",
    "            scoresM.append(rmsemed)\n",
    "            \n",
    "            accuracy = accuracy_score(yclass[val_idx], y_pred_final[\"class\"])\n",
    "            acc.append(accuracy)\n",
    "    \n",
    "#             rmse = mean_squared_error(y[val_idx], y_pred)\n",
    "#             scores.append(rmse)\n",
    "\n",
    "    mM = np.mean(scores)\n",
    "    sM = np.std(scores)\n",
    "    print(\"MSE: %0.3f (+/- %0.2f)\" % (mM, sM))\n",
    "#     return(y_pred_final)\n",
    "    mA = np.mean(acc)\n",
    "    sA = np.std(acc)\n",
    "    print(\"Accuracy: %0.3f (+/- %0.2f)\" % (mA, sA))\n",
    "    \n",
    "    mMed = np.mean(scoresM)\n",
    "    sMed = np.std(scoresM)\n",
    "    print(\"MSEmedian: %0.3f (+/- %0.2f)\" % (mMed, sMed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeMixedAccuracyLog(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE, cross_val,tdepth):\n",
    "    \n",
    "    ##set up cross validation\n",
    "    cv_idx = KFold(n=len(varpd), n_folds=cross_val, shuffle=True, random_state=1)\n",
    "    \n",
    "    yclass = finalFeatures.iloc[:,-1]\n",
    "    posPatternMMSE = normalize(posPatternClass)\n",
    "    unigramsMMSE = normalize(unigramsClass)\n",
    "    mixedMMSE = normalize(mixedClass)\n",
    "    finalFeaturesNormMSE = normalize(finalFeatures)\n",
    "    \n",
    "    #only works with nd numpy arrays\n",
    "    \n",
    "    ycont = finalFeaturesMSE.truthMean.values\n",
    "    yp = posPatternClass.truthClass.values\n",
    "    yu = unigramsClass.truthClass.values\n",
    "    ym = mixedClass.truthClass.values\n",
    "    yf = finalFeatures.truthClass.values\n",
    "    \n",
    "    Xp = posPatternMMSE.as_matrix(columns = posPatternMMSE.columns)\n",
    "    Xu = unigramsMMSE.as_matrix(columns = unigramsMMSE.columns)\n",
    "    Xm = mixedMMSE.as_matrix(columns = mixedMMSE.columns)\n",
    "    Xf = finalFeaturesNormMSE.as_matrix(columns = finalFeaturesNormMSE.columns)\n",
    "    \n",
    "\n",
    "    Xp_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xp, yp)\n",
    "    Xu_new = SelectKBest(f_classif, k=250).fit_transform(Xu, yu)\n",
    "    Xm_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xm, ym)\n",
    "    Xf_new = SelectKBest(f_classif, k=\"all\").fit_transform(Xf, yf)\n",
    "    \n",
    "    \n",
    "    y_out = pd.DataFrame()\n",
    "#     scores = []\n",
    "    accM = []\n",
    "    accS = []\n",
    "#     scoresM = []\n",
    "    count = 0\n",
    "    for train_idx, val_idx in cv_idx:\n",
    "           \n",
    "            clrp = SVC(probability = True)\n",
    "            clru = LogisticRegression()\n",
    "            clrm = SVC(probability = True)\n",
    "            clrf = LogisticRegression()\n",
    "                           \n",
    "            clrp.fit(Xp_new[train_idx], yp[train_idx])\n",
    "            clru.fit(Xu_new[train_idx], yu[train_idx])\n",
    "            clrm.fit(Xm_new[train_idx], ym[train_idx])\n",
    "            clrf.fit(Xf_new[train_idx], yf[train_idx])\n",
    "\n",
    "            yp_pred = clrp.predict_proba(Xp_new[val_idx])[:,1]\n",
    "            yp_pred_train = clrp.predict_proba(Xp_new[train_idx])[:,1]\n",
    "            \n",
    "            \n",
    "            yu_pred = clru.predict_proba(Xu_new[val_idx])[:,1]\n",
    "            yu_pred_train = clru.predict_proba(Xu_new[train_idx])[:,1]\n",
    "            \n",
    "            ym_pred = clrm.predict_proba(Xm_new[val_idx])[:,1]\n",
    "            ym_pred_train = clrm.predict_proba(Xm_new[train_idx])[:,1]\n",
    "           \n",
    "            \n",
    "            yf_pred = clrf.predict_proba(Xf_new[val_idx])[:,1]\n",
    "            yf_pred_train = clrf.predict_proba(Xf_new[train_idx])[:,1]\n",
    "            \n",
    "#             y_out[\"c\"+str(count)]=y_pred\n",
    "#             count = count + 1\n",
    "            \n",
    "            d1= {'p':yp_pred.tolist(), 'u':yu_pred.tolist(), 'm':ym_pred.tolist(), 'f':yf_pred.tolist(), 'y': yclass[val_idx].tolist(), 'ycont': ycont[val_idx].tolist()}\n",
    "            d2= {'p':yp_pred_train.tolist(), 'u':yu_pred_train.tolist(), 'm':ym_pred_train.tolist(), 'f':yf_pred_train.tolist(), 'y': yclass[train_idx].tolist(), 'ycont': ycont[train_idx].tolist()}\n",
    "            df1 = pd.DataFrame(d1)\n",
    "            df2 = pd.DataFrame(d2)\n",
    "            \n",
    "            d= pd.concat([df1, df2])\n",
    "            \n",
    "            clf = RandomForestRegressor(max_depth=tdepth, random_state=0) #i used random forest\n",
    "            \n",
    "#             scores = cross_val_score(clf, d[[\"p\", \"u\", \"m\", \"f\"]], d[\"y\"], cv=10)\n",
    "            #rand = pd.DataFrame(np.random.randn(2459, 2))\n",
    "\n",
    "            #msk = np.random.rand(len(rand)) < 0.8\n",
    "\n",
    "            #train = d[msk]\n",
    "            X_train, y_train, ycont_train = df2[[\"p\", \"u\", \"m\", \"f\"]], df2[\"y\"], df2[\"ycont\"]\n",
    "            #test = d[~msk]\n",
    "            X_test, y_test, ycont_test = df1[[\"p\", \"u\", \"m\", \"f\"]], df1[\"y\"], df1[\"ycont\"]\n",
    "#             X_train, X_test, y_train, y_test = train_test_split( d[[\"p\", \"u\", \"m\", \"f\"]],  d[\"y\"], test_size=0.4, random_state=0)\n",
    "#             prob = cl.predict_proba\n",
    "            clf.fit(X_train, ycont_train) # i changed this to work with regression\n",
    "            #score = clf.score(X_test, ycont_test) ## i removed this so that it worked with regression\n",
    "            prob = clf.predict(X_test)\n",
    "            \n",
    "            lab_pred = prob.copy() # i added this to work with regression\n",
    "            lab_pred=np.where(lab_pred >= 0.5, 1,-1)\n",
    "            score = accuracy_score(lab_pred,y_test)\n",
    "            rmse = mean_squared_error(ycont_test, prob)\n",
    "#             accuracyM = scores.mean()\n",
    "#             accuracyS = scores.std()\n",
    "            accM.append(score)\n",
    "            accS.append(rmse)\n",
    "#             y1 = { 'y': yclass[val_idx].tolist()}\n",
    "#             y2 = { 'y': yclass[train_idx].tolist()}\n",
    "#             yf1 = pd.DataFrame(y1)\n",
    "#             yf2 = pd.DataFrame(y2)\n",
    "#             ylab = pd.concat([y1, y2]) \n",
    "            \n",
    "#             y_pred_final = pd.DataFrame(d)\n",
    "#             y_pred_final = pd.DataFrame(d)\n",
    "#             y_pred_final[\"mean\"] = y_pred_final.mean(axis=1)\n",
    "#             y_pred_final[\"median\"] = y_pred_final.iloc[:,0:4].median(axis=1)\n",
    "#             y_pred_final['class'] = np.where(y_pred_final['mean'] >= 0.5, 1,-1)\n",
    "# #             y_pred_final['class'] = np.where(y_pred_final['median'] >= 0.5, 1,-1)\n",
    "            \n",
    "#             rmse = mean_squared_error(ycont[val_idx], y_pred_final[\"mean\"])\n",
    "#             scores.append(rmse)\n",
    "            \n",
    "#             rmsemed = mean_squared_error(ycont[val_idx], y_pred_final[\"median\"])\n",
    "#             scoresM.append(rmsemed)\n",
    "            \n",
    "#             accuracy = accuracy_score(yclass[val_idx], y_pred_final[\"class\"])\n",
    "#             acc.append(accuracy)\n",
    "    \n",
    "#             rmse = mean_squared_error(y[val_idx], y_pred)\n",
    "#             scores.append(rmse)\n",
    "\n",
    "    mS = np.mean(accS)\n",
    "    sS = np.std(accS)\n",
    "    print(\"MSE: %0.3f (+/- %0.2f)\" % (mS, sS))\n",
    "\n",
    "#     return(d)\n",
    "    mA = np.mean(accM)\n",
    "    sA = np.std(accM)\n",
    "    print(\"Accuracy: %0.3f (+/- %0.2f)\" % (mA, sA))\n",
    "    \n",
    "#     mMed = np.mean(scoresM)\n",
    "#     sMed = np.std(scoresM)\n",
    "#     print(\"MSEmedian: %0.3f (+/- %0.2f)\" % (mMed, sMed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.cross_validation.KFold(n=2459, n_folds=2, shuffle=True, random_state=1)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "MSE: 0.045 (+/- 0.00)\n",
      "Accuracy: 0.749 (+/- 0.03)\n",
      "3\n",
      "MSE: 0.041 (+/- 0.00)\n",
      "Accuracy: 0.769 (+/- 0.03)\n",
      "4\n",
      "MSE: 0.039 (+/- 0.00)\n",
      "Accuracy: 0.776 (+/- 0.03)\n",
      "5\n",
      "MSE: 0.039 (+/- 0.00)\n",
      "Accuracy: 0.779 (+/- 0.03)\n",
      "6\n",
      "MSE: 0.039 (+/- 0.00)\n",
      "Accuracy: 0.782 (+/- 0.03)\n",
      "7\n",
      "MSE: 0.039 (+/- 0.00)\n",
      "Accuracy: 0.771 (+/- 0.02)\n",
      "8\n",
      "MSE: 0.040 (+/- 0.00)\n",
      "Accuracy: 0.770 (+/- 0.02)\n",
      "9\n",
      "MSE: 0.040 (+/- 0.00)\n",
      "Accuracy: 0.770 (+/- 0.02)\n",
      "10\n",
      "MSE: 0.040 (+/- 0.00)\n",
      "Accuracy: 0.771 (+/- 0.02)\n",
      "15\n",
      "MSE: 0.041 (+/- 0.00)\n",
      "Accuracy: 0.767 (+/- 0.02)\n",
      "20\n",
      "MSE: 0.042 (+/- 0.00)\n",
      "Accuracy: 0.762 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "# #only works with nd numpy arrays\n",
    "# posPatternMMSE = normalize(posPatternClass)\n",
    "# y = posPatternClass.truthClass.values\n",
    "# X = posPatternMMSE.as_matrix(columns = posPatternMMSE.columns[1:len(posPatternMMSE.columns)-1])\n",
    "\n",
    "# X_new = SelectKBest(f_classif, k=\"all\").fit_transform(X, y)\n",
    "for tdepth in [2,3,4,5,6,7,8,9,10,15,20]:\n",
    "    print(tdepth)\n",
    "    computeMixedAccuracyLog(posPatternClass, unigramsClass,  mixedClass, finalFeatures, finalFeaturesMSE,10,tdepth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.82751754 -1.33042654 -0.99268915 ... -0.24765905 -0.20373472\n",
      "  -0.19714909]\n",
      " [ 0.82751754 -1.33042654  0.04339855 ... -0.24765905 -0.20373472\n",
      "  -0.19714909]\n",
      " [-1.59062344 -0.34384063  0.04339855 ... -0.24765905 -0.20373472\n",
      "  -0.19714909]\n",
      " ...\n",
      " [-0.98608819 -0.34384063 -0.99268915 ... -0.24765905 -0.20373472\n",
      "  -0.19714909]\n",
      " [ 0.2229823   1.62933117 -0.99268915 ... -0.24765905 -0.20373472\n",
      "  -0.19714909]\n",
      " [-0.38155295 -1.33042654  0.04339855 ...  3.92353005 -0.20373472\n",
      "  -0.19714909]]\n",
      "MSE: 0.054 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "# ypredict = pd.DataFrame()\n",
    "# ypredict[\"y1\"] = computeMSE(posPatternM, 10, \"f_Classif\",\"all\", \"LinearSVR\")\n",
    "# ypredict[\"y2\"] = computeMSE(unigramsM, 10, \"f_Classif\",\"all\" ,\"Log\")\n",
    "# ypredict[\"y3\"] = computeMSE(mixedM, 10, \"f_Classif\",\"all\", \"LinearSVR\")\n",
    "# ypredict[\"y4\"] = computeMSE(finalFeaturesM, 10, \"f_Classif\",\"all\" ,\"Log\")\n",
    "\n",
    "# y_pred = ypredict.mean(axis =1)\n",
    "# rmse = mean_squared_error(finalFeaturesMSE.truthMean.values, y_pred.values)\n",
    "computeMSE(posPatternM, 10, \"f_Classif\",\"all\", \"LinearSVR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFeatures=pd.merge(posPatternClass[posPatternClass.columns[0:len(posPatternClass.columns)-1]],unigramsClass[unigramsClass.columns[0:len(unigramsClass.columns)-1]], on=[\"id\"])\n",
    "allFeatures=pd.merge(allFeatures,mixedClass[mixedClass.columns[0:len(mixedClass.columns)-1]], on=[\"id\"])\n",
    "allFeatures=pd.merge(allFeatures,finalFeaturesMSE, on=[\"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8275175413279654 -1.3304265357116412 -0.9926891489824088 ...\n",
      "  4.374236300204959 0.2994538073821884 -1.3933368436270321]\n",
      " [0.8275175413279654 -1.3304265357116412 0.04339854938250764 ...\n",
      "  -0.4353280201673156 -0.17838974378770206 0.7177015411412194]\n",
      " [-1.5906234380249364 -0.34384063362631984 0.04339854938250764 ...\n",
      "  -0.4353280201673156 -0.6562332949575925 -1.3933368436270321]\n",
      " ...\n",
      " [-0.986088193186711 -0.34384063362631984 -0.9926891489824088 ...\n",
      "  -0.4353280201673156 -0.6562332949575925 0.7177015411412194]\n",
      " [0.2229822964897399 1.6293311705443227 -0.9926891489824088 ...\n",
      "  0.25175259702872366 0.7772973585520789 -1.3933368436270321]\n",
      " [-0.3815529483484855 -1.3304265357116412 0.04339854938250764 ...\n",
      "  -0.4353280201673156 -0.6562332949575925 0.7177015411412194]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unknown label type: 'continuous'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-196-041ce6fce86d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomputeMSE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mallFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"featureself_classif\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Log\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-105-78b00599575f>\u001b[0m in \u001b[0;36mcomputeMSE\u001b[1;34m(finalFeaturesMSE, cross_val, featuresel, numFeat, regressor)\u001b[0m\n\u001b[0;32m     34\u001b[0m                 clr = xgboost.XGBRegressor(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.75,\n\u001b[0;32m     35\u001b[0m                            colsample_bytree=1, max_depth=7)\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mclr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_new\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_idx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1215\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0;32m   1216\u001b[0m                          order=\"C\")\n\u001b[1;32m-> 1217\u001b[1;33m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1218\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    170\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[0;32m    171\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[1;32m--> 172\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Unknown label type: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Unknown label type: 'continuous'"
     ]
    }
   ],
   "source": [
    "computeMSE(allFeatures, 10, \"featureself_classif\", 250, \"Log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log yields accuracy of:0.6945906779814721\n",
      "SVM yields accuracy of:0.7031355862182929\n",
      "RandomForest yields accuracy of:0.690955329895363\n",
      "XgBoost yields accuracy of:0.7010964290920279\n",
      "NaiveBayes yields accuracy of:0.6738621009774504\n"
     ]
    }
   ],
   "source": [
    "features2withLabel = pd.merge(features2, Labels[[\"id\",\"truthClass\"]], on = \"id\")\n",
    "#Pick classifier for informality\n",
    "classifiers = [\"Log\",\"SVM\",\"RandomForest\",\"XgBoost\",\"NaiveBayes\"]\n",
    "for classif in classifiers:\n",
    "    [acc1,acc2]=useNirmalFunctionEfficiently(classif,\"f_classif\",\"all\",features2withLabel)\n",
    "    print(classif+\" yields accuracy of:\"+str(acc1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log yields accuracy of:0.7031504921510423\n",
      "SVM yields accuracy of:0.7059927022057959\n",
      "RandomForest yields accuracy of:0.6881245192853481\n",
      "XgBoost yields accuracy of:0.7019292938860149\n",
      "NaiveBayes yields accuracy of:0.6738604552120049\n"
     ]
    }
   ],
   "source": [
    "features1withLabel = pd.merge(features1, Labels[[\"id\",\"truthClass\"]], on = \"id\")\n",
    "#Pick classifier for informality\n",
    "classifiers = [\"Log\",\"SVM\",\"RandomForest\",\"XgBoost\",\"NaiveBayes\"]\n",
    "for classif in classifiers:\n",
    "    [acc1,acc2]=useNirmalFunctionEfficiently(classif,\"f_classif\",\"all\",features1withLabel)\n",
    "    print(classif+\" yields accuracy of:\"+str(acc1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
